<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<link rel="alternate"
      type="application/rss+xml"
      href="https://dou-meishi.github.io/org-blog/rss.xml"
      title="RSS feed for https://dou-meishi.github.io/org-blog/">
<title>Characteristic Functions and Central Limit Theorem</title>
<meta name="author" content="Dou Meishi">
<meta name="referrer" content="no-referrer">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href= "https://gongzhitaao.org/orgcss/org.css" rel="stylesheet" type="text/css" />
<link href= "https://dou-meishi.github.io/org-blog/static/dou-org-blog.css" rel="stylesheet" type="text/css" />
<!-- Math Support by KaTeX -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
</head>
<body>
<div id="preamble" class="status"><div class="header">
  <div class="sitelinks">
    <a href="https://dou-meishi.github.io/org-blog/index.html">Home</a>
    |
    <a href="https://dou-meishi.github.io/org-blog/archive.html">All Posts</a>
  </div>
</div>
</div>
<div id="content">
<div class="post-date">22 Mar 2024</div><h1 class="post-title"><a href="https://dou-meishi.github.io/org-blog/2024-03-22-CentralLimitTheorem/notes.html">Characteristic Functions and Central Limit Theorem</a></h1>
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org6181b06">Basics of Characteristic Functions</a>
<ul>
<li><a href="#orga27a529">Related to Fourier Transform</a></li>
<li><a href="#org3056bb4">Related to Moments</a></li>
<li><a href="#org64a03a0">Related to Weak Convergence</a></li>
</ul>
</li>
<li><a href="#orge899d2f">Central Limit Theorem and Gaussian Distribution</a></li>
</ul>
</div>
</nav>
<p>
Strong Law of Large Numbers (SLLN) and Central Limit Theorem (CLT) are
two significant results in probability theory and statistics. Both
theorems concern the asymptotic behavior of the sum of i.i.d. random
variables, but they follow different scaling. SLLN examines the case
when the sum is divided by \(n\), while CLT considers the case when the
sum is divided by \(\sqrt{n}\). Their conclusions are also
different. SLLN asserts that the considered random variable will
converge to a constant <i>almost surely</i>, while CLT ensures that the
<i>distribution</i> of the considered random variable converge to a Gaussian
distribution. With the help of characteristic functions, we are able
to prove the CLT straightforwardly and see how a Gaussian distribution
comes out.
</p>

<p>
In this post, we start by introducing the definition of characteristic
functions (c.f.)  of random variables, and some basic properties like
boundness, uniformly continuity, and usage in computing moments. Once
familiar with the definition, we use c.f.s to obtain many useful
results.
</p>

<ol class="org-ol">
<li>Two random variables follow the same distribution if and only if
they have the same c.f.. This result is particularly useful, as it
ensures that it is possible to deduce the distribution of a random
variable from its c.f.s. For example, if we note a random variable
has the same c.f. as a normal distribution, then we can conclude
that it must follows this normal distribution.</li>

<li>Derivatives of c.f.s are proportional to the moments. This result
is particularly useful in calculating the \(k\)-th moment from the
c.f.. by taking \(k\)-th derivative. Moreover, we are able to give
a Taylor expansion of c.f.s if we know the moments.</li>

<li>Convergence in distribution of random variables is equivalent to
pointwise convergence of c.f.s. This result is particularly useful
in studying asymptotic behaviors. For example, in CLT we want to
study the asymptotic behavior of the sum of i.i.d. random
variables. With the help of c.f.s, we need only to study the limit
of c.f.s. Indeed, we will see that this limit will be the c.f. of a
normal distribution, which concludes the CLT.</li>
</ol>

<p>
<i>Suggested readings:</i> Durrett (2019, pp. 108&#x2013;118), Billingsley (2008,
pp. 342&#x2013;351), and Schilling (2017, pp. 214&#x2013;220).
</p>

<div id="outline-container-org6181b06" class="outline-2">
<h2 id="org6181b06">Basics of Characteristic Functions</h2>
<div class="outline-text-2" id="text-org6181b06">
<p>
<i>Definition.</i> The <i>characterstic function (c.f.)</i> \(\varphi(t)\) of a random
variable \(X\) is defined by \[ \varphi_X(t) = \mathbb{E}[e^{itX}] = \mathbb{E}[\cos tX] +
i\mathbb{E}[\sin tX]. \] In general, the c.f. of a finite measure \(\mu\) is \[
\varphi_\mu(t) = \int e^{itx}\, \mu(dx) = \int \cos tx \, \mu(dx) + i \int \sin tx\,
\mu(dx).\] Naturally, the definition can be applied to integrable
functions, which coincides with the <i>inverse Fourier transform</i>: \[
\varphi_f(t) = \int f(x) e^{itx} \, dx = \int f(x) \cos tx \, dx + i \int f(x) \sin
tx \, dx. \] Clearly, when \(f\) is integrable, \(\varphi(t)\) exists for all
\(t\), though \(\varphi\) might not be integrable.
</p>

<p>
For random variables, c.f.s always exist as \(\cos tX\) and \(\sin tX\)
are bounded and thus integrable. One benefit of introducing c.f.s is
the convenience to handle sum of two independent r.v.s. Let \(X_1\) and
\(X_2\) be two independent r.v.s. Then the sum \(X_1+X_2\) has c.f.  \[
\varphi_{X_1+X_2}(t) = \mathbb{E}[e^{it(X_1+X_2)}] = \mathbb{E}[e^{itX_1}] \cdot \mathbb{E}[e^{itX_2}] =
\varphi_{X_1}(t) \varphi_{X_2}(t). \] This result is known as the <i>convolution
theorem</i>, as the distribution of \(X_1+X_2\) is the convolution of
distributions of \(X_1\) and \(X_2\)<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>.
</p>

<p>
Characteristic functions have many useful properties. For example, it
is always bounded by 1, i.e., \(|\varphi(t)| \leq 1\). Clearly, \(\varphi(0) = 1\).  It
is also uniformly continuous. To see this, \[ \varphi(t+h) - \varphi(t) =
\mathbb{E}[e^{i(t+h)X} - e^{itX}] = \mathbb{E}[e^{itX}(e^{ihX} - 1)]. \] As \(|e^{ihX} -
1| \leq 2\), we can apply the dominated convergence theorem and conclude
that the integral converges to 0 as \(h \to 0\).  Moreover, we observe
that \(|e^{it} - 1| = \bigl| \int_0^t ie^{ix} \, dx \bigr| \leq t\) holds for
all real \(t\). Hence, \(|e^{ihX} - 1| / h \leq X\). If \(X\) is integrable,
then by dominated convergence theorem we can interchange the limit and
and the integral, i.e., \[ \lim_{h\to0} \frac{\varphi(t+h) - \varphi(t)}{h} =
\lim_{h\to0} \mathbb{E}\biggl[ e^{itX} \frac{e^{ihX} - 1}{h} \biggr] = \mathbb{E}\biggl[
e^{itX} \lim_{h\to0} \frac{e^{ihX} - 1}{h} \biggr] = \mathbb{E}[iX e^{itX}]. \]
We conclude that \(\varphi\) is differentiable when \(X\) is
integrable. Similarly, we can show that \(\varphi'(t)\) is uniformly
continuous in this case. Moreover, \(\varphi'(0) = i\mathbb{E}[X]\).
</p>

<p>
<i>Example (Dirac distribution).</i> Let \(X\) be a random variable with Dirac
distribution, i.e., \(\mathbb{P}(X=0) = 1\). By definition, its c.f. is \(\varphi(t) =
\mathbb{E}[e^{itX}] = 1\).
</p>

<p>
<i>Example (Two-point mass distribution).</i> Let \(X\) be the result of
flipping a coin, i.e., \(\mathbb{P}(X=1) = \mathbb{P}(X=-1) = 1/2\). By definition, its
c.f. is \(\varphi(t) = \mathbb{E}[e^{itX}] = (e^{it} + e^{-it})/2 = \cos t\).
</p>

<p>
<i>Example (Uniform distribution).</i> Let \(X\) follow the uniform
distribution on \([-a, a]\). By definition, its c.f. is \[\varphi(t) =
\mathbb{E}[e^{itX}] = \frac{1}{2a} \int e^{itx} \mathbb{1}( -a \leq x \leq a) \, dx = \frac{\sin
at}{at}.\]
</p>

<p>
<i>Example (Poisson distribution).</i> Let \(X\) follow the Poisson
distribution, i.e., \(\mathbb{P}(X=k) = e^{-\lambda}\lambda^k / k!\). By definition, its
c.f. is \[\varphi(t) = \mathbb{E}[e^{itX}] = \sum_{k=0}^\infty e^{itk} e^{-\lambda} \lambda^k / k! =
e^{-\lambda} \sum_{k=0}^\infty (\lambda e^{it})^k / k! = \exp(\lambda( e^{it} - 1)).\]
</p>

<p>
<i>Example (Gaussian distribution).</i> Let \(X\) follow the standard Gaussian
distribution \(\mathcal{N}(0, 1)\). By definition, its c.f. is<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup> \[\varphi(t) =
\mathbb{E}[e^{itX}] = \frac{1}{\sqrt{2\pi}} \int e^{itx} e^{-x^2/2} \, dx =
e^{-t^2/2} \int \frac{1}{\sqrt{2\pi}} \exp(-(x-it)^2/2) \, dx =
e^{-t^2/2}.\]
</p>

<p>
In summary, we can draw the following table.
</p>

<table>


<colgroup>
<col  class="org-left">

<col  class="org-left">

<col  class="org-left">
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Distribution</th>
<th scope="col" class="org-left">Density</th>
<th scope="col" class="org-left">Characteristic Functions</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Dirac</td>
<td class="org-left">\(\delta\)</td>
<td class="org-left">1</td>
</tr>

<tr>
<td class="org-left">Two-point mass</td>
<td class="org-left">\((\delta_{+1} + \delta_{-1})/2\)</td>
<td class="org-left">\(\cos t\)</td>
</tr>

<tr>
<td class="org-left">Uniform</td>
<td class="org-left">\(\mathbb{1}_{[-a,a]}/(2a)\)</td>
<td class="org-left">\(\frac{\sin at}{at}\)</td>
</tr>

<tr>
<td class="org-left">Poisson</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">\(\exp(\lambda(e^{it}-1))\)</td>
</tr>

<tr>
<td class="org-left">Gaussian</td>
<td class="org-left">\(\mathcal{N}(0, 1)\)</td>
<td class="org-left">\(\exp(-t^2/2)\)</td>
</tr>
</tbody>
</table>

<p>
Note that except the c.f. of Gaussian distributions, none of these
c.f.s is integrable.
</p>

<p>
We conclude this subsection by an important lemma, which states that
the c.f. of any integrable function must vanish when \(t\to\infty\). The proof
is simple but irrelevant to our main topic and thus is omitted. For
interesting readers, please see, Billingsley's book (2008, p. 345) or
Schilling's book (2017, pp. 221&#x2013;222).
</p>

<p>
<i>Lemma (Riemann-Lebesgue).</i> If \(\mu\) has a density, then \(|\varphi_\mu(t)|\to0\) when
\(t\to\infty\).
</p>

<p>
<i>Remark.</i> Here \(\mu\) has a density is equivalent to say \(\mu\) is absolutely
continuous with respect to the Lebesgue measure, i.e., \(\mu(dx) = f\,
dx\) where \(f\) is integrable. A counterexample is the Dirac measure,
which is of course not absolutely continuous w.r.t. the Lebesgue
measure.
</p>
</div>

<div id="outline-container-orga27a529" class="outline-3">
<h3 id="orga27a529">Related to Fourier Transform</h3>
<div class="outline-text-3" id="text-orga27a529">
<p>
Why we want to study characteristic functions? One reason is that the
c.f. fully characterizes a finite measure. In fact, any finite measure
can be recovered from its c.f.. Consequently, two finite measures
equal if and only if their c.f.s equal. Hence, it is possible to
determine distributions of random variables by looking at their c.f.s.
</p>

<p>
First, we introduce the <i>inversion theorem</i>, which provides a way to
recover the measure from its c.f..
</p>

<p>
<i>Theorem (Inversion).</i> Let \(\varphi\) be the c.f. of a finite measure
\(\mu\). Then, \[ \lim_{T\to\infty} \frac{1}{2\pi} \int_{-T}^T \frac{e^{-ita} -
e^{-itb}}{it} \varphi(t) \, dt = \mu(a, b) + \frac{1}{2}\mu(\{a\}) +
\frac{1}{2}\mu(\{b\}). \]
</p>

<p>
<i>Remark.</i> The integral on the left-hand side is improper when \(\varphi\) is not
integrable, e.g., when \(\varphi(t)\equiv1\). Nevertheless, the limit exists (this
existence is part of the conclusion). Indeed, take \(\mu\) to be the Dirac
measure, then \(\mu(-c, c)=1\) for all positive number \(c\). The integral
on the left-hand side becomes \(\frac{1}{\pi} \int_{-T}^T \frac{\sin ct}{t}
\, dt\), which converges to 1 as \(T\to \infty\).
</p>

<p>
<i>Proof.</i> The proof is based on the direct calculation of the left-hand
side. Consider \(f(x, t) = (e^{it(x-a)} - e^{it(x-b)}) / (it)\). Noting
that \(|f(x,t)| \leq |e^{it(b-a)}| / |t| \leq |b-a|\), we conclude that \(f(x,
t)\) is integrable on the product measure space \(\mu(dx) \otimes dt\). By
Fubini's theorem, we can interchange the order of integrals
</p>

$$ \begin{aligned}
\frac{1}{2\pi} \int_{-T}^T \frac{e^{-ita} - e^{-itb}}{it} \varphi(t) \, dt
&= \frac{1}{2\pi} \int_{-T}^T dt \int \mu(dx) \,
        \frac{e^{it(x-a)} - e^{it(x-b)}}{it} \\
&= \int \mu(dx) \, \frac{1}{2\pi} \int_{-T}^T dt \,
        \frac{e^{it(x-a)} - e^{it(x-b)}}{it} \\
&=: \int \mu(dx) \, R(x; T).
\end{aligned} $$

<p>
The proof is completed by noting that \(R(x; T)\) is bounded and
converges to<sup><a id="fnr.3" class="footref" href="#fn.3" role="doc-backlink">3</a></sup> \(\mathbb{1}_{(a,b)} + \frac{1}{2}\mathbb{1}_{\{a\}} +
\frac{1}{2}\mathbb{1}_{\{b\}}\) as \(T \to \infty\). By dominated convergence theorem,
the desired conclusion holds.
</p>

<p>
Q.E.D.
</p>

<p>
The inversion theorem implies the uniqueness of c.f.s. Assume two
finite measures \(\mu\) and \(\nu\) have the same c.f.. Then they agree on all
these intervals \((a, b)\) such that \(\mu(\{a\}) = \mu(\{b\}) = \nu(\{a\}) =
\mu(\{b\}) = 0\). As such endpoints are at most countable (otherwise \(\mu\)
and \(\nu\) cannot be finite), these intervals can generate the Borel
\(\sigma\)-algebra, implying that \(\mu\) and \(\nu\) agree on all Borel sets.
</p>

<p>
<i>Corollary (Uniqueness).</i> Two finite measures equal if and only if their
c.f.s equal.
</p>

<p>
Consequently, we can conclude that two random variables follow the
same distribution if and only if they have the same c.f.. In other
words, we can deduce the distribution of a random variable from its
c.f.. The previous subsection shows that the standard normal
distribution \(\mathcal{N}(0, 1)\) has c.f. \(\exp(-t^2/2)\). In general, the normal
distribution \(\mathcal{N}(\mu, \sigma^2)\) has c.f. \(\exp(it\mu - \sigma^2 t^2 / 2)\). Assume
\(X_1\) and \(X_2\) are independent and normally distributed with mean
\(\mu_1, \mu_2\) and variance \(\sigma_1^2\) and \(\sigma_2^2\) respectively. Then \(aX_1 +
bX_2\) has c.f.
</p>

$$ \begin{aligned}
\mathbb{E}[e^{it(aX_1 + bX_2)}]
&= \mathbb{E}[e^{i(at)X_1}] \cdot \mathbb{E}[e^{i(bt)X_2}] \\
&= \exp\Bigl(
    i(at)\mu_1 - \sigma_1^2 (at)^2 / 2 + i(bt)\mu_2 - \sigma_2^2 (bt)^2 / 2 \Bigr) \\
&= \exp\Bigl(
    it(a\mu_1 + b\mu_2) - (a^2\sigma_1^2 + b^2\sigma_2^2) t^2 / 2 \Bigr).
\end{aligned} $$

<p>
This concludes that \(aX_1 + bX_2\) has the same c.f. as \(\mathcal{N}(a\mu_1 + b\mu_2,
a^2\sigma_1^2 + b^2\sigma_2^2)\).
</p>

<p>
<i>Corollary (Normal).</i> Linear combinations of independent normal
variables are normal.
</p>

<p>
Finally, we relate the inversion theorem to <i>Fourier transform</i>. As we
see that the inverse Fourier transform pushes a density function to
its c.f., the Fourier transform recovers the density function from a
c.f..  Assume the c.f. \(\varphi\) of a finite measure \(\mu\) is integrable. Then
the integral on the left-hand side can be extended to the real line as
the integrand is integrable. Moreover, we can apply Fubini's theorem
to rewrite the integral
</p>

$$ \begin{aligned}
\frac{1}{2\pi} \int \frac{e^{-ita} - e^{-itb}}{it} \varphi(t) \, dt
&= \frac{1}{2\pi} \int dt \int_{a}^b \, dx \, e^{-itx} \varphi(t) \\
&= \int_{a}^b dx \, \frac{1}{2\pi} \int dt  \, e^{-itx} \varphi(t).
\end{aligned} $$

<p>
<i>Corollary (Fourier Transform).</i> If the c.f. \(\varphi\) is integrable, then \(\mu\)
has a density function \[ f(x) = \frac{1}{2\pi} \int e^{-itx} \varphi(t) \,
dt. \] Moreover, \(f\) is bounded and uniformly continuous (just like
\(\varphi\)).
</p>
</div>
</div>

<div id="outline-container-org3056bb4" class="outline-3">
<h3 id="org3056bb4">Related to Moments</h3>
<div class="outline-text-3" id="text-org3056bb4">
<p>
Studying c.f.s can also help us determine the moments of a
distribution. We have seen that if a random variable \(X\) is
integrable, then \(\varphi'(0) = i\mathbb{E}[X]\). In this subsection, we will extend
this result to \(k\)-th moment, i.e., \(\varphi^{(k)}(0) = i^k \mathbb{E}[X^k]\) if
\(X^k\) is integrable.
</p>

<p>
First, we need a technical lemma to estimate the remainder of the
Taylor expansion of \(e^{i\xi}\). Recall that according to integration by
parts<sup><a id="fnr.4" class="footref" href="#fn.4" role="doc-backlink">4</a></sup>, \[ e^{i\xi} - 1 - \sum_{k=1}^n \frac{i^k}{k!} \xi^k = \int_0^\xi
\frac{(\xi-t)^n}{n!} i^{n+1} e^{it} \, dt. \] Assume \(\xi > 0\). The
remainder can be bounded by \[ \biggl| e^{i\xi} - 1 - \sum_{k=1}^n
\frac{i^k}{k!} \xi^k \biggr| \leq \int_0^\xi \frac{(\xi -t)^n}{n!} \, dt =
\frac{\xi^{n+1}}{(n+1)!}. \] On the other hand, it can also be bounded
by \[ \biggl| e^{i\xi} - 1 - \sum_{k=1}^n \frac{i^k}{k!} \xi^k \biggr| \leq
\biggl| e^{i\xi} - 1 - \sum_{k=1}^{n-1} \frac{i^k}{k!} \xi^k \biggr| +
\frac{\xi^n}{n!} \leq 2\frac{\xi^{n}}{n!}. \] It is easy to generalize the
bound to the case \(\xi \leq 0\) and obtain the following lemma<sup><a id="fnr.5" class="footref" href="#fn.5" role="doc-backlink">5</a></sup>.
</p>

<p>
<i>Lemma.</i> For any real \(\xi\), the remainder of the \(n\)-th order Taylor
expansion of \(e^{i\xi}\) can be bounded by \[ \biggl| e^{i\xi} - 1 -
\sum_{k=1}^n \frac{i^k}{k!} \xi^k \biggr| \leq \min\biggl( 2\frac{|\xi|^n}{n!},
\frac{|\xi|^{n+1}}{(n+1)!} \biggr). \]
</p>

<p>
We can use this lemma to obtain the Taylor expansion of c.f.s. Let \(X\)
be a random variable such that \(|X|^{n+1}\) is integrable. Then,
</p>

$$ \begin{aligned}
\biggl| \mathbb{E}[e^{itX}] - 1 - \sum_{k=1}^n \frac{(it)^k}{k!} \mathbb{E}[X^k] \biggr|
&\leq \mathbb{E}\biggl| e^{itX} - 1 - \sum_{k=1}^n \frac{(it)^k}{k!} X^k \biggr| \\
&\leq |t^n| \mathbb{E}\biggl[ \min\biggl(
   \frac{2|X|^n}{n!}, \frac{|t||X|^{n+1}}{(n+1)!} \biggr) \biggr].
\end{aligned} $$

<p>
Denote by \(c_k = i^k \mathbb{E}[X^k] / k!\). We can show that the remainder has
order \(o(t^n)\).  \[ \lim_{t\to0} \frac{\biggl| \varphi(t) - 1 - \sum_{k=1}^n c_k
t^k \biggr|}{|t^n|} \leq \lim_{t\to0} \mathbb{E}\biggl[ \min\biggl( \frac{2|X|^n}{n!},
\frac{|t||X|^{n+1}}{(n+1)!} \biggr) \biggr]. \] Indeed, the integrand
is bounded by \(2|X|^n/n!\), which is integrable. By dominated
convergence theorem, we can interchange the order of limit and
expectation, concluding that the expectation converges to 0. Note that
this argument only requires \(X^n\) is integrable.
</p>

<p>
<i>Theorem.</i> If \(X^n\) is integrable, then in the neighborhood of \(t=0\),
the c.f. has Taylor expansion \[ \varphi(t) = 1 + \sum_{k=1}^n c_k t^k +
o(t^n), \quad\text{where}\quad c_k = i^k \mathbb{E}[X^k] / k!. \]
</p>

<p>
This result inspires us to compute the \(k\)-th moment \(\mathbb{E}[X^k]\) by
taking \(k\)-th derivative of \(\varphi\). We have shown that \(\varphi'(t) =
\mathbb{E}[iXe^{itX}]\) when \(X\) is integrable. Repeating the argument can show
that \(\varphi^{(k)}(t) = \mathbb{E}[(iX)^k e^{itX}]\) if \(X^k\) is integrable<sup><a id="fnr.6" class="footref" href="#fn.6" role="doc-backlink">6</a></sup>.
</p>

<p>
<i>Corollary.</i> If \(|X|^k\) is integrable, then \(\varphi\) is \(k\)-th
differentiable and \(\varphi^{(k)}(0) = i^k \mathbb{E}[X^k]\). Moreover, the \(k\)-th
derivative is bounded, uniformly continuous, and has an explicit form
\(\varphi^{(k)}(t) = \mathbb{E}[(iX)^k e^{itX}]\).
</p>
</div>
</div>

<div id="outline-container-org64a03a0" class="outline-3">
<h3 id="org64a03a0">Related to Weak Convergence</h3>
<div class="outline-text-3" id="text-org64a03a0">
<p>
Finally, c.f.s are useful in studying limiting distributions. This is
due to <i>the continuity theorem</i>. The proof utilizes the concept of
tightness of measures and thus is omitted here; see, e.g.,
Billingsley's book (2008, pp. 349&#x2013;350) or Durrett's book (2019,
pp. 114&#x2013;115).
</p>

<p>
<i>Theorem (Continuity theorem).</i> Let \(\mu_n\) and \(\mu\) be finite measures
with c.f.s \(\varphi_n\) and \(\varphi\). Then \(\mu_n \Rightarrow \mu\) if and only if \(\varphi_n(t) \to
\varphi(t)\) for each \(t\).
</p>

<p>
<i>Remark.</i> The condition requires that the limiting function \(\lim
\varphi_n(t)\) is indeed a c.f. of some finite measure \(\mu\). However, it might
not be true. For example, let \(\varphi_n(t) = \exp(-nt^2/2)\) be the c.f. of
the Gaussian distribution \(\mathcal{N}(0, n)\). Then \(\lim \varphi_n(t) =
\mathbb{1}_{\{0\}}(t)\), which is clearly not a c.f. (as any c.f. must be
uniformly continuous). Thus, \(\mu_n\) does not converge weakly.
</p>

<p>
The continuity theorem relates the pointwise convergence of c.f.s with
the weak convergence of probability measures. In the following
section, we will use it to study the limiting distribution of sum of
i.i.d. random variables through studying the limiting c.f., as it is
much easier to work with product of c.f.s than the convolution of
distributions.
</p>
</div>
</div>
</div>

<div id="outline-container-orge899d2f" class="outline-2">
<h2 id="orge899d2f">Central Limit Theorem and Gaussian Distribution</h2>
<div class="outline-text-2" id="text-orge899d2f">
<p>
With the help of c.f.s, it is not hard to find out the sum of
i.i.d. random variables follows a Gaussian distribution. Let \(X_n\) be
i.i.d. random variables with mean \(\mu\) and finite variance \(\sigma^2 <
\infty\). Let \[ Z_n = \frac{\sum_{k=1}^n X_k - n\mu}{\sqrt{n}\sigma}. \] Now we show
that the limiting distribution of \(Z_n\) is the standard Gaussian
distribution \(\mathcal{N}(0, 1)\).
</p>

<p>
Let \(Y_n = (X_n - \mu) / \sigma\). Then \(Y_n\) are i.i.d. random variables with
 mean 0 and variance 1.  Let \(\varphi_n\) be their c.f.s. Of course, as \(Y_n\)
 are i.i.d., their c.f. are the same \(\varphi_n \equiv \varphi\). By the continuity
 theorem, it is sufficient to show the c.f. of \(Z_n\) converges to
 \(\exp(-t^2/2)\).  \[ \mathbb{E}[\exp(itZ_n)] = \mathbb{E}\biggl[ \exp\biggl( it
 \frac{\sum_{k=1}^n Y_k}{\sqrt{n}} \biggr) \biggr] =
 \mathbb{E}\biggl[\exp\biggl(it \frac{Y_k}{\sqrt{n}} \biggr) \biggr]^n =
 \biggl[\varphi\biggl(\frac{t}{\sqrt{n}}\biggr)\biggr]^n. \] As \(Y_k\) has
 mean 0 and finite variance 1, it must be square integrable. Thus, its
 c.f. has Taylor expansion \[ \varphi(t) = 1 - \frac{1}{2}t^2 + o(t^2).\]
 Hence, we can continue to caculate the c.f. of \(Z_n\).  \[
 \mathbb{E}[\exp(itZ_n)] = \biggl[\varphi\biggl(\frac{t}{\sqrt{n}}\biggr)\biggr]^n =
 \biggl[ 1 - \frac{t^2}{2n} + o\biggl(\frac{t^2}{n}\biggr) \biggr]^n \to
 \exp(-t^2/2). \] The final limit exists as \((1 + c/n + o(1/n))^n \to
 e^c\) for all real number \(c\)<sup><a id="fnr.7" class="footref" href="#fn.7" role="doc-backlink">7</a></sup>.
</p>

<p>
<i>Theorem. (Central limit theorem).</i> Let \(X_1, X_2, \ldots\) be
i.i.d. random variables with mean \(\mu\) and positive finite variance
\(\sigma^2\). Then \[ \frac{\sum_{k=1}^nX_k - n\mu}{\sigma\sqrt{n}} \Rightarrow \mathcal{N}(0, 1). \]
</p>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
For any Borel set \(B\), there is (the last equality holds
because of independence) \[ \mu_{X_1+X_2}(B) = \mathbb{P}(X_1 + X_2 \in B) =
\mathbb{E}[\mathbb{1}(X_1 + X_2 \in B)] = \int \mathbb{1}(x_1 + x_2 \in B) \, \mu_{X_1}(dx_1)
\mu_{X_2}(dx_2). \] In general, the <i>convolution</i> of two finite measure is
defined by \[ \mu_1 \star \mu_2 (B) := \int \mathbb{1}_B(x+y) \, \mu_1(dx) \mu_2(dy). \]
The convolution theorem states that the c.f. of \(\mu_1 \star \mu_2\) is
exactly \(\varphi_{\mu_1}(t) \varphi_{\mu_2}(t)\). For a direct proof, see Schilling's
book (2017, p. 221).
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
The normal density function with mean \(it\) and variance 1
indeed integrals to 1 for all real \(t\), but this conclusion requires
proof. The rigorous treatment is showing the c.f. of the standard
normal distribution is indeed \(e^{-t^2/2}\). As \(X\) is integrable, the
c.f. is continuously differentiable and
</p>

$$ \begin{aligned}
\varphi'(t) &= \mathbb{E}[iXe^{itX}] \\
&= \int ix e^{itx} \frac{1}{\sqrt{2\pi}} e^{-x^2/2} \, dx \\
&= \int -i e^{itx} \, d\frac{1}{\sqrt{2\pi}} e^{-x^2/2} \\
&= -\int t e^{itx} \frac{1}{\sqrt{2\pi}} e^{-x^2/2}  \, dx \
&= -t \varphi(t).
\end{aligned} $$

<p class="footpara">
Let \(\xi(t) = \varphi(t) \exp(t^2/2)\). Then \(\xi(0) = 1\) and \(\xi'(t) = [\varphi'(t) +
t\varphi(t)]\exp(t^2/2) \equiv 0\). Hence, \(\xi(t) = \varphi(t) \exp(t^2/2) \equiv 1\).
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3" role="doc-backlink">3</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
In order to see this, we prove the following lemma first.
</p>

<blockquote>
<p>
<i>Lemma.</i> The sinc function \(\frac{\sin x}{x}\) is not integrable but its
improper Riemann integral exists \[\lim_{T\to\infty} \int_{-T}^T \frac{\sin
x}{x} \, dx = \pi.\]
</p>
</blockquote>

<p class="footpara">
This sinc function is a sequence of "bumps" of decreasing size. The
 \(n\)-th "bump" bounds area on the order of \(1/n\), but \(\sum 1/n =
 \infty\). To see this,
</p>

$$ \begin{aligned}
\int \biggl| \frac{\sin x}{x} \biggr| \, dx &= 2\int_0^\infty \frac{|\sin x|}{x} \, dx \\
&= 2\sum_{k=0}^\infty \int_{k\pi}^{(k+1)\pi} \frac{|\sin x|}{x} \, dx \\
&\geq 2\sum_{k=0}^\infty \int_{k\pi}^{(k+1)\pi} \frac{|\sin x|}{(k+1)\pi} \, dx \\
&= \frac{4}{\pi} \sum_{k=0}^\infty \frac{1}{k+1}.
\end{aligned} $$

<p class="footpara">
This concludes that \(\frac{\sin x}{x}\) is not
integrable. Nevertheless, the improper Riemann integral exists and
equals \(\pi\); see, e.g., Schilling's book (2017, <a href="./Schilling-p145.png">p. 145</a>) or
Billingsley's book (2008, <a href="./Billingsley-pp235-236.png">pp. 235&#x2013;236</a>).
</p>

<p class="footpara">
Let \(S(T) = \int_0^T \frac{\sin x}{x} \, dx\). Then \(S(T) \to
\pi/2\). Moreover, there exists a constant \(M\) such that \(|S(T)| \leq
M\). Indeed, as \(S(T) \to \pi/2\), there exists \(T_0 > 0\) such that \(|S(T)|
\leq \pi\) for all \(T \geq T_0\). For \(T < T_0\), there is \(|S(T)| \leq
\int_0^{T_0} |\sin x| / |x| \, dx \leq T_0\). Hence, \(|S(T)| \leq \max(T_0,
\pi)\).
</p>

<p class="footpara">
Now we can discuss the boundness and convergence result of \(R(x;
T)\). Consider \(f(\xi, t) = e^{it\xi} \mathbb{1}(x - b \leq \xi \leq x - a)\). Clearly, \(f(\xi,
t)\) is integrable on the product space \(\mathbb{R} \times [-T, T]\). Hence, we can
apply Fubini's theorem to interchange the order of integrals.
</p>

$$ \begin{aligned}
R(x; T)
&:= \frac{1}{2\pi} \int_{-T}^T \frac{e^{it(x-a)} - e^{it(x-b)}}{it} \, dt \\
&= \frac{1}{2\pi} \int_{-T}^T dt \int d\xi \, e^{it\xi} \mathbb{1}(x - b \leq \xi \leq x - a) \\
&= \frac{1}{2\pi} \int d\xi \int_{-T}^T dt \,  e^{it\xi} \mathbb{1}(x - b \leq \xi \leq x - a) \\
&= \frac{1}{\pi} \int_{x-b}^{x-a} \frac{\sin (T\xi)}{\xi} \, d\xi \\
&= \frac{1}{\pi}[\operatorname{sgn} (x-a) S(T|x-a|)
                - \operatorname{sgn} (x-b) S(T|x-b|)].
\end{aligned} $$

<p class="footpara">
Here we use the fact that for any real number \(c\), the integral \(\int_0^c
\frac{\sin Tx}{x} \, dx = \operatorname{sgn}(c) S(T|c|)\). As \(|S(T)| \leq
M\) for some constant \(M\), we conclude that \(|R(x; T)| \leq
2M/\pi\). Moreover, as \(T \to \infty\), \[ R(x; T) \to \begin{cases} 1, &\quad a <
x < b, \\ 1/2, &\quad x = a \text{ or } x = b, \\ 0, &\quad x < a
\text{ or } x > b. \end{cases} \]
</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4" role="doc-backlink">4</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
In general, by integration by parts
</p>

$$ \begin{aligned}
\int_0^x \frac{(x - t)^n}{n!} f^{(n+1)}(t) \, dt
&= \int_0^x \frac{(x - t)^n}{n!} \, df^{(n)}(t)   \\
&= -\frac{x^n}{n!}f^{(n)}(0)
     + \int_0^x \frac{(x-t)^{n-1}}{(n-1)!} f^{(n)}(t) \, dt \\
&= \cdots \\
&= -\frac{x^n}{n!}f^{(n)}(0) - \cdots - \frac{x^2}{2}f''(0) - xf'(0)
     + \int_0^x f'(t) \, dt \\
&= f(x) - f(0) - \sum_{k=1}^n \frac{f^{(k)}(0)}{k!} x^k.
\end{aligned} $$</div></div>

<div class="footdef"><sup><a id="fn.5" class="footnum" href="#fnr.5" role="doc-backlink">5</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Assume \(\xi < 0\). Then,
</p>

$$ \begin{aligned}
\biggl| e^{i\xi} - 1 - \sum_{k=1}^n \frac{i^k}{k!} \xi^k \biggr|
&= \biggl| \int_0^\xi \frac{(\xi -t)^n}{n!} i^{n+1} e^{it} \, dt \biggr| \\
&=  \biggl| \int_\xi^0 \frac{(\xi -t)^n}{n!} i^{n+1} e^{it} \, dt \biggr| \\
&\leq \int_\xi^0 \frac{|\xi -t|^n}{n!} \, dt \\
&= \int_\xi^0 \frac{(t-\xi)^n}{n!} \, dt \\
&= \frac{(-\xi)^{n+1}}{(n+1)!}.
\end{aligned} $$</div></div>

<div class="footdef"><sup><a id="fn.6" class="footnum" href="#fnr.6" role="doc-backlink">6</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
See, e.g., Billingsley's book (2008, <a href="./Billingsley-pp344-345.png">pp. 344&#x2013;345</a>).
</p></div></div>

<div class="footdef"><sup><a id="fn.7" class="footnum" href="#fnr.7" role="doc-backlink">7</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Taking log on the limit yields \[ \lim_{n\to\infty} \frac{\log(1 +
c/n + o(1/n))}{1/n} = \lim_{n\to\infty} \frac{\log(1 + c/n + o(1/n))}{c/n +
o(1/n)} \cdot \frac{c/n + o(1/n)}{1/n} = c. \]
</p></div></div>


</div>
</div><div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-math.html">math</a> </div></div>
<div id="postamble" class="status">Created by <a href="https://github.com/bastibe/org-static-blog/">Org Static Blog</a>
</div>
</body>
</html>
