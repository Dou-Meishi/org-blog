<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<link rel="alternate"
      type="application/rss+xml"
      href="https://dou-meishi.github.io/org-blog/rss.xml"
      title="RSS feed for https://dou-meishi.github.io/org-blog/">
<title>Dou Meishi's Blog</title>
<meta name="author" content="Dou Meishi">
<meta name="referrer" content="no-referrer">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="google-site-verification" content="_Ly4i8BW_CWeGaFdsQgJ2xN-yOkGpSDnLw8LitvkEsw" />
<link href= "https://gongzhitaao.org/orgcss/org.css" rel="stylesheet" type="text/css" />
<link href= "https://dou-meishi.github.io/org-blog/static/dou-org-blog.css" rel="stylesheet" type="text/css" />
<!-- Math Support by KaTeX -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
</head>
<body>
<div id="preamble" class="status"><div class="header">
  <div class="sitelinks">
    <a href="https://dou-meishi.github.io/org-blog/index.html">Home</a>
    |
    <a href="https://dou-meishi.github.io/org-blog/archive.html">All Posts</a>
  </div>
</div>
</div>
<div id="content">
<h1 class=title> Recent Posts </h1><h2 class="post-title"><a href="https://dou-meishi.github.io/org-blog/2025-04-26-DisplayUnicodeCharacters/notes.html">Display Unicode Characters in Emacs</a></h2><div class="post-date">26 Apr 2025</div><p>
<i>Motivation.</i> I prefer Unicode symbols over LaTeX sequences.  In <a href="https://dou-meishi.github.io/org-blog/2024-02-24-EmacsPrettifySymbols/notes.html">this
old post</a> , I discussed how to display LaTeX commands as Unicode
symbols in Emacs. However, after applying these settings, my Emacs
renders both <code>\mathcal{A}</code> and <code>\mathscr{A}</code> identically as the <code>U+1D4D0</code> character from the
<i>Libertinus Math</i> font.
</p>...<div class="taglist"></div><h2 class="post-title"><a href="https://dou-meishi.github.io/org-blog/2025-02-20-ConvMathVisualCode/notes.html">Gradients of Convolution: Direct Computation and Linear Algebra Perspective</a></h2><div class="post-date">20 Feb 2025</div><p>
Convolution operations are foundational in deep learning for
extracting features in image tasks. Calculating their gradients is
critical for training convolutional neural networks, enabling
backpropagation to update parameters and minimize loss. This post
derives these gradients through two complementary approaches: direct
differentiation of the convolution definition, and a linear algebra
perspective which naturally introduces the <i>transposed convolution</i>
(also known as <i>deconvolution</i>).
</p>...<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-ai.html">ai</a> </div><h2 class="post-title"><a href="https://dou-meishi.github.io/org-blog/2024-12-01-UseGiscusInBlog/notes.html">Add A Comment Section to My Blog with Giscus</a></h2><div class="post-date">01 Dec 2024</div><p>
Early this year, I created this blog site and share some well-written
notes on it. Since then, I really enjoy blogging and continue to post
interesting things I learn. Recently, I came across the <a href="https://github.com/giscus/giscus/tree/main">giscus</a>
project, a comment system powered by GitHub Discussions, which allows
me to add a comment section to my posts. It seems to work well on my
site after trying it, so I believe it presents a good opportunity to
enhance this site further and encourage more exchanges of ideas.
</p>...<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-emacs.html">emacs</a> </div><h2 class="post-title"><a href="https://dou-meishi.github.io/org-blog/2024-11-28-UseEinops/notes.html">Practical Einops: Tensor Operations Based on Indices</a></h2><div class="post-date">28 Nov 2024</div><p>
People are familiar with vectors and matrices operations but are less
familiar with tensor operations. In machine learning, <i>tensors</i> often
refer to batched vectors or batched matrices and are represented by an
array-like object with multiple indices. Due to this reason, tensors
operations in most Python packages, including NumPy, PyTorch and
TensorFlow, are typically named after vectors and matrices operations.
However, tensors themselves have a particular useful operation, called
<i>contraction</i>, which uses index-based notations and can cover most
vectors and matrices operations. This index-based notations
intuitively and verbosely describe the relationship between the
components of input and output tensors. Today's topic, the Python's
<a href="https://github.com/arogozhnikov/einops">einops</a> package, extends these notations and provides an elegant API
for flexible and powerful tensor operations.
</p>...<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-ai.html">ai</a> </div><h2 class="post-title"><a href="https://dou-meishi.github.io/org-blog/2024-10-30-LearnTransformer/notes.html">What Happens in A Transformer Layer</a></h2><div class="post-date">30 Oct 2024</div><p>
Transformers serve as the backbone of large language models. Just as
 convolutional networks revolutionized image processing, transformers
 have significantly advanced natural language processing since their
 introduction. The efficient parallel computation and transfer
 learning capabilities of transformers have led to the rise of
 <i>pre-trained paradigm</i>. In this approach, a large-scale
 transformer-based model, referred to as the <i>foundation model</i>, is
 trained on a significant volume of data and subsequently utilized for
 downstream tasks through some form of fine-tuning. Our familiar
 friend ChatGPT is such an example, where GPT stands for generative
 pre-trained transformers. Meanwhile, transformer-base models achieve
 state-of-the-art performace for many different modalities, including
 text, image, video, point cloud, and audio data, and have been used
 for both discriminative and generative applications.
</p>...<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-ai.html">ai</a> </div><div id="archive">
<a href="https://dou-meishi.github.io/org-blog/archive.html">Other posts</a>
</div>
</div>
<div id="postamble" class="status">Created by <a href="https://github.com/bastibe/org-static-blog/">Org Static Blog</a>
</div>
</body>
</html>
