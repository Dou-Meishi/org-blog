<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<link rel="alternate"
      type="application/rss+xml"
      href="https://dou-meishi.github.io/org-blog/rss.xml"
      title="RSS feed for https://dou-meishi.github.io/org-blog/">
<title>Dou Meishi's Blog</title>
<meta name="author" content="Dou Meishi">
<meta name="referrer" content="no-referrer">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href= "https://gongzhitaao.org/orgcss/org.css" rel="stylesheet" type="text/css" />
<link href= "https://dou-meishi.github.io/org-blog/static/dou-org-blog.css" rel="stylesheet" type="text/css" />
<!-- Math Support by KaTeX -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
</head>
<body>
<div id="preamble" class="status"><div class="header">
  <div class="sitelinks">
    <a href="https://dou-meishi.github.io/org-blog/index.html">Home</a>
    |
    <a href="https://dou-meishi.github.io/org-blog/archive.html">All Posts</a>
  </div>
</div>
</div>
<div id="content">
<h1 class=title> Recent Posts </h1><h2 class="post-title"><a href="https://dou-meishi.github.io/org-blog/2024-10-30-LearnTransformer/notes.html">What Happens in A Transformer Layer</a></h2><div class="post-date">30 Oct 2024</div><p>
Transformers serve as the backbone of large language models. Just as
 convolutional networks revolutionized image processing, transformers
 have significantly advanced natural language processing since their
 introduction. The efficient parallel computation and transfer
 learning capabilities of transformers have led to the rise of
 <i>pre-trained paradigm</i>. In this approach, a large-scale
 transformer-based model, referred to as the <i>foundation model</i>, is
 trained on a significant volume of data and subsequently utilized for
 downstream tasks through some form of fine-tuning. Our familiar
 friend ChatGPT is such an example, where GPT stands for generative
 pre-trained transformers. Meanwhile, transformer-base models achieve
 state-of-the-art performace for many different modalities, including
 text, image, video, point cloud, and audio data, and have been used
 for both discriminative and generative applications.
</p>...<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-ai.html">ai</a> </div><h2 class="post-title"><a href="https://dou-meishi.github.io/org-blog/2024-09-24-ImplementBP/notes.html">An In-Depth Introduction to Backpropagation and Automatic Differentiation</a></h2><div class="post-date">24 Sep 2024</div><p>
Backpropagation and automatic differentiation (AD) are fundamental
components of modern deep learning frameworks. However, many
practitioners pay little attention to their implementations and may
regard them as some sort of "black magic". It indeed looks like magic
that PyTorch can virtually calculate derivatives of an arbitrary
function defined by the user, and even accommodate flow control
elements like conditional execution, which is mathematically not
differentiable. Although we understand that mathematically they
primarily employ the chain rule, it remains unclear how they
efficiently apply it to a function whose form is entirely unknown and
will be determined by the user.
</p>...<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-ai.html">ai</a> </div><h2 class="post-title"><a href="https://dou-meishi.github.io/org-blog/2024-03-22-CentralLimitTheorem/notes.html">Characteristic Functions and Central Limit Theorem</a></h2><div class="post-date">22 Mar 2024</div><p>
Strong Law of Large Numbers (SLLN) and Central Limit Theorem (CLT) are
two significant results in probability theory and statistics. Both
theorems concern the asymptotic behavior of the sum of i.i.d. random
variables, but they follow different scaling. SLLN examines the case
when the sum is divided by \(n\), while CLT considers the case when the
sum is divided by \(\sqrt{n}\). Their conclusions are also
different. SLLN asserts that the considered random variable will
converge to a constant <i>almost surely</i>, while CLT ensures that the
<i>distribution</i> of the considered random variable converge to a Gaussian
distribution. With the help of characteristic functions, we are able
to prove the CLT straightforwardly and see how a Gaussian distribution
comes out.
</p>...<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-math.html">math</a> </div><h2 class="post-title"><a href="https://dou-meishi.github.io/org-blog/2024-03-17-LawOfLargeNumbers/notes.html">Strong Law of Large Numbers</a></h2><div class="post-date">17 Mar 2024</div><p>
We all know that probability can be interpreted as <i>frequency</i>, but
behind it there is an important theorem in probability and statistic
theory, called Strong Law of Large Numbers (SLLN). It states that the
emprical mean, i.e., the mean of samples, will converge to the
expectation of the distribution <i>almost surely</i>. Monte Carlo integration
is actually a direct application of SLLN.
</p>...<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-math.html">math</a> </div><h2 class="post-title"><a href="https://dou-meishi.github.io/org-blog/2024-03-12-AcademicCitationsandAPA/notes.html">Academic Citations and APA Style</a></h2><div class="post-date">12 Mar 2024</div><p>
Citations are important in academic writings, not only for
acknowledging the original authors and avoiding plagiarism, but also
for <i>effective communication</i>. Citations help readers track the
evolution of thought and provide the source where they can retrieve
the original material. Citations can also add strength and authority
to your work. In this post, I will show how to properly cite sources
in academic writings. In particular, I will follow the APA (American
Psychological Association) citation style.
</p>...<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-tool.html">tool</a> </div><div id="archive">
<a href="https://dou-meishi.github.io/org-blog/archive.html">Other posts</a>
</div>
</div>
<div id="postamble" class="status">Created by <a href="https://github.com/bastibe/org-static-blog/">Org Static Blog</a>
</div>
</body>
</html>
