<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title><![CDATA[Dou Meishi's Blog]]></title>
<description><![CDATA[Dou Meishi's Blog]]></description>
<link>https://dou-meishi.github.io/org-blog/</link>
<lastBuildDate>Sun, 27 Apr 2025 20:50:56 +0800</lastBuildDate>
<item>
  <title><![CDATA[Display Unicode Characters in Emacs]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org0ee0048">Backgrounds</a></li>
<li><a href="#org84b25e2">A Simple Solution</a></li>
<li><a href="#use-display-tables-in-emacs">Appendix: Notes on Display Tables</a></li>
<li><a href="#appendix-useful-scripts">Appendix: Useful Scripts</a></li>
<li><a href="#appendix:use-opentype-font-features">Appendix: Use OpenType Font Features</a></li>
<li><a href="#org37842cc">Useful Links</a></li>
</ul>
</div>
</nav>
<p>
<i>Motivation.</i> I prefer Unicode symbols over LaTeX sequences.  In <a href="https://dou-meishi.github.io/org-blog/2024-02-24-EmacsPrettifySymbols/notes.html">this
old post</a> , I discussed how to display LaTeX commands as Unicode
symbols in Emacs. However, after applying these settings, my Emacs
renders both <code>\mathcal{A}</code> and <code>\mathscr{A}</code> identically as the <code>U+1D4D0</code> character from the
<i>Libertinus Math</i> font.
</p>

<p>
After investigation, I figure out that this behavior might be set by
my <code>~/.emacs.d/pretty-symbols.csv</code>, where both LaTeX commands map to the
same Unicode character. I did this because Libertinus Math lacks
dedicated glyphs for <code>\mathcal</code> characters. Indeed, this issue can also
be demonstrated by the following TeX file (compiled by <code>XeLaTeX</code>).
</p>

<div class="org-src-container">
<pre class="src src-latex"><span style="color: #859900; font-weight: bold;">\documentclass</span>{<span style="color: #657b83; font-weight: bold;">article</span>}
<span style="color: #859900; font-weight: bold;">\usepackage</span>{<span style="color: #657b83; font-weight: bold;">unicode-math</span>}
<span style="color: #859900; font-weight: bold;">\begin</span>{<span style="color: #268bd2;">document</span>}
<span style="color: #859900; font-weight: bold;">\setmathfont</span>{Libertinus Math}
Libertinus Math:
<span style="color: #859900; font-weight: bold;">\verb</span>|\mathcal{A}| produces <span style="color: #2aa198;">$\mathcal{A}$</span>;
<span style="color: #859900; font-weight: bold;">\verb</span>|\mathscr{A}| produces <span style="color: #2aa198;">$\mathscr{A}$</span>;
Unicode symbol <span style="color: #859900; font-weight: bold;">\verb</span>|U+1D4D0| produces <span style="color: #2aa198;">$&#120016;$</span>;
Unicode symbol <span style="color: #859900; font-weight: bold;">\verb</span>|U+1D49C| produces <span style="color: #2aa198;">$&#119964;$</span>.
<span style="color: #859900; font-weight: bold;">\end</span>{<span style="color: #268bd2;">document</span>}
</pre>
</div>


<figure id="orgb5974e3">
<img src="./1.png" alt="1.png" width="100%">

</figure>
<div id="outline-container-org0ee0048" class="outline-2">
<h2 id="org0ee0048">Backgrounds</h2>
<div class="outline-text-2" id="text-org0ee0048">
<ol class="org-ol">
<li><p>
How a font selects the glyph for a given code point?
</p>

<p>
<i>Answer.</i> We have to distinguish the concepts of <i>characaters</i>, <i>code
points</i>, <i>glyphs</i> and <i>fonts</i>. See a brief explanation in the
footnote<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>; see also the related references in section <a href="#org37842cc">Useful
Links</a>.
</p>

<p>
In the simplest case, a code point corresponds to a single
glyph. For example, <code>U+0041</code> is displayed as a glyph like <code>A</code> in most
fonts. However, a font might also provide alternative glyphs for
the same code point (e.g., for ligatures), or display multiple code
points with a single glyph (e.g., for accents). See more
discussions in <a href="#appendix:use-opentype-font-features">Appendix: Use OpenType Font Features</a>.
</p></li>

<li><p>
How to customize the appearance of a Unicode symbol in Emacs?
</p>

<p>
<i>Answer.</i> The simplest way is to customize the font for displaying
the Unicode symbol via <code>set-fontset-font</code>. For example, the following
snippet tells Emacs to display <code>U+1D4D0</code> with font <i>STIX Two
Math</i><sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>.
</p>

<div class="org-src-container">
<pre class="src src-elisp">(set-fontset-font <span style="color: #2aa198;">"fontset-default"</span> #x1D4D0 <span style="color: #2aa198;">"STIX Two Math"</span>)
</pre>
</div>

<p>
A slightly complicated yet more flexible way is to specify the
glyph for displaying a unicode character using <a href="https://www.gnu.org/software/emacs/manual/html_node/elisp/Display-Tables.html">Display Tables</a>. For
example, the following snippet tells Emacs to display <code>U+1D4D1</code> with
the glyph corresponding to <code>U+1D4D0</code> in <i>Libertinus Math</i>. See the
following section <a href="#use-display-tables-in-emacs">Appendix: Notes on Display Tables</a> for more
explanations.
</p>

<div class="org-src-container">
<pre class="src src-elisp">(aset standard-display-table #x1D4D1
      (vector (make-glyph-code #x1D4D0 'libertinus-math-face)))
</pre>
</div></li>

<li><p>
How a LaTeX command is displayed as another Unicode symbol?
</p>

<p>
<i>Answer.</i> After toggling <code>prettify-symbols-mode</code>, matched strings will
be composed to a single character defined in the variable
<code>prettify-symbols-alist</code>. See also my post <a href="https://dou-meishi.github.io/org-blog/2024-02-24-EmacsPrettifySymbols/notes.html">Display LaTeX Command with
Unicode Characters in Emacs</a>.
</p></li>
</ol>
</div>
</div>
<div id="outline-container-org84b25e2" class="outline-2">
<h2 id="org84b25e2">A Simple Solution</h2>
<div class="outline-text-2" id="text-org84b25e2">
<p>
Perhaps the simplest solution is <code>set-font-fontset</code>.
</p>

<p>
First, I notice that the glyphs of Libertinus Math in range <code>(#x1D4D0
. #x1D503)</code> are suitable for displaying <code>\mathscr</code> characters. So, I use
the below settings.
</p>

<div class="org-src-container">
<pre class="src src-elisp"><span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">for \mathscr letters</span>
(set-fontset-font <span style="color: #2aa198;">"fontset-default"</span> '(#x1D4D0 . #x1D503) <span style="color: #2aa198;">"Libertinus Math"</span>)
</pre>
</div>

<p>
Then, I find that STIX Two Math contains glyphs for displaying
<code>\mathcal</code> characters. To avoid overriding the <code>\mathscr</code> characters, I
choose to only override code points in range <code>(#x1D49C . #x1D4CF)</code>.
</p>

<div class="org-src-container">
<pre class="src src-elisp"><span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">for \mathcal letters</span>
(set-fontset-font <span style="color: #2aa198;">"fontset-default"</span> '(#x1D49C . #x1D4CF) <span style="color: #2aa198;">"STIX Two Math"</span>)
</pre>
</div>

<p>
Finally, I generate lines for my <code>~/.emacs.d/pretty-symbols.csv</code>
file. Each line consists of the LaTeX command and its Unicode symbol,
and looks like <code>\mathcal{A}, ùíú</code>. See the <a href="#appendix-useful-scripts">Appendix: Useful Scripts</a> for a python
script to generate these lines. It should be noted that some code
points in the range <code>(#x1D49C . #x1D4CF)</code> are glyphless and are manually
replaced. To ensure the consistency, the font to display these code
points has to be manually set by <code>set-fontset-font</code> as well, e.g.,
</p>

<div class="org-src-container">
<pre class="src src-elisp">(set-fontset-font <span style="color: #2aa198;">"fontset-default"</span> #x212C <span style="color: #2aa198;">"STIX Two Math"</span>) <span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">mathcal B</span>
(set-fontset-font <span style="color: #2aa198;">"fontset-default"</span> #x2130 <span style="color: #2aa198;">"STIX Two Math"</span>) <span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">mathcal E</span>
(set-fontset-font <span style="color: #2aa198;">"fontset-default"</span> #x2131 <span style="color: #2aa198;">"STIX Two Math"</span>) <span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">mathcal F</span>
(set-fontset-font <span style="color: #2aa198;">"fontset-default"</span> #x210B <span style="color: #2aa198;">"STIX Two Math"</span>) <span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">mathcal H</span>
(set-fontset-font <span style="color: #2aa198;">"fontset-default"</span> #x2110 <span style="color: #2aa198;">"STIX Two Math"</span>) <span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">mathcal I</span>
(set-fontset-font <span style="color: #2aa198;">"fontset-default"</span> #x2112 <span style="color: #2aa198;">"STIX Two Math"</span>) <span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">mathcal L</span>
(set-fontset-font <span style="color: #2aa198;">"fontset-default"</span> #x2133 <span style="color: #2aa198;">"STIX Two Math"</span>) <span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">mathcal M</span>
(set-fontset-font <span style="color: #2aa198;">"fontset-default"</span> #x211B <span style="color: #2aa198;">"STIX Two Math"</span>) <span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">mathcal R</span>
(set-fontset-font <span style="color: #2aa198;">"fontset-default"</span> #x212F <span style="color: #2aa198;">"STIX Two Math"</span>) <span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">mathcal e</span>
(set-fontset-font <span style="color: #2aa198;">"fontset-default"</span> #x210A <span style="color: #2aa198;">"STIX Two Math"</span>) <span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">mathcal g</span>
(set-fontset-font <span style="color: #2aa198;">"fontset-default"</span> #x2113 <span style="color: #2aa198;">"STIX Two Math"</span>) <span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">mathcal o</span>
</pre>
</div>

<p>
The final result is
<img src="./final-results.png" alt="final-results.png">
</p>
</div>
</div>
<div id="outline-container-use-display-tables-in-emacs" class="outline-2">
<h2 id="use-display-tables-in-emacs">Appendix: Notes on Display Tables</h2>
<div class="outline-text-2" id="text-use-display-tables-in-emacs">
<p>
The section <a href="https://www.gnu.org/software/emacs/manual/html_node/elisp/Display-Tables.html">42.23.2 Display Tables</a> and <a href="https://www.gnu.org/software/emacs/manual/html_node/elisp/Active-Display-Table.html">42.23.3 Active Display Table</a> in
the manual have covered the basics of display tables. Here I add a few
additional notes.
</p>

<p>
First, the function <code>make-glyph-code</code> does not seem to
accept anonymous faces as the value of its <i>FACE</i> argument, though
section <a href="https://www.gnu.org/software/emacs/manual/html_node/elisp/Faces.html">42.12 Faces</a> mentioned that
</p>

<blockquote>
<p>
One way to represent a face is as a property list of attributes, like (:foreground "red" :weight bold). Such a list is called an anonymous face.
</p>
</blockquote>

<p>
Therefore, I have to explicitly define named faces by <code>defface</code>.
</p>

<div class="org-src-container">
<pre class="src src-elisp">(<span style="color: #859900; font-weight: bold;">defface</span> <span style="color: #268bd2;">libertinus-math-face</span>
  '((t <span style="color: #657b83; font-weight: bold;">:family</span> <span style="color: #2aa198;">"Libertinus Math"</span>))
  <span style="color: #2aa198;">"Face for Libertinus Math glyphs."</span>)

(<span style="color: #859900; font-weight: bold;">defface</span> <span style="color: #268bd2;">stix-math-face</span>
  '((t <span style="color: #657b83; font-weight: bold;">:family</span> <span style="color: #2aa198;">"STIX Two Math"</span>))
  <span style="color: #2aa198;">"Face for STIX Two Math glyphs."</span>)
</pre>
</div>

<p>
Second, even if the current active display table maps <code>U+1D4D1</code> to the
glyph described by code <code>U+1D4D0</code> along with a customized face, then the
font family specified in the customized face might be overridden by
fontsets specifications. For example, if the following configurations
are both applied, then <code>U+1D4D1</code> will be displayed as <code>U+1D4D0</code> in <i>STIX
Two Math</i>.
</p>

<div class="org-src-container">
<pre class="src src-elisp">(aset standard-display-table #x1D4D1
      (vector (make-glyph-code #x1D4D0 'libertinus-math-face)))
(set-fontset-font <span style="color: #2aa198;">"fontset-default"</span> #x1D4D0 <span style="color: #2aa198;">"STIX Two Math"</span>)
</pre>
</div>

<p>
You may verify this by <code>describe-char</code> and will see something like this.
<img src="./desc-char.png" alt="desc-char.png">
</p>

<p>
Third, only the active display table takes effect and other display
tables are <i>completely ignored</i>. As described in the manual, the
priority is <i>window display table &gt; buffer display table &gt; standard
display table</i>. In my settings, buffers in Org mode will automatically
register their buffer display tables to display invisible lines as <code>‚Ü∫</code>,
while buffers in other modes seem to use the standard display table.
</p>
</div>
</div>
<div id="outline-container-appendix-useful-scripts" class="outline-2">
<h2 id="appendix-useful-scripts">Appendix: Useful Scripts</h2>
<div class="outline-text-2" id="text-appendix-useful-scripts">
<p>
A python script to write unicode characters and their code points.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">with</span> <span style="color: #657b83; font-weight: bold;">open</span>(<span style="color: #2aa198;">"/tmp/unicode_chars.txt"</span>, <span style="color: #2aa198;">"w"</span>, encoding=<span style="color: #2aa198;">"utf-8"</span>) <span style="color: #859900; font-weight: bold;">as</span> <span style="color: #657b83; font-weight: bold;">file</span>:
    <span style="color: #859900; font-weight: bold;">for</span> code_point <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(0x1D49C, 0x1D4CF + 1):
        <span style="color: #657b83; font-weight: bold;">file</span>.write(f<span style="color: #2aa198;">"</span>{<span style="color: #657b83; font-weight: bold;">chr</span>(code_point)}<span style="color: #2aa198;">, U+</span>{code_point:05X}<span style="color: #268bd2; font-weight: bold;">\n</span><span style="color: #2aa198;">"</span>)
    <span style="color: #859900; font-weight: bold;">for</span> code_point <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(0x1D4D0, 0x1D503 + 1):
        <span style="color: #657b83; font-weight: bold;">file</span>.write(f<span style="color: #2aa198;">"</span>{<span style="color: #657b83; font-weight: bold;">chr</span>(code_point)}<span style="color: #2aa198;">, U+</span>{code_point:05X}<span style="color: #268bd2; font-weight: bold;">\n</span><span style="color: #2aa198;">"</span>)
</pre>
</div>

<p>
A python script to generate lines for
<code>~/.emacs.d/pretty-symbols.csv</code>. Note that some code points in range
<code>(#x1D49C . #x1D4CF)</code> are glyphless. For example, the code point <code>U+1D49D</code>
after <code>U+1D49C ùíú</code> is glyphless in most fonts, as the right code point
for <code>‚Ñ¨</code> is <code>U+212C</code>.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">with</span> <span style="color: #657b83; font-weight: bold;">open</span>(<span style="color: #2aa198;">"/tmp/script_chars.csv"</span>, <span style="color: #2aa198;">"w"</span>, encoding=<span style="color: #2aa198;">"utf-8"</span>) <span style="color: #859900; font-weight: bold;">as</span> <span style="color: #657b83; font-weight: bold;">file</span>:
    <span style="color: #268bd2;">glyphless_char_map</span> = {
        <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">Uppercase substitutions</span>
        0x1D49D: 0x212C,  <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">B</span>
        0x1D4A0: 0x2130,  <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">E</span>
        0x1D4A1: 0x2131,  <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">F</span>
        0x1D4A3: 0x210B,  <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">H</span>
        0x1D4A4: 0x2110,  <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">I</span>
        0x1D4A7: 0x2112,  <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">L</span>
        0x1D4A8: 0x2133,  <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">M</span>
        0x1D4AD: 0x211B,  <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">R</span>

        <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">Lowercase substitutions</span>
        0x1D4BA: 0x212F,  <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">e</span>
        0x1D4BC: 0x210A,  <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">g</span>
        0x1D4C4: 0x2113,  <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">o</span>
    }

    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">generate \mathcal mappings (uppercase)</span>
    <span style="color: #268bd2;">start</span> = 0x1D49C
    <span style="color: #859900; font-weight: bold;">for</span> i <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(26):
        <span style="color: #268bd2;">letter</span> = i + <span style="color: #657b83; font-weight: bold;">ord</span>(<span style="color: #2aa198;">"A"</span>)
        <span style="color: #268bd2;">code</span> = i + start
        <span style="color: #859900; font-weight: bold;">if</span> code <span style="color: #859900; font-weight: bold;">in</span> glyphless_char_map:
            <span style="color: #268bd2;">code</span> = glyphless_char_map[code]
        <span style="color: #657b83; font-weight: bold;">file</span>.write(f<span style="color: #2aa198;">"</span><span style="color: #268bd2; font-weight: bold;">\\</span><span style="color: #2aa198;">mathcal{{</span>{<span style="color: #657b83; font-weight: bold;">chr</span>(letter)}<span style="color: #2aa198;">}}, </span>{<span style="color: #657b83; font-weight: bold;">chr</span>(code)}<span style="color: #268bd2; font-weight: bold;">\n</span><span style="color: #2aa198;">"</span>)

    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">generate \mathcal mappings (lowercase)</span>
    <span style="color: #268bd2;">start</span> = 0x1D4B6
    <span style="color: #859900; font-weight: bold;">for</span> i <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(26):
        <span style="color: #268bd2;">letter</span> = i + <span style="color: #657b83; font-weight: bold;">ord</span>(<span style="color: #2aa198;">"a"</span>)
        <span style="color: #268bd2;">code</span> = i + start
        <span style="color: #859900; font-weight: bold;">if</span> code <span style="color: #859900; font-weight: bold;">in</span> glyphless_char_map:
            <span style="color: #268bd2;">code</span> = glyphless_char_map[code]
        <span style="color: #657b83; font-weight: bold;">file</span>.write(f<span style="color: #2aa198;">"</span><span style="color: #268bd2; font-weight: bold;">\\</span><span style="color: #2aa198;">mathcal{{</span>{<span style="color: #657b83; font-weight: bold;">chr</span>(letter)}<span style="color: #2aa198;">}}, </span>{<span style="color: #657b83; font-weight: bold;">chr</span>(code)}<span style="color: #268bd2; font-weight: bold;">\n</span><span style="color: #2aa198;">"</span>)

    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">generate \mathscr mappings (uppercase)</span>
    <span style="color: #268bd2;">start</span> = 0x1D4D0
    <span style="color: #859900; font-weight: bold;">for</span> i <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(26):
        <span style="color: #268bd2;">letter</span> = i + <span style="color: #657b83; font-weight: bold;">ord</span>(<span style="color: #2aa198;">"A"</span>)
        <span style="color: #657b83; font-weight: bold;">file</span>.write(f<span style="color: #2aa198;">"</span><span style="color: #268bd2; font-weight: bold;">\\</span><span style="color: #2aa198;">mathscr{{</span>{<span style="color: #657b83; font-weight: bold;">chr</span>(letter)}<span style="color: #2aa198;">}}, </span>{<span style="color: #657b83; font-weight: bold;">chr</span>(i + start)}<span style="color: #268bd2; font-weight: bold;">\n</span><span style="color: #2aa198;">"</span>)

    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">generate \mathscr mappings (lowercase)</span>
    <span style="color: #268bd2;">start</span> = 0x1D4EA
    <span style="color: #859900; font-weight: bold;">for</span> i <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(26):
        <span style="color: #268bd2;">letter</span> = i + <span style="color: #657b83; font-weight: bold;">ord</span>(<span style="color: #2aa198;">"a"</span>)
        <span style="color: #657b83; font-weight: bold;">file</span>.write(f<span style="color: #2aa198;">"</span><span style="color: #268bd2; font-weight: bold;">\\</span><span style="color: #2aa198;">mathscr{{</span>{<span style="color: #657b83; font-weight: bold;">chr</span>(letter)}<span style="color: #2aa198;">}}, </span>{<span style="color: #657b83; font-weight: bold;">chr</span>(i + start)}<span style="color: #268bd2; font-weight: bold;">\n</span><span style="color: #2aa198;">"</span>)
</pre>
</div>

<p>
A python script to write all LaTeX commands for verification.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">with</span> <span style="color: #657b83; font-weight: bold;">open</span>(<span style="color: #2aa198;">"/tmp/check_results.org"</span>, <span style="color: #2aa198;">"w"</span>) <span style="color: #859900; font-weight: bold;">as</span> fpr:
    <span style="color: #859900; font-weight: bold;">for</span> i <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(26):
        fpr.write(f<span style="color: #2aa198;">"</span><span style="color: #268bd2; font-weight: bold;">\\</span><span style="color: #2aa198;">mathcal{{</span>{<span style="color: #657b83; font-weight: bold;">chr</span>(i + <span style="color: #657b83; font-weight: bold;">ord</span>('A'))}<span style="color: #2aa198;">}} "</span>)
    fpr.write(<span style="color: #2aa198;">"</span><span style="color: #268bd2; font-weight: bold;">\n</span><span style="color: #2aa198;">"</span>)

    <span style="color: #859900; font-weight: bold;">for</span> i <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(26):
        fpr.write(f<span style="color: #2aa198;">"</span><span style="color: #268bd2; font-weight: bold;">\\</span><span style="color: #2aa198;">mathcal{{</span>{<span style="color: #657b83; font-weight: bold;">chr</span>(i + <span style="color: #657b83; font-weight: bold;">ord</span>('a'))}<span style="color: #2aa198;">}} "</span>)
    fpr.write(<span style="color: #2aa198;">"</span><span style="color: #268bd2; font-weight: bold;">\n</span><span style="color: #2aa198;">"</span>)

    <span style="color: #859900; font-weight: bold;">for</span> i <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(26):
        fpr.write(f<span style="color: #2aa198;">"</span><span style="color: #268bd2; font-weight: bold;">\\</span><span style="color: #2aa198;">mathscr{{</span>{<span style="color: #657b83; font-weight: bold;">chr</span>(i + <span style="color: #657b83; font-weight: bold;">ord</span>('A'))}<span style="color: #2aa198;">}} "</span>)
    fpr.write(<span style="color: #2aa198;">"</span><span style="color: #268bd2; font-weight: bold;">\n</span><span style="color: #2aa198;">"</span>)

    <span style="color: #859900; font-weight: bold;">for</span> i <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(26):
        fpr.write(f<span style="color: #2aa198;">"</span><span style="color: #268bd2; font-weight: bold;">\\</span><span style="color: #2aa198;">mathscr{{</span>{<span style="color: #657b83; font-weight: bold;">chr</span>(i + <span style="color: #657b83; font-weight: bold;">ord</span>('a'))}<span style="color: #2aa198;">}} "</span>)
</pre>
</div>

<p>
A Lisp function to edit a given display and map <code>(#x1D4D0 . #x1D503)</code> to
corresponding glyphs in <i>Libertinus Math</i> and maps <code>(#x1D49C . #x1D4CF)</code>
to glyphs in <code>(#x1D4D0 . #x1D503)</code> in <i>STIX Two Math</i>. Sadly, it seems
that the characters composed by <code>prettify-symbols-mode</code> do not respect
display tables and apply the default fontset anyway.
</p>

<div class="org-src-container">
<pre class="src src-elisp">(<span style="color: #859900; font-weight: bold;">defun</span> <span style="color: #268bd2;">setup-math-display-table</span> (current-display-table)
  <span style="color: #2aa198;">"Configure display table for math script characters."</span>
  (<span style="color: #859900; font-weight: bold;">unless</span> current-display-table
    (<span style="color: #859900; font-weight: bold;">setq</span> current-display-table (make-display-table)))

  <span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">Map \mathscr (U+1D4D0 to U+1D503) via Libertinus</span>
  (<span style="color: #859900; font-weight: bold;">dotimes</span> (i (- #x1D504 #x1D4D0))
    (<span style="color: #859900; font-weight: bold;">let</span> ((code (+ #x1D4D0 i)))
      (aset current-display-table code
            (vector (make-glyph-code code 'libertinus-math-face)))))

  <span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">Map \mathcal (U+1D49C to U+1D4CF) via STIX (offset mapping)</span>
  (<span style="color: #859900; font-weight: bold;">dotimes</span> (i (- #x1D4D0 #x1D49C))
    (<span style="color: #859900; font-weight: bold;">let*</span> ((src-code (+ #x1D49C i))
           (stix-code (+ #x1D4D0 i)))  <span style="color: #93a1a1;">; </span><span style="color: #93a1a1;">Offset mapping</span>
      (aset current-display-table src-code
            (vector (make-glyph-code stix-code 'stix-math-face))))))
</pre>
</div>
</div>
</div>
<div id="outline-container-appendix:use-opentype-font-features" class="outline-2">
<h2 id="appendix:use-opentype-font-features">Appendix: Use OpenType Font Features</h2>
<div class="outline-text-2" id="text-appendix:use-opentype-font-features">
<p>
To support ligature a font might have different versions of <code>i</code> to
display a standalone <i>i</i> and a ligature <i>fi</i>. Besides ligature, many
modern OpenType fonts include optional glyph substitution controlled
by <i>stylistic sets</i>. For example, by default <i>STIX Two Math</i> display
script characters as <code>\mathcal</code> characters and provides the <code>ss01</code>
stylistic set to display <code>\mathscr</code> characters. The <a href="https://github.com/stipub/stixfonts/blob/master/docs">code charts</a> for
<i>STIX Two Math</i> contains detailed descriptions for OpenType features
supported by this font.
</p>

<p>
In XeLaTeX, we can choose the stylistic set by <code>\setmathfont</code>.
</p>

<div class="org-src-container">
<pre class="src src-latex"><span style="color: #859900; font-weight: bold;">\documentclass</span>{<span style="color: #657b83; font-weight: bold;">article</span>}
<span style="color: #859900; font-weight: bold;">\usepackage</span>{<span style="color: #657b83; font-weight: bold;">unicode-math</span>}
<span style="color: #859900; font-weight: bold;">\begin</span>{<span style="color: #268bd2;">document</span>}
<span style="color: #859900; font-weight: bold;">\setmathfont</span>{STIX Two Math}
STIX Two Math:
<span style="color: #859900; font-weight: bold;">\verb</span>|\mathcal{A}| produces <span style="color: #2aa198;">$\mathcal{A}$</span>;
<span style="color: #859900; font-weight: bold;">\verb</span>|\mathscr{A}| produces <span style="color: #2aa198;">$\mathscr{A}$</span>;
Unicode symbol <span style="color: #859900; font-weight: bold;">\verb</span>|U+1D4D0| produces <span style="color: #2aa198;">$&#120016;$</span>;
Unicode symbol <span style="color: #859900; font-weight: bold;">\verb</span>|U+1D49C| produces <span style="color: #2aa198;">$&#119964;$</span>.

<span style="color: #859900; font-weight: bold;">\setmathfont</span>[StylisticSet=1]{STIX Two Math}
STIX Two Math (<span style="color: #859900; font-weight: bold;">\textit</span>{<span style="font-style: italic;">ss01</span>}):
<span style="color: #859900; font-weight: bold;">\verb</span>|\mathcal{A}| produces <span style="color: #2aa198;">$\mathcal{A}$</span>;
<span style="color: #859900; font-weight: bold;">\verb</span>|\mathscr{A}| produces <span style="color: #2aa198;">$\mathscr{A}$</span>;
Unicode symbol <span style="color: #859900; font-weight: bold;">\verb</span>|U+1D4D0| produces <span style="color: #2aa198;">$&#120016;$</span>;
Unicode symbol <span style="color: #859900; font-weight: bold;">\verb</span>|U+1D49C| produces <span style="color: #2aa198;">$&#119964;$</span>.

<span style="color: #859900; font-weight: bold;">\setmathfont</span>{STIX Two Math}
<span style="color: #859900; font-weight: bold;">\setmathfont</span>[StylisticSet=1,range=scr]{STIX Two Math}
STIX Two Math (apply <span style="color: #859900; font-weight: bold;">\textit</span>{<span style="font-style: italic;">ss01</span>} variant for scr characters):
<span style="color: #859900; font-weight: bold;">\verb</span>|\mathcal{A}| produces <span style="color: #2aa198;">$\mathcal{A}$</span>;
<span style="color: #859900; font-weight: bold;">\verb</span>|\mathscr{A}| produces <span style="color: #2aa198;">$\mathscr{A}$</span>;
Unicode symbol <span style="color: #859900; font-weight: bold;">\verb</span>|U+1D4D0| produces <span style="color: #2aa198;">$&#120016;$</span>;
Unicode symbol <span style="color: #859900; font-weight: bold;">\verb</span>|U+1D49C| produces <span style="color: #2aa198;">$&#119964;$</span>.
<span style="color: #859900; font-weight: bold;">\end</span>{<span style="color: #268bd2;">document</span>}
</pre>
</div>


<figure id="orgf606a4e">
<img src="./3.png" alt="3.png">

</figure>

<p>
Unfortunately, it seems that we cannot toggle OpenType features in
Emacs in this way. A workaround is to use <a href="https://github.com/twardoch/fonttools-opentype-feature-freezer">OpenType Feature Freezer</a> to
manually modify a font and let selected features <i>on by default</i>.
</p>
</div>
</div>
<div id="outline-container-org37842cc" class="outline-2">
<h2 id="org37842cc">Useful Links</h2>
<div class="outline-text-2" id="text-org37842cc">
<dl class="org-dl">
<dt>A discussion on choosing a particular glyph</dt><dd>Aad, E. (2015). How to set the glyph of a unicode character? <i>Emacs Stack Exchange.</i> <a href="https://emacs.stackexchange.com/questions/6052/how-to-set-the-glyph-of-a-unicode-character">https://emacs.stackexchange.com/questions/6052/how-to-set-the-glyph-of-a-unicode-character</a></dd>

<dt>Emacs lisp manual covering glyphs and display tables</dt><dd>GNU. (2025). <i>GNU Emacs Lisp Reference Manual</i> (Emacs version 30.1). <a href="https://www.gnu.org/software/emacs/manual/html_node/elisp/Character-Display.html">https://www.gnu.org/software/emacs/manual/html_node/elisp/Character-Display.html</a></dd>

<dt>A post about font basics in Emacs</dt><dd>Idiocy. (2019). Emacs, fonts and fontsets. <a href="https://idiocy.org/emacs-fonts-and-fontsets.html">https://idiocy.org/emacs-fonts-and-fontsets.html</a></dd>

<dt>A Stack Overflow discussion on code points and glyphs</dt><dd>Amery, M. (2014). What's the difference between a character, a code point, a glyph and a grapheme? <i>Stack Overflow.</i> <a href="https://stackoverflow.com/questions/27331819/whats-the-difference-between-a-character-a-code-point-a-glyph-and-a-grapheme">https://stackoverflow.com/questions/27331819/whats-the-difference-between-a-character-a-code-point-a-glyph-and-a-grapheme</a></dd>

<dt>A brief post on distinguishing code points and glyphs</dt><dd>Dkf. (2019). Characters, glyphs, code-points, and byte-sequences. <a href="https://wiki.tcl-lang.org/page/Characters%2C+glyphs%2C+code-points%2C+and+byte-sequences">https://wiki.tcl-lang.org/page/Characters%2C+glyphs%2C+code-points%2C+and+byte-sequences</a></dd>

<dt>A post with in-depth introduction on what are code points and how they are rendered</dt><dd>Litherum. (2017). Relationship Between Glyphs and Code Points. <a href="https://litherum.blogspot.com/2017/05/relationship-between-glyphs-and-code.html">https://litherum.blogspot.com/2017/05/relationship-between-glyphs-and-code.html</a></dd>

<dt>A post about using OpenType feature freezer</dt><dd>Catalfamo, D. (2022). Exploring OpenType Font Features. <a href="https://blog.lambda.cx/posts/opentype-font-exploration/">https://blog.lambda.cx/posts/opentype-font-exploration/</a></dd>
</dl>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
The following explanations are summarized by AI.
</p>
<ol class="org-ol">
<li><i>Code Point.</i> A numerical identifier in the Unicode standard (e.g.,
U+1D4D0 = ùìê).</li>
<li><i>Character.</i> An abstract concept, like "MATHEMATICAL BOLD SCRIPT
CAPITAL A".</li>
<li><i>Glyph.</i> The visual representation (shape) of a character on a
screen.</li>
<li><i>Font.</i> A collection of glyphs and metadata for rendering text.</li>
</ol></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
To list all font families in Emacs, go to a buffer and enable
the <code>lisp-interaction-mode</code>. Then, navigate to the end of the following
form and press <code>C-j</code>.
</p>
<div class="org-src-container">
<pre class="src src-elisp">(mapconcat 'identity (font-family-list) <span style="color: #2aa198;">"\n"</span>)
</pre>
</div>
<p class="footpara">
Once the font family name is obtained, use <code>(describe-font name)</code> to
inspect more info about the font, like the file path.
</p></div></div>


</div>
</div><div class="taglist"></div>
]]></description>
  <link>https://dou-meishi.github.io/org-blog/2025-04-26-DisplayUnicodeCharacters/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2025-04-26-DisplayUnicodeCharacters/notes.html</guid>
  <pubDate>Sat, 26 Apr 2025 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Gradients of Convolution: Direct Computation and Linear Algebra Perspective]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgc92f111">The definition of convolution in deep learning</a></li>
<li><a href="#org6698d49">Method 1: Direct calculation</a></li>
<li><a href="#org015ebba">Notations and linear operators</a></li>
<li><a href="#org4b4aec1">Downsampling and upsampling operators</a></li>
<li><a href="#org254bb5f">Correlation and convolution</a></li>
<li><a href="#orgef7ae65">Method 2: Linear algebra perspective</a></li>
<li><a href="#org552b24b">Transposed convolution in deep learning</a></li>
<li><a href="#org8aaa328">A practical example</a></li>
<li><a href="#org87ed03c">Appendix: Benchmark against PyTorch implementations</a></li>
<li><a href="#org643d552">Appendix: The two-dimensional case</a></li>
<li><a href="#appendix-explanation-convolution-DL">Appendix: Explanation of the convolution-DL</a></li>
</ul>
</div>
</nav>
<p>
Convolution operations are foundational in deep learning for
extracting features in image tasks. Calculating their gradients is
critical for training convolutional neural networks, enabling
backpropagation to update parameters and minimize loss. This post
derives these gradients through two complementary approaches: direct
differentiation of the convolution definition, and a linear algebra
perspective which naturally introduces the <i>transposed convolution</i>
(also known as <i>deconvolution</i>).
</p>

<p>
Before proceeding, we clarify a key terminological nuance: this posts
adopts the definitions of <i>correlation</i> and <i>convolution</i> in signal
processing. These differ from the operation conventionally termed
"convolution" in deep learning frameworks (which technically should be
correlation in signal processing). To avoid ambiguity, we explicitly
append "DL" to deep learning-specific usages (e.g., "convolution-DL")
throughout this post.
</p>

<p>
We begin by giving an explicit definition of the convolution-DL in the
one-dimensional case, which can be easily generalized to
two-dimensional and three dimensional cases.  We then review the
vector-Jacobian product (VJP) framework and apply it to calculate the
gradient of convolution, thereby completing the first part of this
post. To develop the linear algebra approach, we first define the
underlying linear space and some key notations. After that, we
introduce the standard definitions of correlation and convolution in
signal processing. These operators are linear and calculating their
vector-Jacobian products naturally involves considering their adjoint
operators. This motivates the concept of transposed convolution,
defined by the adjoint operator of convolution in deep learning. For
concision, we place some technical details in appendices, including a
detailed explanation of the convolution-DL definition and derivations
for the two-dimensional case. Python code is also included to
benchmark our formulae against the PyTorch implementations.
</p>

<p>
<i>Convention.</i> We index vectors with brackets starting from 0 (e.g.,
\(x[0]\) and \(x[i]\)). Out-of-bounds indices are implicitly evaluated
to 0. For example, a vector \(x\) with 6 elements would treat \(x[-1]\)
and \(x[6]\) as 0.
</p>
<div id="outline-container-orgc92f111" class="outline-2">
<h2 id="orgc92f111">The definition of convolution in deep learning</h2>
<div class="outline-text-2" id="text-orgc92f111">
<p>
The one-dimensional convolution-DL operation is defined by \[ y[t] =
\sum_\tau x[st+\tau-p] \cdot w[\tau], \quad 0 \leq t
\leq \lfloor (I - K + 2p) / s \rfloor. \]
</p>

<p>
Let us briefly explain the symbols in this equation.
</p>

<ul class="org-ul">
<li>The input is \(x[i]\) (\(0 \leq i \leq I - 1\)).</li>
<li>The output is \(y[t]\) (\(0 \leq t \leq T-1\)), where \(T:= 1 + \lfloor (I - K + 2p)
  / s \rfloor\).</li>
<li>The kernel is \(w[\tau]\) (\(0 \leq \tau \leq K - 1\)).</li>
<li>The stride \(s\) is a nonzero integer.</li>
<li>The padding \(p\) is the number of zeros padded to both sides of \(x\).</li>
</ul>

<p>
For simplicity, we denote this function by
\(y=\operatorname{Conv-DL}(x,w;s,p)\) afterwards.  See <a href="#appendix-explanation-convolution-DL">the appendix</a> for
how this compact formulation is obtained.  This definition aligns with
the PyTorch implementation <a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html">torch.nn.Conv1d</a>, except that we ignore the
bias term and do not take linear combination of convolutions over
multiple kernels.
</p>

<p>
In practice, we determine the boundary of the summation index \(\tau\) by
removing zero terms. As invalid indices won't contribute to the sum,
we may require
</p>
$$ \left. \begin{aligned}
&0 \leq st + \tau - p \leq I - 1\\
&0 \leq \tau \leq K - 1
\end{aligned} \right\} \quad \Rightarrow \quad \max(0, p-st) \leq \tau \leq \min(K-1,I-1+p-st). $$

<p>
For the two-dimensional case, we simply do convolution-DL in different
dimensions independently, e.g., \[ y[t_1,t_2] = \sum_{\tau_1,\tau_2}
x[s_1t_1+\tau_1-p_1,s_2t_2+\tau_2-p_2] \cdot w[\tau_1,\tau_2]. \] The boundaries of
indices \(t_1\) and \(t_2\) can be calculated by the previous
formula. The extension to three-dimensional is similar.
</p>
</div>
</div>
<div id="outline-container-org6698d49" class="outline-2">
<h2 id="org6698d49">Method 1: Direct calculation</h2>
<div class="outline-text-2" id="text-org6698d49">
<p>
If we view convolution-DL as a function of the input \(x\) and the
kernel \(w\), then the gradients \(\partial y / \partial x\) and \(\partial y/ \partial w\) would be
matrices, also known as the Jacobian matrices. In deep learning,
however, we rarely calculate Jacobian matrices directly. In most
cases, the purpose is to perform backpropagation from a scalar loss
function and it is sufficient to calculate the vector-Jacobian product
(VJP). Given a vector \(v\) (with the same shape as \(y\)), the VJP is
defined by
</p>
$$ \begin{aligned}
\operatorname{VJP}_\text{Conv-DL}(x,v)[i] &:= \sum_t \frac{\partial y[t]}{\partial x[i]}v[t],\\
\operatorname{VJP}_\text{Conv-DL}(w,v)[\tau] &:= \sum_t \frac{\partial y[t]}{\partial w[\tau]}v[t].
\end{aligned} $$
<p>
Usually, the vector \(v\) is set to the upstream derivative \(\partial L / \partial y\),
i.e., the derivative of the loss \(L\) w.r.t. \(y\). In this case, these
vector-Jacobian products give the gradient \(\partial L / \partial x\) and \(\partial L / \partial
w\), allowing for gradient-based methods to update \(w\) or to further
propagate \(\partial L/\partial x\).
</p>

<p>
For the one-dimensional case, the vector-Jacobian product of convolution-DL w.r.t. the input is
</p>
$$ \begin{aligned}
\operatorname{VJP}_\text{Conv-DL}(x,v)[i] &= \sum_t \frac{\partial y[t]}{\partial x[i]}v[t] \\
&= \sum_t v[t] \frac{\partial }{\partial x[i]} \sum_\tau x[st+\tau-p]\cdot w[\tau] \\
&= \sum_t v[t] \frac{\partial }{\partial x[i]} \sum_{i'} x[i']\cdot w[i'+p-st] \\
&= \sum_t w[i+p-st] \cdot v[t].
\end{aligned} $$
<p>
Noting that only valid indices contribute to the sum, we could
determine the boundary of the summation index \(t\) by
</p>
$$ \left. \begin{aligned}
&0 \leq i + p - st \leq K - 1\\
&0 \leq t \leq T - 1
\end{aligned} \right\} \quad \Rightarrow \quad \max(0, \biggl\lceil\frac{i+p-K+1}{s} \biggr\rceil) \leq t \leq \min(T-1,\biggl\lfloor
\frac{i+p}{s} \biggr\rfloor). $$

<p>
<b>Observation.</b> The vector-Jacobian products of convolution-DL looks like
a convolution-DL of \(v\) and \(w\) with fractional stride \(1/s\). Indeed,
the transposed convolution in deep learning is exactly defined by the
this vector-Jacobian product, which is also called
<i>fractionally-strided convolution</i> or <i>decovolution</i>. We will discuss this
topic in the following sections.
</p>
</div>
</div>
<div id="outline-container-org015ebba" class="outline-2">
<h2 id="org015ebba">Notations and linear operators</h2>
<div class="outline-text-2" id="text-org015ebba">
<p>
Let us consider the space of finite sequences \[\Omega:=\{x\in\mathbb{R}^{\mathbb{Z}}:
\text{only finitely many elements of } x \text{ are nonzero.} \}.\]
Clearly, this is a linear space and we can equip it with the standard
inner product \[\langle x,y\rangle:=\sum_i x[i]\cdot y[i], \quad \forall x,y\in\Omega.\] An operator
\(A\) on \(\Omega\) is said to be <i>linear</i> if it satisfies the additivity and
homogeneity properties, i.e., for any \(x,y\in\Omega\) and any real number \(k\),
\[ A(x+y) = Ax + Ay, \quad A(kx) = k(Ax). \] The adjoint operator
\(A^*\) of \(A\) is defined by the following property \[ \langle Ax, y\rangle = \langle x,
A^*y\rangle, \quad \forall x,y\in\Omega. \]
</p>

<p>
<b>Observation.</b> Once an operator is linear, its vector-Jacobian product
can be directly calculated using its adjoint operator \[
\operatorname{VJP}_A(x,v) := \frac{\partial \langle Ax, v\rangle }{\partial x} = \frac{\partial \langle x,
A^*v\rangle}{\partial x} = A^*v. \]
</p>

<p>
<i>Convention.</i> We use double brackets for closed intervals over integers,
e.g., \(\llbracket-1, 3\rrbracket:=[-1,3]\cap\mathbb{Z}\).
</p>
</div>
</div>
<div id="outline-container-org4b4aec1" class="outline-2">
<h2 id="org4b4aec1">Downsampling and upsampling operators</h2>
<div class="outline-text-2" id="text-org4b4aec1">
<p>
For a nonzero integer \(s\) and an arbitrary integer \(p\), define the
downsampling operator \(D_{s,p}\) and upsampling operator \(U_{s,p}\) by
</p>
$$ \begin{aligned}
(D_{s,p}x)[t] &:= x[st+p], \\
(U_{s,p}x)[t] &:= \begin{cases} x[(t+p)/s], &\quad \text{ if } (t+p)/s \in \mathbb{Z}, \\ 0, &\quad \text{ otherwise}.  \end{cases}
\end{aligned} $$

<p>
We abbreviate the notation to \(D_s\) and \(U_s\) if \(p=0\).
</p>

<p>
Note that both downsampling and upsampling operators are linear on
\(\Omega\). Moreover, they are adjoint operators to each other \[ \langle
D_{s,-p}x, y\rangle = \sum_t x[st-p]\cdot y[t] = \sum_{i\in\{st-p\,|\, t\in\mathbb{Z}\}} x[i] \cdot
y[(i + p) /s] = \langle x, U_{s,p}y\rangle. \] Hence, \(D_{s,p}^* = U_{s,-p}\) and
\(U_{s,p}^* = D_{s,-p}\). Furthermore, they are unitary operators \[
D_{s,p}D_{s,p}^* = D_{s,p}^*D_{s,p} = I, \quad U_{s,p}U_{s,p}^* =
U_{s,p}^*U_{s,p} = I.  \]
</p>

<p>
In particular, the flipping operator \(R:=D_{-1,0}\) and shifting
operator \(S_p:=D_{1,p}\) are also unitary.  Moreover, it holds that
</p>
$$ \begin{aligned}
RS_p&= S_{-p}R, &\quad (RS_p)^* &= RS_p, \\
D_{s,p}&= D_sS_p, &\quad U_{s,p} &= S_pU_s.
\end{aligned} $$
</div>
</div>
<div id="outline-container-org254bb5f" class="outline-2">
<h2 id="org254bb5f">Correlation and convolution</h2>
<div class="outline-text-2" id="text-org254bb5f">
<p>
For any \(x,y\in\Omega\), the correlation and convolution in signal processing are defined by
</p>
$$ \begin{aligned}
\operatorname{Corr}(x,w)[t] &:= \langle S_t x,w \rangle = \sum_\tau x[t+\tau]\cdot w[\tau],\\
\operatorname{Conv}(x,w)[t] &:= \langle  S_t x,Rw  \rangle = \sum_\tau x[t+\tau]\cdot w[-\tau].
\end{aligned} $$

<p>
Fix the kernel \(w\). Both the correlation and convolution are linear
operators w.r.t. \(x\). Due to this reason, we introduce the following
notations \[ C_w x := \operatorname{Corr}(x, w), \quad C^*_w x:=
\operatorname{Conv}(x, w).  \]
</p>

<p>
<b>Observation.</b> Correlation and convolution are adjoint operators \[ \langle
C_w x, y\rangle = \sum_{\tau,t}x[t+\tau]\cdot w[\tau]\cdot y[t] = \sum_{\tau,i}x[i]\cdot y[i-\tau]\cdot w[\tau] = \langle
x, C_w^* y\rangle.  \] Therefore, we could foresee that the vector-Jacobian
product of correlation is convolution and vice versa.
</p>
$$ \begin{aligned}
\operatorname{VJP}_\text{Corr}(x, v) &:= \frac{\partial \langle C_w x , v\rangle}{\partial x} = \frac{\partial \langle x, C_w^* v\rangle}{\partial x} = C_w^* v = \operatorname{Conv}(v, w),\\
\operatorname{VJP}_\text{Conv}(x, v) &:= \frac{\partial \langle C_w^* x, v\rangle}{\partial x} = \frac{\partial \langle x, C_w v\rangle}{\partial x} = C_w v = \operatorname{Corr}(v, w).
\end{aligned} $$

<p>
<b>Observation.</b> Convolution is symmetric w.r.t. its two arguments but
correlation is not \[ C_w^*x=C_x^*w, \quad C_wx = R C_xw. \] Moreover,
convolution is equivalent to correlation with the flipped kernel \(Rw\).
\[ C_w^*x = C_{Rw}x. \] In particular, if the kernel is symmetric,
then the output of correlation and convolution are identical \[ C_w^*x
= C_{Rw}x = C_xw, \quad \text{ if } Rw=w. \]
</p>
</div>
</div>
<div id="outline-container-orgef7ae65" class="outline-2">
<h2 id="orgef7ae65">Method 2: Linear algebra perspective</h2>
<div class="outline-text-2" id="text-orgef7ae65">
<p>
Let us reconsider the convolution operation in deep learning. Rewrite
it with linear operators \[ \operatorname{Conv-DL}(x,w;s,p) \equiv
D_{s,-p}C_wx. \] That is, the convolution-DL is equivalent to first
perform correlation and then downsampling. The vector-Jacobian product
w.r.t. \(x\) is clearly \(C_w^* U_{s,p} v\), i.e., first upsampling the
upstream derivative and then perform correlation with the flipped
kernel; see also <a href="https://github.com/vdumoulin/conv_arithmetic">here</a> for animations of these operations.
</p>

<p>
The <i>transposed convolution</i> in deep learning is defined by this adjoint
operator, \[ \operatorname{TransposedConv-DL}(v, w; s, p):=
C^*_wU_{s,p}v.  \] We may verify that this aligns with our <i>Method 1</i>
</p>
$$ \begin{aligned}
(C^*_wU_{s,p}v)[i]
&= \operatorname{Corr}(U_{s,p}v, Rw)[i] \\
&= \langle S_iS_pU_{s}v, Rw \rangle \\
&= \langle v, D_s S_{-i-p} Rw \rangle \\
&= \langle v, D_s R S_{i+p}w \rangle \\
&= \langle v, D_{-s, i+p}w \rangle \\
&= \sum_t v[t] \cdot w[-st + i + p].
\end{aligned} $$
</div>
</div>
<div id="outline-container-org552b24b" class="outline-2">
<h2 id="org552b24b">Transposed convolution in deep learning</h2>
<div class="outline-text-2" id="text-org552b24b">
<p>
Mathematically, the transposed convolution in deep
learning is defined by the adjoint operator of convolution-DL. As
shown above, the explicit definition is \[ u[i] = \sum_{t=\max(0,
\lceil\frac{i+p-K+1}{s} \rceil)}^{\min(T-1,\lfloor \frac{i+p}{s} \rfloor)} w[i+p-st]\cdot
v[t]. \] Here is a brief review of the notations.
</p>

<ul class="org-ul">
<li>The input is \(v[t]~(0 \leq t \leq T-1)\).</li>
<li>The output is \(u[i]~(0 \leq i \leq I^* - 1)\), where \(I^*:= s(T-1)-2p+K + p^*\).</li>
<li>The kernel is \(w[\tau]~(0 \leq \tau \leq K-1)\).</li>
<li>The (input) padding is \(p\).</li>
<li>The stride is \(s\).</li>
<li>The output padding is \(p^*\).</li>
</ul>

<p>
While the input padding \(p\) and output padding \(p^*\) might initially
seem confusing, it is important to clarify that \(p^*\) serves
exclusively to truncate the support of the result \(u\). Unlike \(p\), the
output padding \(p^*\) does not influence the calculation of \(u[i]\), but
determines the output shape \(I^*\). Below we justify the reason why
\(p^*\) is useful.
</p>

<p>
Note that \(u[i]\) is by definition zero if the summation is void. We
focus on indices \(i\) satisfying \[ \max(0,
\biggl\lceil\frac{i+p-K+1}{s}\biggr\rceil) \leq \min(T-1, \biggl\lfloor \frac{i+p}{s}
\biggr\rfloor). \] Solving this for \(i\) yields \[ -p \leq i \leq
s(T-1)-p+K - 1. \] Indices outside this range would be evaluated
to 0. In practice, however, we expect \(u\) has the same shape as
another tensor and require \(0 \leq i \leq I - 1\), where \(I\) is some positive
integer, e.g., when calculating the vector-Jacobian product of
convolution-DL. To adjust the output shape, we may choose \(p^*\) such
that \(I^*=I\); see also the next section for a concrete example.
</p>
</div>
</div>
<div id="outline-container-org8aaa328" class="outline-2">
<h2 id="org8aaa328">A practical example</h2>
<div class="outline-text-2" id="text-org8aaa328">
<p>
To conclude this post, we use a simple example to demonstrate the
calculation. Given a convolution-DL operation \(y=
\operatorname{Conv-DL}(x,w;s,p)\) and an upstream derivative \(\partial L/\partial y\),
we calculate the gradient \(\partial L/\partial x\) using the transposed convolution
in deep learning.
</p>

<p>
According to the results established in previous sections \[ \frac{\partial
L}{\partial x[i]} = \operatorname{TransposedConv-DL}(\frac{\partial L}{\partial y}, w;
s,p)[i], \quad \forall i. \] The practical issue here is to ensure the
output of the right-hand side matches the shape of \(x\). Specifically,
compute and only compute the indices \(0 \leq i \leq I-1\), where \(I\) is the
length of \(x\). In this case, we may adjust the output padding number
\(p^*\) to align the shape of the output with \(I\). Setting
\(s(T-1)-2p+K + p^* = I\) yields \[p^*=I - s(T-1)+2p-K.\]
</p>

<p>
Here is a simple example to verify our formulae for the output
padding. Let \(I=10,~K=3,~s=2,~p=1\). The length of \(y\) is
\(T=1+\lfloor(I-K+2p)/s \rfloor=5\). Then, the output padding should be \(p^*=1\).
The following script compares the gradient and the result of
transposed convolution.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">import</span> torch

<span style="color: #268bd2;">x</span> = torch.randn(1, 1, 10, requires_grad=<span style="color: #268bd2; font-weight: bold;">True</span>)
<span style="color: #268bd2;">w</span> = torch.randn(1, 1, 3)
<span style="color: #268bd2;">s</span> = 2
<span style="color: #268bd2;">p</span> = 1

<span style="color: #268bd2;">y</span> = torch.nn.functional.conv1d(x, w, stride=s, padding=p)
<span style="color: #268bd2;">v</span> = torch.rand_like(y, requires_grad=<span style="color: #268bd2; font-weight: bold;">False</span>)

<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">calculate the gradient by autograd</span>
<span style="color: #268bd2;">loss</span> = torch.<span style="color: #657b83; font-weight: bold;">sum</span>(v * y)
loss.backward()

<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">calculate the gradient manually by transposed convolution</span>
<span style="color: #859900; font-weight: bold;">with</span> torch.no_grad():
    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">output_padding = x.shape[-1] - s * (y.shape[-1] - 1) + 2 * p - w.shape[-1]</span>
    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">it equals to 1 in this case</span>
    <span style="color: #268bd2;">output_padding</span> = 1
    <span style="color: #268bd2;">u</span> = torch.nn.functional.conv_transpose1d(
        v, w, stride=s, padding=p, output_padding=output_padding
    )

<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">compare these two results</span>
<span style="color: #859900; font-weight: bold;">assert</span> torch.allclose(x.grad, u)
<span style="color: #657b83; font-weight: bold;">print</span>(<span style="color: #2aa198;">"Test passed."</span>)
</pre>
</div>

<p>
Implementation details of convolution-DL and transposed convolution
are left in the appendix.
</p>
</div>
</div>
<div id="outline-container-org87ed03c" class="outline-2">
<h2 id="org87ed03c">Appendix: Benchmark against PyTorch implementations</h2>
<div class="outline-text-2" id="text-org87ed03c">
<p>
To validate our formulae for convolution and transposed convolution in
deep learning, we implement them in Python and benchmark them against
PyTorch implementations <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.conv1d.html">conv1d</a> and <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.conv_transpose1d.html">convTranspose1d</a>. The complete
script is available <a href="./code/test_conv.py">here</a>. Running this test script confirms that our
formulae are consistent with PyTorch implementations.
</p>

<p>
For convolution in deep learning, we use the following formula \[ y[t]
= \sum_{\tau=\max(0,p-st)}^{\min(K-1,I-1+p-st)} x[st+\tau -p] \cdot w[\tau], \quad 0 \leq t \leq \lfloor (I -K +2p) /s\rfloor. \] Here,
\(x[i]\) (\(0\leq i \leq I-1\)) is the input and \(w[\tau]\) (\(0\leq \tau\leq K-1\)) is the
kernel. The nonzero integer \(s\) is the stride and the integer \(p\) is
the number of zeros padded to both side of \(x\).
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">my_conv1d_DL</span>(x: np.ndarray, w: np.ndarray, s: <span style="color: #657b83; font-weight: bold;">int</span> = 1, p: <span style="color: #657b83; font-weight: bold;">int</span> = 0) -&gt; np.ndarray:
    <span style="color: #859900; font-weight: bold;">assert</span> x.ndim == 1 <span style="color: #859900; font-weight: bold;">and</span> w.ndim == 1
    <span style="color: #268bd2;">I</span>, <span style="color: #268bd2;">K</span> = x.shape[-1], w.shape[-1]
    <span style="color: #268bd2;">y</span> = []
    <span style="color: #859900; font-weight: bold;">for</span> t <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(1 + math.floor((I - K + 2 * p) / s)):
        <span style="color: #268bd2;">end</span> = <span style="color: #657b83; font-weight: bold;">min</span>(K - 1, I - 1 + p - s * t)
        <span style="color: #268bd2;">start</span> = <span style="color: #657b83; font-weight: bold;">max</span>(0, p - s * t)
        y.append(<span style="color: #657b83; font-weight: bold;">sum</span>(w[tau] * x[s * t - p + tau] <span style="color: #859900; font-weight: bold;">for</span> tau <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(start, end + 1)))
    <span style="color: #859900; font-weight: bold;">return</span> np.stack(y, axis=-1)
</pre>
</div>

<p>
For transposed convolution in deep learning, we use the following
formula \[ u[i] = \sum_{t=\max(0, \lceil\frac{i+p-K+1}{s} \rceil)}^{\min(T-1,\lfloor
\frac{i+p}{s} \rfloor)} w[i+p-st]\cdot v[t], \quad 0 \leq i \leq s(T-1)-2p+K-1+p^* . \]
</p>

<p>
Here, \(v[t]\) (\(0\leq t \leq T-1\)) is the input and \(w[\tau]\) (\(0\leq \tau\leq K-1\)) is
the kernel. The nonzero integer \(s\) is the stride, the integer \(p\) is
the padding, and the integer \(p^*\) is the output padding.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">my_convtransposed1d_DL</span>(
    v: np.ndarray,
    w: np.ndarray,
    s: <span style="color: #657b83; font-weight: bold;">int</span> = 1,
    p: <span style="color: #657b83; font-weight: bold;">int</span> = 0,
    pstar: <span style="color: #657b83; font-weight: bold;">int</span> = 0,
) -&gt; np.ndarray:
    <span style="color: #859900; font-weight: bold;">assert</span> v.ndim == 1 <span style="color: #859900; font-weight: bold;">and</span> w.ndim == 1
    <span style="color: #268bd2;">T</span>, <span style="color: #268bd2;">K</span> = v.shape[-1], w.shape[-1]
    <span style="color: #268bd2;">u</span> = []
    <span style="color: #859900; font-weight: bold;">for</span> i <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(s * (T - 1) - 2 * p + K + pstar):
        <span style="color: #268bd2;">end</span> = <span style="color: #657b83; font-weight: bold;">min</span>(T - 1, math.floor((i + p) / s))
        <span style="color: #268bd2;">start</span> = <span style="color: #657b83; font-weight: bold;">max</span>(0, math.ceil((i + p - K + 1) / s))
        u.append(<span style="color: #657b83; font-weight: bold;">sum</span>(v[t] * w[i + p - s * t] <span style="color: #859900; font-weight: bold;">for</span> t <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(start, end + 1)))
    <span style="color: #859900; font-weight: bold;">return</span> np.stack(u, axis=-1)
</pre>
</div>
</div>
</div>
<div id="outline-container-org643d552" class="outline-2">
<h2 id="org643d552">Appendix: The two-dimensional case</h2>
<div class="outline-text-2" id="text-org643d552">
<p>
Recall that the two-dimensional convolution-DL operation is defined by
\[ y[t_1,t_2] = \sum_{\tau_1,\tau_2} x[s_1t_1+\tau_1-p_1,s_2t_2+\tau_2-p_2] \cdot
w[\tau_1,\tau_2]. \]
Direct differentiation yields
</p>
$$ \begin{aligned}
\operatorname{VJP}_\text{Conv-DL}(x,v)[i_1,i_2] &= \sum_{t_1,t_2} \frac{\partial y[t_1,t_2]}{\partial x[i_1,i_2]}v[t_1,t_2] \\
&= \sum_{t_1,t_2} v[t_1,t_2] \frac{\partial }{\partial x[i_1,i_2]} \sum_{\tau_1,\tau_2} x[s_1t_1+\tau_1-p_1,s_2t_2+\tau_2-p_2]\cdot w[\tau_1,\tau_2] \\
&= \sum_{t_1,t_2} v[t_1,t_2] \frac{\partial }{\partial x[i_1,i_2]} \sum_{i'_1,i'_2} x[i'_1,i'_2]\cdot w[i'_1+p_1-s_1t_1,i'_2+p_2-s_2t_2] \\
&= \sum_{t_1,t_2} w[i_1+p_1-s_1t_1,i_2+p_2-s_2t_2] \cdot v[t_1,t_2].
\end{aligned} $$
<p>
The boundary of the summation index \(t_\alpha~(\alpha=1,2)\) can be determined by
</p>
$$ \left. \begin{aligned}
&0 \leq i_\alpha + p_\alpha - s_\alpha t_\alpha \leq K_\alpha - 1\\
&0 \leq t_\alpha \leq T_\alpha - 1
\end{aligned} \right\} \quad \Rightarrow \quad \max(0, \biggl\lceil\frac{i_\alpha+p_\alpha-K_\alpha+1}{s_\alpha} \biggr\rceil) \leq t_\alpha \leq \min(T_\alpha-1,\biggl\lfloor
\frac{i_\alpha+p_\alpha}{s_\alpha} \biggr\rfloor). $$

<p>
To apply the linear algebra method, we should define the linear space
first \[ \Omega(\mathbb{Z}^2):= \{x:\mathbb{Z}^2 \to \mathbb{R}\, |\,\exists E \subset \mathbb{Z}^2,~~E \text{ is finite and
for any } \eta\not\in E,~~ x[\eta]=0.  \}. \] We also equip it with the
standard inner product \[ \langle x,y \rangle := \sum_{\eta \in \mathbb{Z}^2} x[\eta]\cdot y[\eta], \quad \forall
x,y \in \Omega(\mathbb{Z}^2). \] For any \(w \in \Omega(\mathbb{Z}^2)\), define the correlation operator
and convolution operator by
</p>
$$ \begin{aligned}
(C_wx)[t_1,t_2] &:= \operatorname{Corr}(x,w)[t_1,t_2] = \langle S_{t_1;t_2}x, w \rangle,\\
(C_w^*x)[t_1,t_2] &:= \operatorname{Conv}(x,w)[t_1,t_2] = \langle S_{t_1;t_2}x, Rw \rangle.
\end{aligned} $$
<p>
Then, we rewrite the two-dimensional convolution-DL by linear
operators \[ \operatorname{Conv-DL}(x,w;s_1,p_1;s_2,p_2) \equiv
D_{(s_1,-p_1);(s_2,-p_2)} C_w x. \] Its adjoint operator, i.e., the
two-dimensional transpoed convolution in deep learning, is \[
\operatorname{TransposedConv-DL}(v,w;s_1,p_1;s_2,p_2):=C_w^*U_{(s_1,p_1);(s_2,p_2)}v.
\]
</p>
</div>
</div>
<div id="outline-container-appendix-explanation-convolution-DL" class="outline-2">
<h2 id="appendix-explanation-convolution-DL">Appendix: Explanation of the convolution-DL</h2>
<div class="outline-text-2" id="text-appendix-explanation-convolution-DL">
<ol class="org-ol">
<li><p>
How to do valid correlation?
</p>

<p>
\[ y[t] = \sum_\tau x[t+\tau] \cdot w[\tau].\]
</p>

<p>
We require that during the summation these indices remain valid, i.e.,
remain in their support respectively.  \[ \forall t\in
   \operatorname{Supp}(y),\forall \tau\in \operatorname{Supp}(w), t+\tau \in
   \operatorname{Supp}(x).  \] Solving this yields \[ t +0 \geq 0, \quad t +
   K-1 \leq I-1.  \] Hence, we have \(0 \leq t \leq I-K\).
</p></li>

<li><p>
How to do valid correlation with padding?
</p>

<p>
After padding, the valid indices of \(x\) is enlarged to \(\llbracket -p,
   I-1+p\rrbracket\), leading to \[ t+0 \geq -p, \quad t+K-1\leq I-1+p. \] Then, we
have \(-p \leq t \leq I - K + p\). Thus, we should correct the formula to
\[ y[t] = \tilde{y}[t-p] = \sum_\tau x[t+\tau-p]\cdot w[\tau], \quad 0 \leq t \leq I -
   K + 2p.  \]
</p></li>

<li><p>
How to do valid correlation with both padding and stride?
</p>

<p>
Notice that convolution with stride is equivalent to first
performing a convolution with unit stride and then downsampling the
result by a factor of \(s\)
</p>

$$ \begin{aligned}
z[t] &= \sum_\tau x[t + \tau - p]\cdot w[\tau], \quad 0 \leq t \leq I -K + 2p,\\
y[t] &= z[ st ], \quad 0 \leq t \leq \lfloor (I -K +2p) /s\rfloor.
\end{aligned} $$</li>
</ol>

<p>
Putting it together, we have
\[ y[t] = \sum_\tau x[st+\tau -p] \cdot w[\tau], \quad 0 \leq t \leq \lfloor (I -K +2p) /s\rfloor. \]
</p>
</div>
</div>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-ai.html">ai</a> </div>
]]></description>
  <category><![CDATA[ai]]></category>
  <link>https://dou-meishi.github.io/org-blog/2025-02-20-ConvMathVisualCode/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2025-02-20-ConvMathVisualCode/notes.html</guid>
  <pubDate>Thu, 20 Feb 2025 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Add A Comment Section to My Blog with Giscus]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org7e2cf3e">Giscus and its basic usage</a></li>
<li><a href="#org13413cc">Use giscus in my blog</a></li>
<li><a href="#org30956a1">References</a></li>
</ul>
</div>
</nav>
<p>
Early this year, I created this blog site and share some well-written
notes on it. Since then, I really enjoy blogging and continue to post
interesting things I learn. Recently, I came across the <a href="https://github.com/giscus/giscus/tree/main">giscus</a>
project, a comment system powered by GitHub Discussions, which allows
me to add a comment section to my posts. It seems to work well on my
site after trying it, so I believe it presents a good opportunity to
enhance this site further and encourage more exchanges of ideas.
</p>

<p>
In this post, I will give a brief introduction to <code>giscus</code> and explain
how I use it on this site.
</p>
<div id="outline-container-org7e2cf3e" class="outline-2">
<h2 id="org7e2cf3e">Giscus and its basic usage</h2>
<div class="outline-text-2" id="text-org7e2cf3e">
<p>
Several years ago, GitHub released the GitHub Discussion feature that
provides smoother experiences of iteracting with the audience other
than GitHub Issues. This motivates the <code>giscus</code> project, which takes
advantage of the GitHub Discussion search API to find the Discussion
associated with a page and display them in a comment
section. Therefore, visitors can sign in with their GitHub account and
leave comments in this section provided by <code>giscus</code>, which were actually
posted as comments in GitHub Discussion.
</p>

<p>
To enable <code>giscus</code> comments on a website, we basically need to do
several things.
</p>

<ol class="org-ol">
<li>Select a <i>public</i> GitHub repository which has enabled the GitHub
Discussion feature to hold comments. We can use a new repository
or a existing repository, as long as it is public.</li>

<li>Install giscus as a GitHub APP from <a href="https://github.com/apps/giscus">this page</a> and allow it to
access and modify data in the repository.</li>

<li>Configure how <code>giscus</code> maches pages and comments by going through
intructions on <a href="https://giscus.app/">this page</a>.  After configuration, <code>giscus</code> will give a
short script that can be embedded into our website's template,
appearing as a comment section.</li>

<li>Paste or integrate the generated code into our website's
template. This step depends on how we create the site. I will give
a detailed example for my site later.</li>

<li>Configure <code>giscus.json</code>. To ensure that <code>giscus</code> works properly on our
website, we may need to create a <code>giscus.json</code> file at the root of
the repository. This file can specify the domains that can load
<code>giscus</code>; refer to <a href="https://github.com/giscus/giscus/blob/main/ADVANCED-USAGE.md">their documentation</a>. I will also give an example
later that I use on my site.</li>
</ol>
</div>
</div>
<div id="outline-container-org13413cc" class="outline-2">
<h2 id="org13413cc">Use giscus in my blog</h2>
<div class="outline-text-2" id="text-org13413cc">
<p>
First, I choose the current blog repository to hold both posts and
comments. Then, I install and configure <code>giscus</code> and obtain the snippet
to be used, which is stored in <code>./static/giscus.html</code> and looks like
</p>

<div class="org-src-container">
<pre class="src src-html">&lt;<span style="color: #268bd2;">p</span>&gt;Enjoyed the read? Like and share your thoughts below. Your feedback matters!&lt;/<span style="color: #268bd2;">p</span>&gt;

&lt;<span style="color: #268bd2;">script</span> <span style="color: #268bd2;">src</span>=<span style="color: #2aa198;">"https://giscus.app/client.js"</span>
        <span style="color: #268bd2;">data-repo</span>=<span style="color: #2aa198;">"Dou-Meishi/org-blog"</span>
        <span style="color: #268bd2;">data-repo-id</span>=<span style="color: #2aa198;">"R_kgDOLJfSOw"</span>
        <span style="color: #268bd2;">data-category</span>=<span style="color: #2aa198;">"Announcements"</span>
        <span style="color: #268bd2;">data-category-id</span>=<span style="color: #2aa198;">"DIC_kwDOLJfSO84CkxDd"</span>
        <span style="color: #268bd2;">data-mapping</span>=<span style="color: #2aa198;">"pathname"</span>
        <span style="color: #268bd2;">data-strict</span>=<span style="color: #2aa198;">"0"</span>
        <span style="color: #268bd2;">data-reactions-enabled</span>=<span style="color: #2aa198;">"1"</span>
        <span style="color: #268bd2;">data-emit-metadata</span>=<span style="color: #2aa198;">"0"</span>
        <span style="color: #268bd2;">data-input-position</span>=<span style="color: #2aa198;">"bottom"</span>
        <span style="color: #268bd2;">data-theme</span>=<span style="color: #2aa198;">"light"</span>
        <span style="color: #268bd2;">data-lang</span>=<span style="color: #2aa198;">"en"</span>
        <span style="color: #268bd2;">crossorigin</span>=<span style="color: #2aa198;">"anonymous"</span>
        async&gt;
&lt;/<span style="color: #268bd2;">script</span>&gt;
</pre>
</div>

<p>
The <code>data-repo</code> refers to the public repo that enables GitHub Discussion
and is used to store comments. The <code>data-mapping</code> is set to <code>pathname</code>,
which means <code>giscus</code> will match a page with the comments in GitHub
Discussion by searching its pathname.
</p>

<p>
As I showed in <a href="../2024-01-22-TryOrgStaticBlog/notes.html">the previous post</a>, this site is generated by
<a href="https://github.com/bastibe/org-static-blog/tree/master">Org-Static-Blog</a>, a static site generator using org-mode. To add a
comment section at the end of each post, I simply set the variable
<code>org-static-blog-post-comments</code> to the content of <code>./static/giscus.html</code>,
i.e.,
</p>

<div class="org-src-container">
<pre class="src src-lisp">(setq org-static-blog-post-comments (<span style="color: #859900; font-weight: bold;">with-temp-buffer</span>
  (insert-file-contents (format <span style="color: #2aa198;">"%sstatic/giscus.html"</span> dms/org-static-blog-root-dir))
  (buffer-string)))
</pre>
</div>

<p>
Finally, I create a <code>giscus.json</code> at the root of my blog repository to
allow <code>giscus</code> operates on this site.
</p>

<div class="org-src-container">
<pre class="src src-js">{
  <span style="color: #2aa198;">"origins"</span>: [
      <span style="color: #2aa198;">"https://dou-meishi.github.io/org-blog"</span>
   ],
  <span style="color: #2aa198;">"originsRegex"</span>: [
      <span style="color: #2aa198;">"https://localhost:[0-9]+"</span>,
  ],
  <span style="color: #2aa198;">"defaultCommentOrder"</span>: <span style="color: #2aa198;">"newest"</span>
}
</pre>
</div>

<p>
That's it! Every post generated by <code>org-static-blog</code> will now display a
comment section. Visitors can login with GitHub account and leave
their comments.
</p>
</div>
</div>
<div id="outline-container-org30956a1" class="outline-2">
<h2 id="org30956a1">References</h2>
<div class="outline-text-2" id="text-org30956a1">
<ul class="org-ul">
<li>Giscus (2021). Giscus. GitHub. <a href="https://github.com/giscus/giscus">https://github.com/giscus/giscus</a></li>
<li>Zhauniarovich Y. (2023). Giscus: The New Commenting Engine for My Website. <a href="https://zhauniarovich.com/post/2021/2021-06-giscus/">https://zhauniarovich.com/post/2021/2021-06-giscus/</a></li>
</ul>
</div>
</div>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-emacs.html">emacs</a> </div>
]]></description>
  <category><![CDATA[emacs]]></category>
  <link>https://dou-meishi.github.io/org-blog/2024-12-01-UseGiscusInBlog/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2024-12-01-UseGiscusInBlog/notes.html</guid>
  <pubDate>Sun, 01 Dec 2024 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Practical Einops: Tensor Operations Based on Indices]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orge036a52">A quick example for summations</a></li>
<li><a href="#orgd511de3">Another example for permutations</a></li>
<li><a href="#org061c370">More features provided by einops</a></li>
<li><a href="#org9870668">References</a></li>
</ul>
</div>
</nav>
<p>
People are familiar with vectors and matrices operations but are less
familiar with tensor operations. In machine learning, <i>tensors</i> often
refer to batched vectors or batched matrices and are represented by an
array-like object with multiple indices. Due to this reason, tensors
operations in most Python packages, including NumPy, PyTorch and
TensorFlow, are typically named after vectors and matrices operations.
However, tensors themselves have a particular useful operation, called
<i>contraction</i>, which uses index-based notations and can cover most
vectors and matrices operations. This index-based notations
intuitively and verbosely describe the relationship between the
components of input and output tensors. Today's topic, the Python's
<a href="https://github.com/arogozhnikov/einops">einops</a> package, extends these notations and provides an elegant API
for flexible and powerful tensor operations.
</p>

<p>
<i>Notations.</i> In this post, we use letters to denote tensors, for
example, \(x, y\), and use brackets to denote their components, such as
\(x[i,j,k], y[i,j]\). With a slight abuse of notation, we may also use
\(x[i,j,k]\) directly to denote a tensor with three indices.
</p>
<div id="outline-container-orge036a52" class="outline-2">
<h2 id="orge036a52">A quick example for summations</h2>
<div class="outline-text-2" id="text-orge036a52">
<p>
Consider the batched bilinear form \(x^\intercal Q y\) for batched vectors \(x\),
\(y\) and batched matrices \(Q\). These calculations often arise when
dealing with stochastic sequential data, e.g., \(x\), \(y\) and \(Q\) are
elements of stochastic processes and \(x[i,j,k]\) stands for the value
of the \(k\)-th component at time step \(i\) for the \(j\)-th sample
path. With the index-based notations, the output tensor can be written
as \[ \mathsf{BatchedBilinearForm}(x, Q, y)= \sum_{k,l} x[i,j,k]\,
Q[i,j,k,l]\, y[i,j,l], \] which is a tensor with two indices \(i\) and \(j\)
as the summation is performed over \(k\) and \(l\). The following code
demonstrates how to do this with and without <code>einops</code> in for PyTorch
tensors.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">without einops</span>
<span style="color: #268bd2;">output</span> = (x * (Q @ y.unsqueeze(-1)).squeeze(-1)).<span style="color: #657b83; font-weight: bold;">sum</span>(dim=-1)
<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">with einops</span>
<span style="color: #268bd2;">output</span> = einops.einsum(x, Q, y, <span style="color: #2aa198;">"i j k, i j k l, i j l -&gt; i j"</span>)
</pre>
</div>

<p>
From this example, we can perceive some benefits and drawbacks of
<code>einops</code> already.
</p>

<ol class="org-ol">
<li>It gives the input and output shapes explicitly. The indices where
the summations occur are deduced from these shapes. Therefore,
while using and reading operations described by <code>einops</code> we can have
a crystal-clear understanding of the involved tensors shapes.</li>
<li>It offers a unified way to perform summation operations, including
but not limited to <code>torch.dot</code>, <code>torch.bmm</code>, <code>torch.sum</code>, and so on.</li>
<li>It describes the operations by indices directly, which is not so
transparent if we want to translate it back to vector and matrix
operations.</li>
<li>It describes the operations by a string following specific
patterns, which might be very confusing for people who are
unfamiliar with it.</li>
<li>It might be inefficient and slow since it needs to parse a
string. The decreased performance might not be significant if the
same pattern is used multiple times with caching techniques</li>
</ol>

<p>
Despite the last performance issue, <code>einops</code> provides an <i>alternative</i> way
to describe tensor operations based on indices. The pattern string
gives the input and output shapes but requires additional effort to
learn its usage.
</p>
</div>
</div>
<div id="outline-container-orgd511de3" class="outline-2">
<h2 id="orgd511de3">Another example for permutations</h2>
<div class="outline-text-2" id="text-orgd511de3">
<p>
The index-based notation is also intuitive for axis
permutations. Consider a tensor \(x[t,b,c,i]\) with the shape of <code>(time,
batch, channel, feature)</code>. If we want to permute the axes and expect
the output tensor \(y\) to have a shape of <code>(channel, feature, batch,
time)</code>, then we effectively mean \[ x[t,b,c,i] = y[c,i,b,t]. \] The
following code demonstrates how to do this with and without <code>einops</code> in
for PyTorch tensors.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">without einops</span>
<span style="color: #268bd2;">y</span> = x.permute(2, 3, 1, 0)
<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">with einops</span>
<span style="color: #268bd2;">y</span> = einops.rearrange(x, <span style="color: #2aa198;">"t b c i -&gt; c i b t"</span>)
</pre>
</div>

<p>
Once again, <code>einops</code> is more intuitive as it expilcitly specifies the
input and output shapes.
</p>
</div>
</div>
<div id="outline-container-org061c370" class="outline-2">
<h2 id="org061c370">More features provided by einops</h2>
<div class="outline-text-2" id="text-org061c370">
<p>
In fact, both NumPy and PyTorch provide a routine function <code>einsum</code>,
which is actually the motivation behind <code>einops.einsum</code>. The two
examples given above can also be achieved by <code>torch.einsum</code>
directly. However, the einops package extends the idea further, and
provide more advanced features. Below are some usage scenarios that I
believe might be useful. There are also official tutorials and
examples on <a href="https://github.com/arogozhnikov/einops">github</a>.
</p>

<ul class="org-ul">
<li><p>
<i>Use ellipsis</i>. For the first example on the batched bilinear form,
the demonstrated code with <code>einops</code> is slightly restrictive than the
pure PyTorch approach. Indeed, the pattern given there explicitly
specifies that the input tensor \(x\) has three indices. This level of
specification may be desired based on requirements. But if we want
to remove this restriction, then the code can be modified to
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #268bd2;">output</span> = einops.einsum(x, Q, y, <span style="color: #2aa198;">"... k, ... k l, ... l -&gt; ..."</span>)
</pre>
</div></li>

<li><p>
<i>Reshape and check axis sizes.</i> For the second example on axis
permutations, we can also explicitly specify axis sizes and let
<code>einops</code> to check, say,
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #268bd2;">y</span> = einops.rearrange(x, <span style="color: #2aa198;">"t b c i -&gt; c i b t"</span>, t=10, c=30)
<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">this is equivalent to</span>
<span style="color: #268bd2;">t</span>, <span style="color: #268bd2;">b</span>, <span style="color: #268bd2;">c</span>, <span style="color: #268bd2;">i</span> = x.size()
<span style="color: #859900; font-weight: bold;">assert</span> t == 10 <span style="color: #859900; font-weight: bold;">and</span> c == 30
<span style="color: #268bd2;">y</span> = einops.rearrange(x, <span style="color: #2aa198;">"t b c i -&gt; c i b t"</span>)
</pre>
</div></li>

<li><p>
<i>Split axes</i>. Sometimes, we may want to split each image into 4
pieces. For example, the following code demonstrates how to take an
input tensor with the shape of <code>(b, c, h, w)</code> and return a tensor with
the shape of <code>(b, c, 2, 2, h2, w2)</code>
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">split_patches_without_einops</span>(x: torch.Tensor) -&gt; torch.Tensor:
    <span style="color: #268bd2;">b</span>, <span style="color: #268bd2;">c</span>, <span style="color: #268bd2;">h</span>, <span style="color: #268bd2;">w</span> = x.size()
    <span style="color: #268bd2;">y</span> = x.view(b, c, h // 2, 2, w // 2, 2)
    <span style="color: #859900; font-weight: bold;">return</span> y.permute(0, 1, 3, 5, 2, 4)

<span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">split_patches_with_einops</span>(x: torch.Tensor) -&gt; torch.Tensor:
    <span style="color: #859900; font-weight: bold;">return</span> einops.rearrange(x, <span style="color: #2aa198;">"b c (h s1) (w s2) -&gt; b c s1 s2 h w"</span>, s1=2, s2=2)
</pre>
</div></li>

<li><p>
<i>Join axes</i>. Sometimes, we may want to flatten a tensor by joining
multiple axes. For example, the following code demonstrates how to
take an input tensor with the shape of <code>(b, c, h, w)</code> and return a
tensor with the shape of <code>(b, c*h*w)</code>
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">join_axes_without_einops</span>(x: torch.Tensor) -&gt; torch.Tensor:
    <span style="color: #268bd2;">b</span>, <span style="color: #268bd2;">c</span>, <span style="color: #268bd2;">h</span>, <span style="color: #268bd2;">w</span> = x.size()
    <span style="color: #859900; font-weight: bold;">return</span> x.view(b, c * h * w)

<span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">join_axes_with_einops</span>(x: torch.Tensor) -&gt; torch.Tensor:
    <span style="color: #859900; font-weight: bold;">return</span> einops.rearrange(x, <span style="color: #2aa198;">"b c h w -&gt; b (c h w)"</span>)
</pre>
</div></li>

<li><p>
<i>Layer</i>. It is possible to create an <code>torch.nn.Module</code> instance for an
<code>einops.rearrange</code> operation and put it into the <code>torch.nn.Sequential</code>
container. For example, the following code demonstrates how to build
a simple image classifier. Note that the first layer is included to
check axis sizes and can be skipped.
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">from</span> einops.layers.torch <span style="color: #859900; font-weight: bold;">import</span> Rearrange

<span style="color: #268bd2;">model</span> = torch.nn.Sequential(
    Rearrange(<span style="color: #2aa198;">"b c h w -&gt; b c h w"</span>, c=3, h=8, w=8),
    torch.nn.Conv2d(3, 16, 3, stride=2, padding=1),
    Rearrange(<span style="color: #2aa198;">"b c h w -&gt; b (c h w)"</span>, c=16, h=4, w=4),
    torch.nn.Linear(16 * 4 * 4, 120),
    torch.nn.ReLU(),
    torch.nn.Linear(120, 10),
)
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org9870668" class="outline-2">
<h2 id="org9870668">References</h2>
<div class="outline-text-2" id="text-org9870668">
<ul class="org-ul">
<li>Rogozhnikov A. (2018). Einops. GitHub. <a href="https://github.com/arogozhnikov/einops">https://github.com/arogozhnikov/einops</a></li>
<li>Duran-Martin. G. (2021). Einsums in the wild. Notion. <a href="https://grrddm.notion.site/Einsums-in-the-wild-bd773f01ba4c463ca9e4c1b5a6d90f5f#3cc76f8130ac4a348888f531069f7c8a">https://grrddm.notion.site/Einsums-in-the-wild-bd773f01ba4c463ca9e4c1b5a6d90f5f#3cc76f8130ac4a348888f531069f7c8a</a></li>
<li>Noobbodyjourney. (2021). [Discussion] Why are Einstein Sum Notations not popular in ML? They changed my life. [Reddit Post]. R/MachineLearning. <a href="https://www.reddit.com/r/MachineLearning/comments/r8tsv6/discussion_why_are_einstein_sum_notations_not/">https://www.reddit.com/r/MachineLearning/comments/r8tsv6/discussion_why_are_einstein_sum_notations_not/</a></li>
</ul>
</div>
</div>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-ai.html">ai</a> </div>
]]></description>
  <category><![CDATA[ai]]></category>
  <link>https://dou-meishi.github.io/org-blog/2024-11-28-UseEinops/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2024-11-28-UseEinops/notes.html</guid>
  <pubDate>Thu, 28 Nov 2024 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[What Happens in A Transformer Layer]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org1fa9243">Inputs and outputs</a></li>
<li><a href="#org6ba264e">Scaled dot-product attention</a></li>
<li><a href="#org3a3d1b7">Self-attention and its multihead version</a></li>
<li><a href="#org29e4d98">Masks in self-attention</a></li>
<li><a href="#org035675a">Transformer Layers</a></li>
<li><a href="#org0936a44">Conclusions</a></li>
<li><a href="#org3610b49">References</a></li>
<li><a href="#org3e2e4e7">Appendix: Layer normalization and batch normalization</a></li>
<li><a href="#orgf44d07c">Appendix: Positional encoding</a></li>
<li><a href="#org93c6a54">Appendix: Cross-attention</a></li>
<li><a href="#org8fc0ec8">Appendix: Encoder-decoder transformer</a></li>
</ul>
</div>
</nav>
<p>
Transformers serve as the backbone of large language models. Just as
 convolutional networks revolutionized image processing, transformers
 have significantly advanced natural language processing since their
 introduction. The efficient parallel computation and transfer
 learning capabilities of transformers have led to the rise of
 <i>pre-trained paradigm</i>. In this approach, a large-scale
 transformer-based model, referred to as the <i>foundation model</i>, is
 trained on a significant volume of data and subsequently utilized for
 downstream tasks through some form of fine-tuning. Our familiar
 friend ChatGPT is such an example, where GPT stands for generative
 pre-trained transformers. Meanwhile, transformer-base models achieve
 state-of-the-art performace for many different modalities, including
 text, image, video, point cloud, and audio data, and have been used
 for both discriminative and generative applications.
</p>

<p>
In this post, I will give a brief introduction to transformers. In
particular, I will focus on the computational flow within
<a href="https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html">torch.nn.TransformerEncoderLayer</a>, outlining the process of deriving
outputs from its inputs. The PyTorch implementation of transformers is
based on the paper <i>Attention is all you need</i> by Vaswani et al. (2017).
After reading this post, it should be straightforward to implement the
transformer architecture from scratch. For a detailed illustration,
refer to the appendix where I have included an example implementation
for reference. With this knowledge, it is even possible to train a GPT
given sufficient data and computational resources. For an example,
check out Karpathy's <a href="https://github.com/karpathy/nanoGPT">nanoGPT project</a> on GitHub.
</p>
<div id="outline-container-org1fa9243" class="outline-2">
<h2 id="org1fa9243">Inputs and outputs</h2>
<div class="outline-text-2" id="text-org1fa9243">
<p>
Similar to its predecessors, RNN and LSTM, the transformer
architecture is a sequence-to-sequence model. Mathematically, it
receives a sequence of vectors \(\{x_1, x_2, \ldots, x_n\}\), and
returns another sequence of vectors of the same length \(\{y_1, y_2,
\ldots, y_n\}\). Unlike RNN and LSTM, transformers do not compress
historical information into a single hidden state vector and avoid the
recurrence structure, which inherently precludes
parallelization. Instead, transformers exclusively leverage the
attention mechanism to explore dependencies between output and input
vectors.
</p>

<p>
<i>Notations.</i> We use matrices to represent sequential data. For example,
a sequence of vectors \(\{x_1, x_2, \ldots, x_N\}\) can be represented
by a matrix \(X = (x_1, x_2, \ldots, x_N)^\intercal\), where \(x_n^\intercal\) is the
\(n\)-th row vector of \(X\).
</p>
</div>
</div>
<div id="outline-container-org6ba264e" class="outline-2">
<h2 id="org6ba264e">Scaled dot-product attention</h2>
<div class="outline-text-2" id="text-org6ba264e">
<p>
The fundamental part of transformers is the attention function
</p>
\begin{equation}
\operatorname{Attention}(Q, K, V) =
\operatorname{Softmax}\bigl(\frac{QK^\intercal }{\sqrt{d_k}}\bigr)V,
\tag{1}
\end{equation}
<p>
where \(\operatorname{Softmax}[X]\) is an operator that ensures each row
vector of \(X\) sums up to one.
</p>

<p>
Let's explain what happends here.  The inputs and output of this
attention function are all matrices, where
</p>

<ul class="org-ul">
<li>\(Q\) is a \(N \times d_k\) matrix, referred to as the <i>query matrix</i>;</li>
<li>\(K\) is a \(M \times d_k\) matrix, referred to as the <i>key matrix</i>;</li>
<li>\(V\) is a \(M \times d_v\) matrix, referred to as the <i>value matrix</i>;</li>
<li>the output is a \(N \times d_v\) matrix.</li>
</ul>

<p>
For each row \(q_n^\intercal\) in \(Q\), we compute the <i>similarity</i> between it and
all row vectors of \(K\),
</p>
\begin{equation}
a_{nm} = \frac{q_n^\intercal k_m}{\sqrt{d_k}},
\quad m=1, 2, \ldots, M.
\tag{2}
\end{equation}
<p>
After normalizing this set of coefficients to sum to one by the
softmax function, the returned vector is a linear combination of row
vectors of \(V\)
</p>
\begin{equation}
y_n = \frac{1}{Z_n} \sum_m e^{a_{nm}} v_m, \quad Z_n = \sum_m e^{a_{nm}}.
\tag{3}
\end{equation}
<p>
The final output is a matrix with the same number of rows as \(Q\), and
\(y_n\) is the row vector of the output which corresponds to the input
row vector \(q_n\).
</p>

<p>
Equation (1) is called <i>scaled dot-product</i> attention because it applies
the scaled dot-product to measure the similarity between query vectors
and key vectors. Imagine that for a given query \(q_n\), if there are
three key vectors \(k_1, k_3\) and \(k_5\) that have the most responses,
then the attention function returns a linear combination of the
corresponding value vectors \(v_1, v_3\) and \(v_5\) where the
coefficients are proportional to the similarities.
</p>
</div>
</div>
<div id="outline-container-org3a3d1b7" class="outline-2">
<h2 id="org3a3d1b7">Self-attention and its multihead version</h2>
<div class="outline-text-2" id="text-org3a3d1b7">
<p>
Self-attention is an applications of the attention function in
transformers mentioned in the attention paper. It takes a sequential
data \(X\) as inputs, and computes \((Q,K,V)\) in Eq. (1) by learnable
linear transformations<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup> of \(X\)
</p>
\begin{equation}
Q = \operatorname{Linear}^Q(X), \quad
K = \operatorname{Linear}^K(X), \quad V =
\operatorname{Linear}^V(X).
\tag{4}
\end{equation}
<p>
Note that these linear maps act on rows of \(X\), and thus their returns
have the same number of rows (i.e., the same number of time steps) as
\(X\). It is also noteworthy that these learnable linear maps are
distinct from one another and each has its own set of parameters.
</p>

<p>
The self-attention can be enhanced by applying multiple attention
heads. While a single attention head computes one set of \((Q, K, V)\),
multihead attention computes multiple sets of these matrices using
different linear maps, concatenates these outputs and applies a final
linear transformation.
</p>

<p>
Take the multihead attention with 4 heads as an example.  For each
head \(i\), it first computes \((Q_i, K_i, V_i)\) by self-attention
Eq. (4) and applies Eq. (1) to obtain the output of this head, denoted
by \(H_i\). Then, it concatenates these matrices into a <i>wider</i> matrix \(H
= [H_1, H_2, H_3, H_4]\). Finally, it applies a linear transformation
to \(H\) and obtain the output.
</p>
</div>
</div>
<div id="outline-container-org29e4d98" class="outline-2">
<h2 id="org29e4d98">Masks in self-attention</h2>
<div class="outline-text-2" id="text-org29e4d98">
<p>
As a sequence-to-sequence model, it is often the case that the output
vector should depend on only a subset of the vectors within the input
sequence. For example, assume the input sequence is \(\{x_1, x_2,
\ldots, x_N\}\) and the expected output sequence is \(\{y_1, y_2,
\ldots, y_N\}\). In certain scenarios, it is imperative to ensure that
generating \(y_n\) involves only the <i>present</i> infomation
\(\{x_i\}_{i=1}^{n}\). In other words, this requirement implies that
\(y_n\) should not depend on <i>future</i> information \(\{x_i\}_{i=n+1}^N\).
</p>

<p>
To enforce this causal constraint on self-attention, we can add a mask
matrix to the similarity matrix and eliminate undesired
dependence. Specifically, consider Eq. (2) and Eq. (3), which describe
how to compute the output \(y_n\) from the row vectors \((q_n, k_m, v_m)\)
of \((Q, K, V)\). In self-attention these vectors are obtained by linear
transformations of the respective input data vectors \[ q_n =
\operatorname{Linear}^Q(x_n), \quad k_m =
\operatorname{Linear}^K(x_m), \quad v_m =
\operatorname{Linear}^V(x_m). \] Therefore, the similarity coefficient
\(a_{nm}\) in Eq. (2) relies on \(x_n\) and \(x_m\). To ensure \(y_n\) remains
independent of \(\{x_i\}_{i=n+1}^N\), we need only to ensure the
summation in Eq. (3) encompasses only \(1 \leq m \leq n\). By introducing a
mask matrix \(b_{nm}\) and adding it to the similarity matrix, we get \[
b_{nm} := \begin{cases} 0 & \quad m \leq n \\ -\infty & \quad m > n
\end{cases}, \qquad \hat{a}_{nm} := a_{nm} + b_{nm} = \begin{cases}
a_{nm} & \quad m \leq n \\ -\infty &\quad m > n \end{cases}. \] Consequently,
the output of the masked self-attention is \[ \hat{y}_n =
\frac{1}{Z_n}\sum_m e^{\hat{a}_{nm}} v_m = \frac{1}{Z_n}\sum_{m=1}^n
e^{a_{nm}} v_m, \quad Z_n = \sum_{m=1}^n e^{a_{nm}}. \] As a result, the
calculation of \(\hat{y}_n\) no longer depends on future information
\(\{x_i\}_{i=n+1}^N\).
</p>

<p>
The same trick can be applied to ignore the impact of <code>&lt;pad&gt;</code> tokens. In
some applications, particularly in natural language process tasks, the
input sequence is padded to a fixed length for batch
computations. Say, for example, an input sequence may look like
\(\{x_1, x_2, x_3, x_4, x_5\}\) where \(x_4\) and \(x_5\) are actually <code>&lt;pad&gt;</code>
tokens.  In that case, we may want to ensure that the output \(y_n\) do
not pay attention to those <code>&lt;pad&gt;</code> tokens. We therefore introduce a mask
to restrict the summation in Eq. (3) to \(1 \leq m \leq 3\), \[ b_{nm}
:= \begin{cases} 0 & \quad m \leq 3 \\ -\infty & \quad m > 3 \end{cases},
\qquad \hat{a}_{nm} := a_{nm} + b_{nm} = \begin{cases} a_{nm} & \quad
m \leq 3 \\ -\infty &\quad m > 3 \end{cases}. \] Consequently, the output of
the masked self-attention is \[ \hat{y}_n = \frac{1}{Z_n}\sum_m
e^{\hat{a}_{nm}} v_m = \frac{1}{Z_n}\sum_{m=1}^3 e^{a_{nm}} v_m, \quad
Z_n = \sum_{m=1}^3 e^{a_{nm}}. \] As a result, the calculation of
\(\hat{y}_n\) no longer depends on <code>&lt;pad&gt;</code> tokens \(\{x_4, x_5\}\).
</p>
</div>
</div>
<div id="outline-container-org035675a" class="outline-2">
<h2 id="org035675a">Transformer Layers</h2>
<div class="outline-text-2" id="text-org035675a">
<figure style="text-align: center;">
  <img src="./transformer-encoder-layer.png" alt="One layer of the transformer architecture" style="width: 30%;">
  <figcaption>Figure 1: One layer of the transformer architecture. Cropped from Figure 12.9 in Bishop & Bishop (2024).</figcaption>
</figure>

<p>
Figure 1 shows an overall architecture for a single transformer
layer. It consists of two major trainable blocks, a self-attention
block and a multilayer perceptron block. Each block is enclosed by a
residual connection and a layer normalization; see <a href="#orga73945e">the appendix</a> for a
brief explanation of layer normalization. The pseudocode for the
forward pass is provided below.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">forward</span>(
    <span style="color: #859900; font-weight: bold;">self</span>, x: torch.Tensor, mask: torch.Tensor | <span style="color: #268bd2; font-weight: bold;">None</span> = <span style="color: #268bd2; font-weight: bold;">None</span>
) -&gt; torch.Tensor:
    <span style="color: #2aa198;">"""</span>
<span style="color: #2aa198;">    Forward pass of the Transformer Encoder Layer.</span>

<span style="color: #2aa198;">    Args:</span>
<span style="color: #2aa198;">    - x (torch.Tensor): Input tensor with shape (batch_size, seq_len, d_model).</span>
<span style="color: #2aa198;">    - mask (torch.Tensor, optional): Mask tensor with shape (seq_len, seq_len) or None.</span>

<span style="color: #2aa198;">    Returns:</span>
<span style="color: #2aa198;">    - torch.Tensor: Transformed output tensor with shape (batch_size, seq_len, d_model).</span>
<span style="color: #2aa198;">    """</span>
    <span style="color: #268bd2;">y</span> = <span style="color: #859900; font-weight: bold;">self</span>.norm1(x)
    <span style="color: #268bd2;">y</span> = <span style="color: #859900; font-weight: bold;">self</span>.self_attn(y, y, y, mask=mask)
    <span style="color: #268bd2;">x</span> = x + y

    <span style="color: #268bd2;">y</span> = <span style="color: #859900; font-weight: bold;">self</span>.norm2(x)
    <span style="color: #268bd2;">y</span> = <span style="color: #859900; font-weight: bold;">self</span>.mlp(y)
    <span style="color: #268bd2;">x</span> = x + y

    <span style="color: #859900; font-weight: bold;">return</span> x
</pre>
</div>

<p>
It is worthing noting that in this pseudocode, we employ the <i>pre-norm</i>
configuration to wrap the blocks. This differs from the original
structure in the attention paper by Vaswani et al. (2017), which takes
the <i>post-norm</i> configuration. For a more detailed comparison between
pre-norm and post-norm, please refer to the paper by Xiong et
al. (2020).
</p>

<p>
The GPT-3 architecture is essentially a stack of such transformer
layers, supplemented by an initial embedding layer to translate tokens
into vectors and a final layer to predict tokens based on the
transformer outputs.
</p>
</div>
</div>
<div id="outline-container-org0936a44" class="outline-2">
<h2 id="org0936a44">Conclusions</h2>
<div class="outline-text-2" id="text-org0936a44">
<p>
The core of a transformer layer is the multi-head self-attention
layer, whose inputs and outputs are both sequences. I have explained
the overall computational flow step by step in the sections above, and
readers should now feel comfortable with what happens in
<a href="https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html">torch.nn.TransformerEncoderLayer</a>. For demonstration purposes, a simple
implementation is provided <a href="https://github.com/Dou-Meishi/learnTransformer">here</a>. Lastly, it should be mentioned that
the original paper used a slightly more complex architecture. For
interested readers, please refer to the appendices of this post or the
references listed below.
</p>
</div>
</div>
<div id="outline-container-org3610b49" class="outline-2">
<h2 id="org3610b49">References</h2>
<div class="outline-text-2" id="text-org3610b49">
<p>
Books and Papers
</p>

<ul class="org-ul">
<li>Bishop, C. M., &amp; Bishop, H. (2024). Deep learning: Foundations and concepts (pp. 357-406). Springer.</li>

<li>Zhang, A., Lipton, Z. C., Li, M., &amp; Smola, A. J. (2023). Attention Mechanisms and Transformers. In <i>Dive into Deep Learning</i>. Cambridge University Press. <a href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html">https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html</a></li>

<li>Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., &amp; Liu, T.-Y. (2020). On layer normalization in the transformer architecture. Proceedings of the 37th International Conference on Machine Learning, 119, 10524‚Äì10533. <a href="https://arxiv.org/pdf/2002.04745">https://arxiv.org/pdf/2002.04745</a></li>

<li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å. ukasz, &amp; Polosukhin, I. (2017). Attention is all you need. Advances in Neural Information Processing Systems, 30. <a href="https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html</a></li>
</ul>

<p>
Online resources (concepts)
</p>

<ul class="org-ul">
<li>Kim, E., &amp; Ashish, N. (2024). Discussion 6. In <i>Data C182 Fall 2024</i>. <a href="https://datac182fa24.github.io/assets/section_notes/week08_solution.pdf">https://datac182fa24.github.io/assets/section_notes/week08_solution.pdf</a></li>

<li>Raschka, S. (2023). About layernorm variants in the original transformer paper.  <a href="https://magazine.sebastianraschka.com/p/why-the-original-transformer-figure">https://magazine.sebastianraschka.com/p/why-the-original-transformer-figure</a></li>

<li>Zhang, M. (2022). DIsucssion 7. In <i>CS182/282A Spring 2022</i>.  <a href="https://datac182fa24.github.io/assets/section_notes/week09_solution.pdf">https://datac182fa24.github.io/assets/section_notes/week09_solution.pdf</a></li>

<li>Mongaras, G. (2022). How do self-attention masks work? Medium. <a href="https://gmongaras.medium.com/how-do-self-attention-masks-work-72ed9382510f">https://gmongaras.medium.com/how-do-self-attention-masks-work-72ed9382510f</a></li>

<li>Adaloglou, N. (2020). How Transformers work in deep learning and NLP: an intuitive introduction. AI Summer. <a href="https://theaisummer.com/transformer/">https://theaisummer.com/transformer/</a></li>

<li>Karpathy, A. (2019). A recipe for training neural networks. <a href="https://karpathy.github.io/2019/04/25/recipe/">https://karpathy.github.io/2019/04/25/recipe/</a></li>

<li>Alammar, J. (2018). The illustrated transformer. <a href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a></li>
</ul>

<p>
Online resources (codes)
</p>

<ul class="org-ul">
<li>BavalpreetSinghh (2024). Transformer from scratch using Pytorch. Medium. <a href="https://medium.com/@bavalpreetsinghh/transformer-from-scratch-using-pytorch-28a5d1b2e033">https://medium.com/@bavalpreetsinghh/transformer-from-scratch-using-pytorch-28a5d1b2e033</a></li>

<li>Erdogan, E. (2024). Examining Multihead Attention. GitHub Gist. <a href="https://gist.github.com/eneserdo/77b468f61fa5c3c9f4587b4a51fca963">https://gist.github.com/eneserdo/77b468f61fa5c3c9f4587b4a51fca963</a></li>

<li>Karpathy, A. (2023). Let's build GPT: from scratch, in code, spelled out [Video]. YouTube. <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY&amp;t=5722s">https://www.youtube.com/watch?v=kCc8FmEb1nY&amp;t=5722s</a></li>

<li>PyTorch (2023). Transformer Layers. PyTorch Documentations.  <a href="https://pytorch.org/docs/stable/nn.html#transformer-layers">https://pytorch.org/docs/stable/nn.html#transformer-layers</a></li>

<li>Karpathy, A. (2022). NanoGPT. GitHub. <a href="https://github.com/karpathy/nanoGPT">https://github.com/karpathy/nanoGPT</a></li>

<li>Arunmohan003 (2022). Transformer from scratch using pytorch. Kaggle. <a href="https://www.kaggle.com/code/arunmohan003/transformer-from-scratch-using-pytorch/notebook">https://www.kaggle.com/code/arunmohan003/transformer-from-scratch-using-pytorch/notebook</a></li>

<li>CS182 HW03 (2021). Natural language processing. <a href="https://github.com/cs182sp21/hw3_public/blob/master/2%20Summarization.ipynb">https://github.com/cs182sp21/hw3_public/blob/master/2%20Summarization.ipynb</a></li>

<li>Harvard NLP (2018). The Annotated Transformer. <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">https://nlp.seas.harvard.edu/2018/04/03/attention.html</a></li>

<li>Lynn-Evans, S. (2018). How to code The Transformer in Pytorch. Medium.  <a href="https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec">https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec</a></li>
</ul>
</div>
</div>
<div id="outline-container-org3e2e4e7" class="outline-2">
<h2 id="org3e2e4e7">Appendix: Layer normalization and batch normalization</h2>
<div class="outline-text-2" id="text-org3e2e4e7">
<p>
<a id="orga73945e"></a>
</p>

<p>
Layer normalization and batch normalization are both normalization
operations. The key difference is that they operate on different
dimensions. Layer normalization computes the statistics accross the
feature dimension, whereas batch normalization computes the statistics
across the batch dimension. For example, given a batch of inputs \(x\)
with a shape of \((N, C)\), where \(N\) is the batch dimension and \(C\) is
the feature dimension. Layer normalization computes the mean,
variance, and normalized outputs by \[ \mu_i = \frac{1}{C}\sum_j x_{ij},
\quad \sigma_i = \frac{1}{C}\sum_j(x_{ij} - \mu_i)^2, \quad \tilde{x}_{ij} =
\frac{x_{ij} - \mu_i}{\sqrt{\sigma_i}}. \] On the other hand, batch
normalization computes the mean, variance, and normalized outputs by
\[ \mu_j = \frac{1}{N}\sum_i x_{ij}, \quad \sigma_j = \frac{1}{N}\sum_j(x_{ij} -
\mu_j)^2, \quad \tilde{x}_{ij} = \frac{x_{ij} - \mu_j}{\sqrt{\sigma_j}}. \]
</p>

<p>
One main drawback of batch normalization is that it cannot process
unbatched data where \(N=1\), which is common in the prediction
phase. The common resolution is to separate its training logic and
inference logic. In training mode, batch normalization requires \(N >
1\) and estimates the sample mean and sample variance of the inputs. In
inference mode, however, batch normalization uses the mean and
variance of the whole training dataset, and thus works with \(N=1\)
data. In practice, the mean and variance of the whole training dataset
are obtained by maintaining a moving mean and a moving variance
throughout the training mode; see <a href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html">torch.nn.BatchNorm1d</a> for more
details.
</p>

<p>
In practice, both layer normalization and batch normalization can be
extended to high-order tensor inputs. For example,
<a href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html">torch.nn.BatchNorm2d</a> accepts inputs with shapes of \((N, C, H, W)\) and
computes statistics over \((N, H, W)\) dimensions. Similarly,
<a href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html">torch.nn.LayerNorm</a> can also accept inputs with shape \((N, L, D)\) and
computes statistics over \((L, D)\); see its manual for more details.
</p>
</div>
</div>
<div id="outline-container-orgf44d07c" class="outline-2">
<h2 id="orgf44d07c">Appendix: Positional encoding</h2>
<div class="outline-text-2" id="text-orgf44d07c">
<p>
One limitation of the attention function is that Eq. (1) is
<i>equivariant</i> w.r.t. row permutations. Specifically, it is not hard to
observe that for any permutation matrix \(P\), it holds that<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup> \[
\operatorname{Attention}(PQ, PK, PV) = P(\operatorname{Attention}(Q,
K, V)). \] Moreover, given that the linear maps Eq. (4) also follows
this equivariance, self-attention exhibits equivariance with respect
to row permutations. As a result, models lacking this property are
incompatible with self-attention. For instance, self-attention fails
to learn straightforward patterns like \(y_n = nx_n\). Indeed, for an
input sequence \((x_1, x_2, x_3)\) we expect an output sequence \((x_1,
2x_2, 3x_3)\).  Yet, when the input sequence is reordered, e.g., \((x_1,
x_3, x_2)\), we expect the output sequence to be \((x_1, 2x_3,
3x_2)\). However, this is impossible for models that are equivariant
w.r.t. row permutations.
</p>

<p>
The remedy is to explicitly inject some information about the relative
or absolute position of the tokens in the sequence. A straightforward
way is to concatenate time information into the features by rewriting
the input sequence \(\{x_n\}_{n=1}^N\) as a new sequence \(\{(n,
x_n)\}_{n=1}^N\). However, this may lead to unbounded inputs and
increase the computational cost due to the introduction of an extra
time dimension. Alternatively, a widely accepted way is to encode the
position information into a supplementary seqeuence \(\{r_n\}_{n=1}^N\),
called positional encoding, which is independent of input
sequences. For any input sequence \(\{x_n\}\), the positional encoding
is added before feeding it to attention layers \[ \tilde{x}_n = x_n +
r_n. \] The positional encoding \(r_n\) can either be learned as network
parameters or set manually. One possible form of the positional
encoding is based on sinusodial functions \[ r_n^{(i)} = \begin{cases}
\sin \frac{n}{L^{i/D}}, \quad \text{ if $i$ is even}, \\ \cos
\frac{n}{L^{(i-1)/D}}, \quad \text{ if $i$ is odd}. \end{cases} \]
Here, \(r_n^{(i)}\) is the \(i\)-th component of \(r_n\), \(D\) is the
dimension of \(x_n\), and \(L\) is a constant, e.g., 10000.
</p>

<p>
Lastly, it is worth noting that positional encoding can be dropped in
theory when employing causal masks as, in such cases, the transformer
layer is no longer equivariant with respect to row permutations.
</p>
</div>
</div>
<div id="outline-container-org93c6a54" class="outline-2">
<h2 id="org93c6a54">Appendix: Cross-attention</h2>
<div class="outline-text-2" id="text-org93c6a54">
<p>
Cross-attention is another application of the attention mechanism
mentioned in the original attention paper. Different from
self-attention Eq. (4), which computes \((Q, K, V)\) based on the same
input \(X\), cross-attention requires an addition input sequence \(Z\) and
uses it to compute the key and value matrices.
</p>
\begin{equation}
Q = \operatorname{Linear}^Q(X), \quad K =
\operatorname{Linear}^K(Z), \quad V = \operatorname{Linear}^V(Z).
\tag{5}
\end{equation}
<p>
The output of cross-attention has the same number of rows as \(X\) and
the same number of columns as \(Z\). Like self-attention,
cross-attention in practice often utilizes multiple attention heads.
</p>

<p>
The causal mask and padding mask may also be applied in
cross-attention. Let the input sequences be \(\{x_1, x_2, \ldots,
x_N\}\) and \(\{z_1, z_2, \ldots, z_M\}\) and let the expected output
sequence be \(\{y_1, y_2, \ldots, y_N\}\). The query, key and value
vectors are computed by \[ q_n = \operatorname{Linear}^Q(x_n), \quad
k_m = \operatorname{Linear}^K(z_m), \quad v_m =
\operatorname{Linear}^V(z_m). \] Considering Eq. (3) for the
calculation of \(y_n\), we note that if there is no mask then \(y_n\) may
depend on \(x_n\) and the whole sequence \(\{z_1, z_2, \ldots, z_M\}\).
Therefore, the causal mask and padding mask used in self-attention can
also help to the eliminate dependence between \(y_n\) and \(\{z_1, z_2,
\ldots, z_M\}\).
</p>
</div>
</div>
<div id="outline-container-org8fc0ec8" class="outline-2">
<h2 id="org8fc0ec8">Appendix: Encoder-decoder transformer</h2>
<div class="outline-text-2" id="text-org8fc0ec8">
<figure style="text-align: center;">
  <img src="./encoder-decoder-transformer.png" alt="Encoder-decoder transformer architecture" style="width: 60%;">
  <figcaption>Figure 2: Encoder-decoder transformer architecture. Cropped from Figure 1 in Vaswani et al. (2017).</figcaption>
</figure>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
In PyTorch implementation, e.g., <a href="https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer">TransformerEncoderLayer</a>, there
is a parameter <code>bias</code> to determine whether or not a bias term will be
included in this linear transformation.
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Indeed, for any matrix \(X\) and any permutation matrix \(P\), it
holds that \[P^{-1} = P^\intercal, \quad \operatorname{Softmax}[PX] = P
\operatorname{Softmax}[X], \quad \operatorname{Softmax}[XP] =
\operatorname{Softmax}[X]P. \]
</p></div></div>


</div>
</div><div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-ai.html">ai</a> </div>
]]></description>
  <category><![CDATA[ai]]></category>
  <link>https://dou-meishi.github.io/org-blog/2024-10-30-LearnTransformer/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2024-10-30-LearnTransformer/notes.html</guid>
  <pubDate>Wed, 30 Oct 2024 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[An In-Depth Introduction to Backpropagation and Automatic Differentiation]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgbeda738">Four ways to calculate gradients</a></li>
<li><a href="#orgd00098b">Automatic differentiation</a>
<ul>
<li><a href="#org9654957">Forward-mode AD</a></li>
<li><a href="#org006a24b">Reverse-mode AD</a></li>
</ul>
</li>
<li><a href="#orgd268b38">Examples: reverse-mode AD for scalar functions</a></li>
<li><a href="#org54bdb65">Jacobian-vector product</a></li>
<li><a href="#org4d46603">Implement forward-mode and reverse-mode AD</a></li>
<li><a href="#orgf719635">Hessian-vector product</a></li>
<li><a href="#orgdac8ba1">Conclusion</a></li>
<li><a href="#orgcd67d14">References</a></li>
<li><a href="#org54ca842">Appendix: An inductive proof of reverse-mode AD formula</a></li>
<li><a href="#org4876bdc">Appendix: JVP and VJP for linear maps</a></li>
<li><a href="#org6fd5062">Appendix: Overview of the Python Implementation</a></li>
</ul>
</div>
</nav>
<p>
Backpropagation and automatic differentiation (AD) are fundamental
components of modern deep learning frameworks. However, many
practitioners pay little attention to their implementations and may
regard them as some sort of "black magic". It indeed looks like magic
that PyTorch can virtually calculate derivatives of an arbitrary
function defined by the user, and even accommodate flow control
elements like conditional execution, which is mathematically not
differentiable. Although we understand that mathematically they
primarily employ the chain rule, it remains unclear how they
efficiently apply it to a function whose form is entirely unknown and
will be determined by the user.
</p>

<p>
In this post, I will introduce the underlying mechanisms of AD and
give a simple implementatin for demonstration. Moreover, I want to
clarify the following facts of AD.
</p>

<ol class="org-ol">
<li>Backpropagation is a special case of AD that runs in reverse
mode. Backpropagation is a specialized algorithm for calculating
gradients of neural networks. On the other hand, AD refers to a
general technique that generates numerical derivative evaluations
rather than derivative expressions. Besides reverse-mode AD, there
are forward-mode AD and even reverse-on-forward AD.</li>

<li>AD is neither traditional numerical differentiation nor symbolic
differentiation. However, AD does in fact provide numerical values
of derivatives and it does so by using symbolic rules of
differentiation, giving it a two-sided nature that is partly
symbolic and partly numerical.  To some extent, AD should be
regarded as an efficient way to <i>evaluate</i> the derivative expressions
without the need to explicitly construct their mathematical expressions.</li>

<li>AD is guaranteed to take no more than 6 times the computational
cost of a single function evaluation. In practice, the overhead is
typically closer to a factor of 2 or 3 (Bishop &amp; Bishop, 2024,
p. 250; see also Baydin et al., 2018, p. 16).</li>

<li>Although evaluating the Hessian matrix \(H\) of some scalar function
\(f: \mathbb{R}^n \to \mathbb{R}\) by AD has \(\mathcal{O}(n^2)\) complexity, evaluating the
Hessian-vector product \(H v\) by reverse-on-forward AD has only
\(\mathcal{O}(n)\) complexity. The same difference exists between evaluating
the Jacobian matrix \(J\) of some vector-valued function \(\mathbb{R}^n \to \mathbb{R}^m\)
and evaluating the Jacobian-vector product \(Jv\).</li>
</ol>

<p>
The original motivation of this post is <a href="https://github.com/datac182fa24/datac182_hw1_student">the first homework assignment</a>
of <a href="https://datac182fa24.github.io/">Data C182</a>, which requires to implement the forward pass and
backward pass of common neural network layers, including affine layer,
dropout layer, batchnorm layer and so on. While scanning through the
recommended textbook (Bishop &amp; Bishop, 2024), I notice that there are
not only reverse-mode AD, which is a general version of
backpropagation, and also forward-mode AD, which can be used in
combine with reverse-mode AD to efficiently compute the
Hessian-vector product. After reading the survey paper (Baydin et al.,
2018), I finally open up the "balck-box" of automatic differentiation
hidden in PyTorch and have a clearer understanding of AD.
</p>
<div id="outline-container-orgbeda738" class="outline-2">
<h2 id="orgbeda738">Four ways to calculate gradients</h2>
<div class="outline-text-2" id="text-orgbeda738">
<p>
<i>Adapted from the textbook (Bishop &amp; Bishop, 2024) and the survey paper
(Baydin et al., 2018).</i>
</p>

<p>
Methods for the computation of derivatives in computer programs can be
classified into four categories:
</p>

<ol class="org-ol">
<li>manually working out derivatives and coding them;</li>

<li>numerical differentiation using finite difference approximations;</li>

<li>symbolic differentiation using expression manipulation in computer
algebra systems such as Mathematica, Maxima, and Maple;</li>

<li>automatic differentiation, also called algorithmic differentiation.</li>
</ol>

<p>
Numerical approximations of derivatives are inherently ill-conditioned
and unstable. It suffers truncation and round-off errors inflicted by
the limited precision of computations and the chosen value of the step
size \(h\). Truncation error tends to zero as \(h \to 0\). However, as \(h\)
is decreased, round-off error increases and becomes
dominant. Moreover, numerical differentiation requires \(\mathcal{O}(n)\)
evaluations (for gradients in \(n\) dimensions) of the original
function, which is the main obstacle to its usefulness in machine
learning where \(n\) can be as large as millions or billions in
state-of-the-art deep learning models.
</p>

<p>
Symbolic differentiation, on the other hand, faces the problem called
<i>expression well</i>, i.e., the resulting expressions for derivatives can
become exponentially longer than the original function. A further
major drawback with symbolic differentiation is that it requires that
the expression to be differentiated is expressed in closed form. It
therefore excludes important control flow operations such as loops,
recursions, conditional execution, and procedure calls, which are
valuable constructs that we might wish to use when defining the
network function.
</p>

<p>
The first approach, which formed the mainstay of neural networks for
many years, is to derive the backpropagation equations by hand and
then to implement them explicitly in software. If this is done
carefully it results in efficient code that gives precise results that
are accurate to numerical precision. However, the process of deriving
the equations as well as the process of coding them both take time and
are prone to errors. If the model is altered, both the forward and
backward implementations need to be changed in unison. This effort can
easily become a limitation on how quickly and effectively different
architectures can be explored empirically.
</p>

<p>
Modern deep learning frameworks use automatic differentiation to
evaluate gradients of neural networks. Unlike symbolic
differentiation, the goal of automatic differentiation is not to find
a mathematical expression for the derivatives but to have the computer
automatically generate the code that implements the gradient
calculations given only the code for the forward propagation
equations. It is accurate to machine precision, just as with symbolic
differentiation, but is more efficient because it is able to exploit
intermediate variables used in the definition of the forward
propagation equations and thereby avoid redundant evaluations.
</p>
</div>
</div>
<div id="outline-container-orgd00098b" class="outline-2">
<h2 id="orgd00098b">Automatic differentiation</h2>
<div class="outline-text-2" id="text-orgd00098b">
<p>
Automatic differentiation (AD) can be thought of as performing a
non-standard interpretation of a computer program where this
interpretation involves augmenting the standard computation with the
calculation of various derivatives. All numerical computations are
ultimately compositions of a finite set of elementary operations for
which derivatives are known, and combining the derivatives of the
constituent operations through the chain rule gives the derivative of
the overall composition. Usually these elementary operations include
the binary arithmetic operations, the unary sign switch, and
transcendental functions such as the exponential, the logarithm, and
the trigonometric functions. Besides that, users can define their own
operations, as long as they provide their derivative functions.
</p>

<p>
For a given function \(y=f(x)\) for differentiation, assume it can be
described by the following computational graph
</p>
$$ \begin{aligned}
z_0 &= x, \\
z_1 &= \phi_1(z_0), \\
z_2 &= \phi_2(z_0, z_1), \\
&\cdots \\
z_k &= \phi_k(z_0, z_1, z_2, \ldots, z_{k-1}), \\
&\cdots \\
z_N &= \phi_N(z_0, z_1, z_2, \ldots, z_{k-1}, \ldots, z_{N-1}), \\
y &= z_N,
\end{aligned} $$
<p>
where \(\phi_1, \phi_2, \ldots, \phi_N\) are elementary operations whose
derivatives \(\partial \phi_1, \partial \phi_2, \ldots, \partial \phi_N\) are known.  Obviously, any
computational graph is a directed acyclic graph and always falls
within the considered case. In practice, the initial variable \(z_0\)
contains all leaf nodes of the computational graph, and \(\phi_k\) may only
depends on one or two variables in \(\{z_\alpha\}_{\alpha=0}^{k-1}\).
</p>

<p>
<i>Notations.</i> For a vector \(x\), we denote by \(x^{(i)}\) the \(i\)-th
component of \(x\), i.e., \(x = (x^{(0)}, x^{(1)}, x^{(2)}
\ldots)^\intercal\). For a vector \(y\) that depends on \(x\), the Jacobian matrix
\(\partial y/\partial x\) is defined by \[ \biggl(\frac{\partial y}{\partial x}\biggr)_{ij} =
\frac{\partial y^{(i)}}{\partial x^{(j)}}. \] For a function \(y=f(x)\), we also use \(\partial
f\) to represent the Jacobian matrix \(\partial y / \partial x\). With this notation,
the chain rule can be written as \(\partial (f \circ g ) = [\partial f] [\partial g]\). It
should be noted that the Jacobian matrix \(\partial f\) of a scalar function is
a row vector in this notation.
</p>
</div>
<div id="outline-container-org9654957" class="outline-3">
<h3 id="org9654957">Forward-mode AD</h3>
<div class="outline-text-3" id="text-org9654957">
<p>
Forward-mode AD is the conceptually most simple type: <i>apply symbolic
differentiation at the elementary operation level and keep
intermediate numerical results, in lockstep with the evaluation of the
main function</i>. Viewing \(z_k\) (\(k=1, 2, \ldots, N\)) as functions of
\(x\), we differentiate the function \[ z_k(x) = \phi_k(z_0(x), z_1(x),
z_2(x), \ldots, z_{k-1}(x)) \] w.r.t. \(x\) and obtain
</p>
\begin{equation}\tag{chain-rule: FAD}
\frac{\partial z_k}{\partial x} =
\sum_{\alpha=0}^{k-1} \frac{\partial \phi_k}{\partial z_\alpha} \frac{\partial z_\alpha}{\partial x}, \quad k=1,
2, \ldots, N.
\end{equation}
<p>
Starting from \(\partial z_0 / \partial x = I\), we evaluate \(\partial z_k / \partial x\) in
accordance with the evaluation of \(z_k\). When finally we obtain
\(y=z_N\), the derivative \(\partial y/\partial x=\partial z_N/ \partial x\) is also obtained.
</p>
$$ \begin{aligned}
\frac{\partial z_1}{\partial x} &= \frac{\partial \phi_1}{\partial z_0} \frac{\partial z_0}{\partial x}, \\
\frac{\partial z_2}{\partial x} &= \frac{\partial \phi_2}{\partial z_0} \frac{\partial z_0}{\partial x} + \frac{\partial \phi_2}{\partial z_1} \frac{\partial z_1}{\partial x}, \\
&\cdots \\
\frac{\partial z_N}{\partial x} &= \sum_{\alpha=0}^{N-1} \frac{\partial \phi_k}{\partial z_\alpha} \frac{\partial z_\alpha}{\partial x}.
\end{aligned} $$
<p>
In other words, these equations are <b>evaluated from top to the bottom,
row by row</b>.
</p>
</div>
</div>
<div id="outline-container-org006a24b" class="outline-3">
<h3 id="org006a24b">Reverse-mode AD</h3>
<div class="outline-text-3" id="text-org006a24b">
<p>
Reverse-mode AD differes from forward-mode AD by using different
auxiliary variables. In forward-mode AD, the auxiliary variables are
\(\partial z_k / \partial x\) (called <i>tangent</i> variables), and are evaluated from \(k=1\)
to \(k=N\). In reverse-mode AD, the auxiliary variables are \(\partial y / \partial
z_k\) (called <i>adjoint</i> variables), and are evaluated from \(k=N-1\) to
\(k=0\). For any \(0 \leq k \leq N-1\), we regard \(\mathcal{Z}_k := \{z_\alpha\}_{\alpha=0}^{k}\) as
independent variables and view \(\{z_\beta\}_{\beta=k+1}^N\) as functions of
\(\mathcal{Z}_k\). Thus, we can differentiate \[ y = y(z_0, z_1, \ldots, z_k) \]
w.r.t. \(z_k\) and obtain (see <a href="#orgd4be40f">the appendix</a> for an inductive proof)
</p>
\begin{equation}\tag{chain-rule: RAD}
\frac{\partial y}{\partial z_{k}} = \sum_{\beta=k+1}^N \frac{\partial y}{\partial z_\beta} \frac{\partial \phi_\beta}{\partial
z_k}, \quad k=N-1, N-2, \ldots, 0.
\end{equation}
<p>
The partial derivative \(\partial y/ \partial
z_k\) should be understood as the sensitivity of \(y\) w.r.t. \(z_k\). An
intuitive explanation of this formula is: <i>when \(z_k\) changes, the
change of the final output is determined by the cumulative effect of
changes in downstream variables</i> \(\{z_\beta\}_{\beta=k+1}^N\).
</p>

<p>
Starting from \(\partial y / \partial z_N = I\), we evaluate \(\partial y / \partial z_k\)
reversely.
</p>
$$ \begin{aligned}
\frac{\partial y}{\partial z_{N-1}} &= {\color{blue}\frac{\partial y}{\partial z_{N}} \frac{\partial \phi_N}{\partial z_{N-1}}}, \\
\frac{\partial y}{\partial z_{N-2}} &= {\color{blue} \frac{\partial y}{\partial z_{N}} \frac{\partial \phi_N}{\partial z_{N-2}}} + {\color{green} \frac{\partial y}{\partial z_{N-1}} \frac{\partial \phi_{N-1}}{\partial z_{N-2}}}, \\
\frac{\partial y}{\partial z_{N-3}} &= {\color{blue} \frac{\partial y}{\partial z_{N}} \frac{\partial \phi_N}{\partial z_{N-3}}} + {\color{green} \frac{\partial y}{\partial z_{N-1}} \frac{\partial \phi_{N-1}}{\partial z_{N-3}}} + \frac{\partial y}{\partial z_{N-2}} \frac{\partial \phi_{N-2}}{\partial z_{N-3}}, \\
&\cdots \\
\frac{\partial y}{\partial z_{0}} &= \sum_{\beta=1}^N \frac{\partial y}{\partial z_\beta} \frac{\partial \phi_\beta}{\partial z_k}.
\end{aligned} $$
<p>
Initially, all adjoint variables are set to 0 except \(\partial y/\partial z_N\),
which is set to \(I\).  First, we compute the blue terms \(\frac{\partial y}{\partial
z_N}\frac{\partial \phi_N}{\partial z_k}\) and add them to the corresponding adjoint
variable \(\partial y/\partial z_k\). In the next step, we compute the green terms
\(\frac{\partial y}{\partial z_{N-1}}\frac{\partial \phi_{N-1}}{\partial z_k}\) and add them to the
corresponding adjoint variable \(\partial y/\partial z_k\). Repeat this process until
all terms listed above have been calculated. Then, the accumulated
values in adjoint variables are theire true values.
</p>

<p>
In other words, these equations are <b>evaluated from left to right,
column by column</b>. After the evaluation of column \(\beta\), all adjoint
variables \(\partial y/ \partial z_k\) for \(k \geq \beta-1\) has been obtained.
</p>

<p>
It should be noted that the reverse calculations of \(\partial y/ \partial
z_k\) actually happens in the second phase of a two-phase process,
while intermediate variables \(z_k\) are calculated in the first
phase. This is different from forward-mode AD, where \(\partial z_k / \partial x\)
and \(z_k\) are calculated simultaneously and in a forward manner.
</p>
</div>
</div>
</div>
<div id="outline-container-orgd268b38" class="outline-2">
<h2 id="orgd268b38">Examples: reverse-mode AD for scalar functions</h2>
<div class="outline-text-2" id="text-orgd268b38">
<p>
<a id="org60a8cec"></a>
</p>

<p>
Here we apply reverse-mode AD to scalar functions and demonstrate how
it works. We will discuss the case of vector functions in the
following sections.
</p>

<p>
Reverse-mode AD is inherently suitable for scalar output
\(y\). Examining the formula (chain-rule: RAD), the Jacobian \(\partial y / \partial
z_\beta\) now simplifies to row vectors as \(y\) is one
dimensional. Consequently, all matrix multiplications \([\partial y/\partial z_\beta][\partial
\phi_\beta /\partial z_k]\) reduces to vector-matrix products.
</p>

<p>
For a variable \(z\), we denote by a column vector \(\dot{z} = [\partial y / \partial
z]^\intercal\). With this notation, the chain-rule of reverse-mode AD can be
written as \[ \dot{z}_k = \sum_{\beta=k+1}^N \biggl[ \frac{\partial \phi_\beta}{\partial z_k}
\biggr]^\intercal \dot{z}_\beta, \quad k=N-1, N-2, \ldots, 0.\] Note that the true
gradient \(\dot{z}_k\) is a summation accumulated from \(\beta=N\) to
\(\beta=k+1\). At each stage \(\beta\), we can only compute a single term \([\partial \phi_\beta /\partial
z_k]^\intercal \dot{z}_\beta\) for \(\dot{z}_k\).
</p>

<p>
In order to apply the chain rule, we record all the operations
\(\{\phi_k\}\) along with their inputs and outputs on a <i>tape</i> (alternatively
known as a <i>Wengert list</i> or an <i>evaluation trace</i>).
</p>

<p>
<i>Example.</i> Consider the function \(f(a, b) = \langle a, a+ b\rangle\).  The
computational graph is
</p>
<table>


<colgroup>
<col  class="org-right">

<col  class="org-left">

<col  class="org-left">

<col  class="org-left">
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">\(k\)</th>
<th scope="col" class="org-left">input</th>
<th scope="col" class="org-left">\(\phi_k\)</th>
<th scope="col" class="org-left">output</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-left">\(a, b\)</td>
<td class="org-left">\(z_1 = a+b\)</td>
<td class="org-left">\(z_1\)</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-left">\(a, z_1\)</td>
<td class="org-left">\(z_2 = a^\intercal z_1\)</td>
<td class="org-left">\(z_2\)</td>
</tr>
</tbody>
</table>

<p>
Initially, set \(\dot{a}=0, \dot{b}=0\) and \(\dot{z}_k = 0\).  Starting
at \(\dot{z}_2 = [\partial y / \partial z_2]^\intercal = 1\), we then apply the chain rule to
propagate gradient reversely and accumulate the obtained value.
</p>
$$ \begin{aligned}
(\text{Initialize}) &\qquad & \dot{z}_2 &\leftarrow 1 \\
(k=2) & \qquad &  \dot{a} &\leftarrow \dot{a} + \biggl[\frac{\partial \phi_k}{\partial a}\biggr]^\intercal \dot{z}_2 \\
& & \dot{z}_1 &\leftarrow \dot{z}_1 + \biggl[\frac{\partial \phi_k}{\partial z_1}\biggr]^\intercal  \dot{z}_2 \\
(k=1) & \qquad & \dot{a} &\leftarrow \dot{a} + \biggl[ \frac{\partial \phi_k}{\partial a} \biggr]^\intercal \dot{z}_1 \\
& & \dot{b} &\leftarrow \dot{b} +  \biggl[\frac{\partial \phi_k}{\partial b}\biggr]^\intercal \dot{z}_1\\
\end{aligned} $$
<p>
We can explicitly write down these values and verify.
</p>
$$ \begin{aligned}
\dot{z}_2 &= 1 ,\\
\dot{z}_1 &= a ,\\
\dot{a} &=  z_1 \dot{z}_2 + \dot{z}_1 = 2a + b ,\\
\dot{b} &= \dot{z}_1 = a.
\end{aligned} $$

<p>
<i>Example.</i> Consider the function \(f(a, b) = \|a\|^2 + a^\intercal b - \sin (a^\intercal
b)\). The computational graph is
</p>

<table>


<colgroup>
<col  class="org-right">

<col  class="org-left">

<col  class="org-left">

<col  class="org-left">
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">\(k\)</th>
<th scope="col" class="org-left">input</th>
<th scope="col" class="org-left">\(\phi_k\)</th>
<th scope="col" class="org-left">output</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-left">\(a, b\)</td>
<td class="org-left">\(z_1 = a^\intercal b\)</td>
<td class="org-left">\(z_1\)</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-left">\(a\)</td>
<td class="org-left">\(z_2 = a^\intercal a\)</td>
<td class="org-left">\(z_2\)</td>
</tr>

<tr>
<td class="org-right">3</td>
<td class="org-left">\(z_1,z_2\)</td>
<td class="org-left">\(z_3 = z_1 + z_2\)</td>
<td class="org-left">\(z_3\)</td>
</tr>

<tr>
<td class="org-right">4</td>
<td class="org-left">\(z_1\)</td>
<td class="org-left">\(z_4 = \sin z_1\)</td>
<td class="org-left">\(z_4\)</td>
</tr>

<tr>
<td class="org-right">5</td>
<td class="org-left">\(z_3,z_4\)</td>
<td class="org-left">\(z_5=z_3-z_4\)</td>
<td class="org-left">\(z_5\)</td>
</tr>
</tbody>
</table>

<p>
Initially, set \(\dot{a}=0, \dot{b}=0\) and \(\dot{z}_k = 0\).  Starting
at \(\dot{z}_5 = [\partial y / \partial z_5]^\intercal = 1\), we then apply the chain rule to
propagate gradient reversely and accumulate the obtained value.
</p>
$$ \begin{aligned}
(\text{Initialize}) &\qquad & \dot{z}_5 &\leftarrow 1 \\
(k=5) & \qquad &  \dot{z}_4 &\leftarrow \dot{z}_4 + \biggl[ \frac{\partial \phi_k}{\partial z_4}\biggr]^\intercal  \dot{z}_5 \\
& & \dot{z}_3 &\leftarrow \dot{z}_3 + \biggl[ \frac{\partial \phi_k}{\partial z_3}\biggr]^\intercal \dot{z}_5 \\
(k=4) & \qquad & \dot{z}_1 &\leftarrow \dot{z}_1 + \biggl[ \frac{\partial \phi_k}{\partial z_1}\biggr]^\intercal \dot{z}_4\\
(k=3) & \qquad & \dot{z}_2 &\leftarrow \dot{z}_2 + \biggl[ \frac{\partial \phi_k}{\partial z_2}\biggr]^\intercal \dot{z}_3\\
&  & \dot{z}_1 &\leftarrow \dot{z}_1 + \biggl[ \frac{\partial \phi_k}{\partial z_2}\biggr]^\intercal \dot{z}_3 \\
(k=2) & \qquad & \dot{a} &\leftarrow \dot{a} + \biggl[ \frac{\partial \phi_k}{\partial a}\biggr]^\intercal \dot{z}_2\\
(k=1) & \qquad & \dot{a} &\leftarrow \dot{a} + \biggl[ \frac{\partial \phi_k}{\partial a}\biggr]^\intercal \dot{z}_1\\
 &  & \dot{b} &\leftarrow \dot{b} + \biggl[ \frac{\partial \phi_k}{\partial b}\biggr]^\intercal \dot{z}_1\\
\end{aligned} $$
<p>
We can explicitly write down these values and verify.
</p>
$$ \begin{aligned}
\dot{z}_5 &= 1 ,\\
\dot{z}_4 &= -\dot{z}_5 = -1 ,\\
\dot{z}_3 &= \dot{z}_5 = 1 ,\\
\dot{z}_2 &= \dot{z}_3 = 1 ,\\
\dot{z}_1 &= (\cos z_1) \dot{z}_4 + \dot{z}_3 = -\cos a^\intercal b + 1 ,\\
\dot{a} &= 2a \dot{z}_2 + b\dot{z}_1 = 2a + b(1 - \cos a^\intercal b) ,\\
\dot{b} &= a\dot{z}_1 = a(1 - \cos a^\intercal b).
\end{aligned} $$
</div>
</div>
<div id="outline-container-org54bdb65" class="outline-2">
<h2 id="org54bdb65">Jacobian-vector product</h2>
<div class="outline-text-2" id="text-org54bdb65">
<p>
Examing the chain-rule formulae used in forward-mode and reverse-mode
AD, we note that it suffices to calculate the Jacobian \(\partial \phi_k\). In
practice, however, we don't calculate Jacobian directly. Instead, we
calculate the so-called Jacobian-vector product (or vector-Jacobian
product).
</p>

<p>
Consider the function \(y=f(x)\) and assume \(x\in\mathbb{R}^n, y\in\mathbb{R}^m\).
</p>

<p>
For a vector \(v\in\mathbb{R}^n\), forward-mode AD can efficiently calculate
the Jacobian-vector product \([\partial f]v\) by
</p>
\begin{equation}\tag{chain-rule: JVP}
\dot{z}_k = \sum_{\alpha=0}^{N-1} \biggl[\frac{\partial \phi_k}{\partial z_\alpha}\biggr] \dot{z}_\alpha, \quad
\text{ where }\dot{z}_\alpha := \biggl[\frac{\partial z_\alpha}{\partial x}\biggr] v.
\end{equation}

<p>
For a vector \(v\in\mathbb{R}^m\), reverse-mode AD can efficiently calculate
the vector-Jacobian product \([\partial f]^\intercal v\) by
</p>
\begin{equation}\tag{chain-rule: VJP}
\dot{z}_k = \sum_{\beta=k+1}^N \biggl[\frac{\partial \phi_\beta}{\partial z_k}\biggr]^\intercal \dot{z}_\beta, \quad
\text{ where }\dot{z}_\beta := \biggl[ \frac{\partial y}{\partial z_\beta} \biggr]^\intercal v.
\end{equation}

<p>
There are two reasons why using Jacobian-vector product is always
better than using the Jacobian.
</p>

<ol class="org-ol">
<li>It requires less memory. Indeed, all calculations in (chain-rule:
VJP) are matrix-vector product, while all calculations in
(chain-rule: RAD) are matrix multiplications.</li>

<li>It remains efficient when parallized. If we want the full Jacobian
matrix, we can run the algorithm with different \(v_j=e_j\)
concurrently and then stack the result.</li>
</ol>

<p>
In deep learning, we use reverse-mode AD because the loss function is
a scalar function \[ \ell(\theta) = L(y, \hat{y}), \quad \text{ where }
y:=f(x;\theta). \] The gradient is indeed a vector-Jacobian product \[ \frac{\partial
\ell(\theta)}{\partial \theta} = \frac{\partial L}{\partial y} \frac{\partial y}{\partial \theta}. \]
</p>

<p>
Finally, we should mention that in our formulation all vectors can
generalized to tensors. Indeed, for a tensor \(x[\eta]\) of order \(n\),
where \(\eta \in \mathcal{I}(x) \subset \mathbb{N}^{n}\) is a multi-index, we can treat it as a vector
by iterating on \(\mathcal{I}(x)\). Moreover, for a function \(y=f(x)\) with \(y\) a
tensor of order \(m\), we can define the Jacobian-vector product by \[
\operatorname{JVP}_f(x, v)[\zeta] := \sum_{\eta \in \mathcal{I}(x)} \frac{\partial (f[\zeta])}{ \partial
(x[\eta])} v[\eta], \quad \zeta\in\mathcal{I}(y)\subset\mathbb{N}^m. \] Similarly, the vector-Jacobian
product is defined by \[ \operatorname{VJP}_f(x, v)[\eta] := \sum_{\zeta \in \mathcal{I}(y)}
\frac{\partial (f[\zeta])}{ \partial (x[\eta])} v[\zeta], \quad \eta\in\mathcal{I}(x)\subset\mathbb{N}^n. \] See
<a href="#org3190f16">the appendix</a> for an example illustrating how we apply this definition
to matrix multiplication \(f(A, B) = AB\).
</p>
</div>
</div>
<div id="outline-container-org4d46603" class="outline-2">
<h2 id="org4d46603">Implement forward-mode and reverse-mode AD</h2>
<div class="outline-text-2" id="text-org4d46603">
<p>
<a id="org880a6fd"></a>
</p>

<p>
In this section, we give the formal algorithm for differentiating a
function \(y=f(x)\).
</p>

<p>
Let's begin with forward-mode AD, which evaluates the
Jacobian-vector product \([\partial f(x)]v\) at a given point \(x\). Assume we
have access to the tape which records the sequence of elementary
operations and their inputs/output during the computation of the
target function \(y=f(x)\). The tables in section <a href="#org60a8cec">Examples: reverse-mode
AD for scalar functions</a> are examples of such tapes.
</p>

<p>
Forward-mode AD relies on the mathematical formula (chain-rule:
JVP). Starting with the gradient \(v\) of the initial input \(x\), we
traverse the tape in a forward direction, propagating gradients from
the inputs of \(\phi_k\) to its output via its JVP. The pseudocode of
forward-mode AD can be summarized as follows.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">forwardAD_along_tape</span>(inputs, call_tape, inputs_v, *, gradkey):
    <span style="color: #2aa198;">"""Forward propagate gradient starting at inputs. Initially the grad of</span>
<span style="color: #2aa198;">    inputs is set to inputs_v.  `gradkey` is a string used for the dict key. For</span>
<span style="color: #2aa198;">    a given tensor `a`, the grad is stored in `a.buffer[gradkey]`"""</span>
    <span style="color: #859900; font-weight: bold;">for</span> x, v <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">zip</span>(inputs, inputs_v):
        x.<span style="color: #657b83; font-weight: bold;">buffer</span>[gradkey] = v
    <span style="color: #859900; font-weight: bold;">for</span> k_inputs, k_outputs, k_phi, k_kwargs <span style="color: #859900; font-weight: bold;">in</span> call_tape:
        <span style="color: #268bd2;">grad_inputs</span> = [x.<span style="color: #657b83; font-weight: bold;">buffer</span>[gradkey] <span style="color: #859900; font-weight: bold;">for</span> x <span style="color: #859900; font-weight: bold;">in</span> k_inputs]
        k_outputs.<span style="color: #657b83; font-weight: bold;">buffer</span>[gradkey] = k_phi.jvp(
            k_inputs, k_outputs, grad_inputs, **k_kwargs
        )
</pre>
</div>

<p>
Reverse-mode AD relies on the mathematical formula (chain-rule:
VJP). Starting with the gradient \(v\) of the final output \(y\), we
traverse the tape in a reverse direction, propagating gradients from
the output of \(\phi_k\) to its inputs via its VJP. The pseudocode of
reverse-mode AD can be summarized as follows.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">reverseAD_along_tape</span>(y, call_tape, v, *, gradkey):
    <span style="color: #2aa198;">"""Backpropagate gradient starting at y. Initially the grad of y is set to</span>
<span style="color: #2aa198;">    v.  `gradkey` is a string used for the dict key. For a given tensor `a`, the</span>
<span style="color: #2aa198;">    grad is stored in `a.buffer[gradkey]`"""</span>
    y.<span style="color: #657b83; font-weight: bold;">buffer</span>[gradkey] = v
    <span style="color: #859900; font-weight: bold;">for</span> k_inputs, k_outputs, k_phi, k_kwargs <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">reversed</span>(call_tape):
        <span style="color: #268bd2;">grad_inputs</span> = k_phi.vjp(
            k_inputs, k_outputs, k_outputs.<span style="color: #657b83; font-weight: bold;">buffer</span>[gradkey], **k_kwargs
        )
        <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">accumulate grad</span>
        <span style="color: #859900; font-weight: bold;">for</span> x, grad <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">zip</span>(k_inputs, grad_inputs):
            x.<span style="color: #657b83; font-weight: bold;">buffer</span>[gradkey] += grad
</pre>
</div>

<p>
Comparing forward-mode AD and reverse-mode AD, a subtle difference is
the gradient accumulation process in the latter. In the formula
(chain-rule: JVP), the gradient \(\dot{z}_k\) is obtained by a single
call of the JVP of \(\phi_k\). Thus, in the algorithm, the value of
\(\dot{z}_k\) is computed in a single step of the iteration.  In the
formula (chain-rule: VJP), however, the gradient \(\dot{z}_k\) is
obtained by successive calls of the VJP of
\(\{\phi_\beta\}_{\beta=k+1}^N\). Consequently, in the algorithm, the value of
\(\dot{z}_k\) accumulates in several steps of the iteration.
</p>

<p>
See <a href="#orgb898803">the appendix</a> for an overview of my implementation in Python.
</p>
</div>
</div>
<div id="outline-container-orgf719635" class="outline-2">
<h2 id="orgf719635">Hessian-vector product</h2>
<div class="outline-text-2" id="text-orgf719635">
<p>
The Hessian matrix of a <i>scalar</i> function \(f\) can be defined by the
Jacobian matrix of \(\partial f\), \[ (\partial^2 f)_{i,j} := (\partial(\partial f))_{i,j} =
\frac{\partial}{\partial x_j}(\partial f)_i = \frac{\partial}{\partial x_j} \frac{\partial}{\partial x_i} f. \] Let
\(g(x)=\partial f(x)\). Calculating the Jacobian-vector product \([\partial g]v\) is
essentially calculating the Hessian-vector product \([\partial^2 f]v\). This
can be achieved by a combination of forward-mode AD and
reverse-mode AD.
</p>

<p>
Given the input \(x\), first calculate \(y=f(x)\) and record the
 operations <i>during this procedure</i> in a tape \(T_1\). Then, <i>take a new
 tape</i> \(T_2\). Use \(T_2\) to record the operations of applying
 forward-mode AD on \(T_1\) to calculate the Jacobian-vector product
 \(L=[\partial f]v\), which is a scalar in this case. Finally, apply
 reverse-mode AD on \(T_1 \cup T_2\) to obtain gradient \(\partial L = \partial([\partial f]v) =
 v^\intercal [\partial^2 f]\). This is exactly the Hessian-vector product \([\partial^2f]v\) if
 the Hessian matrix is symmetric.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">hvp_by_reverse_forwardAD</span>(f, inputs, v_vars, *, inputs_vars):
    <span style="color: #2aa198;">"""Calculate the Hessian-vector product of function `f` using</span>
<span style="color: #2aa198;">    reverse-on-forward mode automatic differentiation.</span>

<span style="color: #2aa198;">    `inputs_vars` is an subset of `inputs` specifying independent</span>
<span style="color: #2aa198;">    variables in the Hessian matrix.  `inputs_vars` aligns with the</span>
<span style="color: #2aa198;">    number of tensors in `v_vars`.</span>
<span style="color: #2aa198;">    """</span>
    <span style="color: #268bd2;">tape1</span> = []
    <span style="color: #859900; font-weight: bold;">with</span> my_func_tracker.track_func(<span style="color: #268bd2; font-weight: bold;">True</span>, tape=tape1):
        <span style="color: #268bd2;">y</span> = f(*inputs)  <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">do computations and track in tape1</span>

    <span style="color: #268bd2;">tape2</span> = []
    <span style="color: #859900; font-weight: bold;">with</span> my_func_tracker.track_func(<span style="color: #268bd2; font-weight: bold;">True</span>, tape=tape2):
        forwardAD_along_tape(inputs_vars, tape1, v_vars, gradkey=<span style="color: #2aa198;">"rfgrad1"</span>)
        <span style="color: #268bd2;">yy</span> = y.<span style="color: #657b83; font-weight: bold;">buffer</span>[<span style="color: #2aa198;">"rfgrad1"</span>]

    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">apply reverse-mode AD to yy</span>
    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">ATTENTION: we have to use a different gradkey to avoid modifying inputs</span>
    <span style="color: #93a1a1;">#            </span><span style="color: #93a1a1;">recorded in tape 2</span>
    reverseAD_along_tape(yy, tape1 + tape2, MyTensor(1.0), gradkey=<span style="color: #2aa198;">"rfgrad2"</span>)
    <span style="color: #859900; font-weight: bold;">return</span> [x.<span style="color: #657b83; font-weight: bold;">buffer</span>[<span style="color: #2aa198;">"rfgrad2"</span>] <span style="color: #859900; font-weight: bold;">for</span> x <span style="color: #859900; font-weight: bold;">in</span> inputs_vars]
</pre>
</div>

<p>
The above procedure is the reverse-on-forward mode AD for calculating the
Hessian-vector product. There are, of course, other procedures to
obtain the same result.
</p>

<ol class="org-ol">
<li><i>Forward-on-reverse mode AD.</i> After recording the operations of
\(y=f(x)\) in a tape \(T_1\), use a new tape \(T_2\) to record the
operations of applying reverse-mode AD on \(T_1\) to calculate
the gradient \(\partial f(x)\). Then, apply forward-mode AD on \(T_1 \cup
   T_2\) to calculate the Jacobian-vector product \([\partial (\partial f)]v\), which
is exactly the Hessian-vector product \([\partial^2f]v\).</li>

<li><i>Reverse-on-reverse mode AD</i>. After recording the operations of
\(y=f(x)\) in a tape \(T_1\), use a new tape \(T_2\) to record the
operations of 1) applying reverse-mode AD on \(T_1\) to calculate
the gradient \(\partial f(x)\) (which is a row vector in our notation); 2)
computing the Jacobian-vector product of \(L=[\partial f(x)]v\). Then, apply
reverse-mode AD on \(T_1 \cup T_2\) to calculate the gradient \(\partial L =
   \partial([\partial f]v)=v^\intercal [\partial^2f]\). This is exactly the Hessian-vector product
\([\partial^2f]v\) if the Hessian matrix is symmetric.</li>
</ol>

<p>
See <a href="#orgb898803">the appendix</a> for an overview of my implementation in Python.
</p>
</div>
</div>
<div id="outline-container-orgdac8ba1" class="outline-2">
<h2 id="orgdac8ba1">Conclusion</h2>
<div class="outline-text-2" id="text-orgdac8ba1">
<p>
AD refers to a general technique that generates numerical derivative
evaluations rather than derivative expressions. Backpropagation is a
special case of AD that runs in reverse mode. Besides reverse-mode AD,
there are forward-mode AD and even reverse-on-forward AD.
</p>

<p>
For a function \(y=f(x)\) with \(x\in\mathbb{R}^n\) and \(y\in\mathbb{R}^m\), forward-mode AD can
efficiently compute the Jacobian-vector product \([\partial f]v\) for any given
initial gradient \(v\in\mathbb{R}^n\), while reverse-mode AD can efficiently
compute the vector-Jacobian product \(v^\intercal [\partial f]\) for any given terminal
gradient \(v\in\mathbb{R}^m\). Deep learning uses reverse-mode AD because loss
functions are scalar functions, which means \(m=1\) and setting \(v=1\)
yields the gradient \(\partial f\) directly.
</p>

<p>
For a scalar function \(y=f(x)\), combining forward-mode AD and
reverse-mode AD can efficiently evaluate the Hessian-vector product
\([\partial^2 f]v\) for any vector \(v\).
</p>
</div>
</div>
<div id="outline-container-orgcd67d14" class="outline-2">
<h2 id="orgcd67d14">References</h2>
<div class="outline-text-2" id="text-orgcd67d14">
<p>
Books and Papers
</p>

<ul class="org-ul">
<li>Bishop, C. M., &amp; Bishop, H. (2024). Deep learning: Foundations and concepts. Springer.</li>
<li>Zhang, A., Lipton, Z. C., Li, M., &amp; Smola, A. J. (2023). Dive into Deep Learning. Cambridge University Press. <a href="https://d2l.ai">https://d2l.ai</a></li>
<li>Baydin, A. G., Pearlmutter, B. A., Radul, A. A., &amp; Siskind, J. M. (2018). Automatic differentiation in machine learning: A survey. Journal of Machine Learning Research, 18(153), 1‚Äì43.</li>
<li>Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., &amp; Lerer, A. (2017). Automatic differentiation in PyTorch. Neural Information Processing Systems. <a href="https://openreview.net/forum?id=BJJsrmfCZ">https://openreview.net/forum?id=BJJsrmfCZ</a></li>
</ul>

<p>
Online resources
</p>

<ul class="org-ul">
<li>Holmes, A. (2024). What's Automatic Differentiation? Hugging Face. <a href="https://huggingface.co/blog/andmholm/what-is-automatic-differentiation">https://huggingface.co/blog/andmholm/what-is-automatic-differentiation</a></li>
<li>Lashoun (2020). Crash Course on PyTorch and Autograd. <a href="https://lashoun.com/science/crash-course-on-pytorch-and-autograd/">https://lashoun.com/science/crash-course-on-pytorch-and-autograd/</a></li>
<li>Wang, J. (2021). Understanding pytorch‚Äôs autograd with <code>grad_fn</code> and <code>next_functions</code> <a href="https://amsword.medium.com/understanding-pytorchs-autograd-with-grad-fn-and-next-functions-b2c4836daa00">https://amsword.medium.com/understanding-pytorchs-autograd-with-grad-fn-and-next-functions-b2c4836daa00</a></li>
<li>Simple Grad. [Example implementation of reverse-mode autodiff].  <a href="https://colab.research.google.com/drive/1VpeE6UvEPRz9HmsHh1KS0XxXjYu533EC">https://colab.research.google.com/drive/1VpeE6UvEPRz9HmsHh1KS0XxXjYu533EC</a></li>
<li>PyTorch. Automatic Differentiation with torch.autograd. <a href="https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html">https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html</a></li>
<li>PyTorch. Jacobians, Hessians, hvp, vhp, and more: composing function
transforms. <a href="https://pytorch.org/tutorials/intermediate/jacobians_hessians.html">https://pytorch.org/tutorials/intermediate/jacobians_hessians.html</a></li>
</ul>
</div>
</div>
<div id="outline-container-org54ca842" class="outline-2">
<h2 id="org54ca842">Appendix: An inductive proof of reverse-mode AD formula</h2>
<div class="outline-text-2" id="text-org54ca842">
<p>
<a id="orgd4be40f"></a>
</p>

<p>
Consider the following computational graph
</p>
$$ \begin{aligned}
z_0 &= x, \\
z_k &= \phi_k(z_0, z_1, \ldots, z_{k-1}), \quad k=1, 2, \ldots, N, \\
y &= z_N.
\end{aligned} $$

<p>
Define functions \(\{f_k\}\) inductively from \(k=N\) to \(k=1\).
</p>

<ol class="org-ol">
<li>For \(k=N\), \(f_N:= \phi_N\).</li>

<li>Assume \(f_{k+1}\) has been defined. Then, \(f_k\) is defined by the
map \[ (z_0, z_1, \ldots, z_{k-1}) \mapsto f_{k+1}(z_0, z_1, \ldots,
   z_{k-1}, \phi_k(z_0, z_1, \ldots, z_{k-1})). \]</li>
</ol>

<p>
We prove by induction that for any \(1 \leq k \leq N-1\), \[ \frac{\partial
f_{k+1}}{\partial z_s} = \frac{\partial \phi_N}{\partial z_s} + \sum_{\beta=k+1}^{N-1} \frac{\partial
f_{\beta+1}}{\partial z_\beta}\frac{\partial \phi_\beta}{\partial z_s}, \quad 0 \leq s \leq k-1. \]
</p>

<ol class="org-ol">
<li>For \(k=N-1\), \(f_{k+1} = f_N = \phi_N\). The statement holds.</li>

<li>Assume the statement holds for function \(f_{k+1}\). We want to prove
that it holds for \(f_k\) too. According to the definition, \[
   f_k(z_0, z_1, \ldots, z_{k-1}) = f_{k+1}(z_0, z_1, \ldots, z_{k-1},
   \phi_k(z_0, z_1, \ldots, z_{k-1})). \] For any \(0 \leq s \leq k-1\), we
differentiate both sides of the equation w.r.t. \(z_s\) and obtain \[
   \frac{\partial f_k}{\partial z_s} = \frac{\partial f_{k+1}}{\partial z_s} + \frac{\partial f_{k+1}}{\partial
   z_k} \frac{\partial \phi_k}{z_s}. \] Substituting the hypothesis yields \[
   \frac{\partial f_k}{\partial z_s} = \frac{\partial \phi_N}{\partial z_s} + \sum_{\beta=k+1}^{N-1} \frac{\partial
   f_{\beta+1}}{\partial z_\beta}\frac{\partial \phi_\beta}{\partial z_s} + \frac{\partial f_{k+1}}{\partial z_k}
   \frac{\partial \phi_k}{z_s}. \] This shows that the statement holds for
\(f_k\).</li>

<li>By induction, the statement holds for any \(1 \leq k \leq N-1\).</li>
</ol>

<p>
The adjoint variables \(\partial y / \partial z_k\) is then formally defined by \(\partial
f_{k+1} / \partial z_k\). Moreover, \(\partial y / \partial z_N\) is defined by the identity
matrix.  Setting \(s=k-1\) in the statement yields \[ \frac{\partial y}{\partial z_k}
= \frac{\partial \phi_N}{\partial z_k} + \sum_{\beta=k+1}^{N-1} \frac{\partial y}{\partial z_\beta}\frac{\partial
\phi_\beta}{\partial z_k}.  \] This is exactly the formula uesd in reverse-mode AD.
</p>
</div>
</div>
<div id="outline-container-org4876bdc" class="outline-2">
<h2 id="org4876bdc">Appendix: JVP and VJP for linear maps</h2>
<div class="outline-text-2" id="text-org4876bdc">
<p>
<a id="org3190f16"></a>
</p>

<p>
Let \(y=f(A, B) = AB\) where \(A\) and \(B\) are two matrices. The output
\(y\) is also a matrix and \(y[i,k] = \sum_j A[i,j]B[j,k]\).
</p>

<p>
For a matrix \(v\) with the same shape as \(A\), \[
\operatorname{JVP}_f(A, v)[i,k] = \sum_{i',j} \frac{\partial y[i,k]}{\partial
A[i',j]}v[i',j] = \sum_j B[j, k]v[i, j] = vB. \]
</p>

<p>
For a matrix \(v\) with the same shape as \(B\), \[
\operatorname{JVP}_f(B, v)[i,k] = \sum_{j, k'} \frac{\partial y[i,k]}{\partial
B[j, k']}v[j, k'] = \sum_j A[i, j]v[j, k] = Av. \]
</p>

<p>
For a matrix \(v\) with the same shape as \(y\),  \[
\operatorname{VJP}_f(A, v)[i,j] = \sum_{i', k} \frac{\partial y[i',k]}{\partial
A[i, j]}v[i', k] = \sum_k B[j, k]v[i, k] = vB^\intercal. \]
</p>

<p>
For a matrix \(v\) with the same shape as \(y\),  \[
\operatorname{VJP}_f(B, v)[j, k] = \sum_{i, k'} \frac{\partial y[i,k']}{\partial
B[j, k]}v[i, k'] = \sum_i A[i, j]v[i, k] = A^\intercal v. \]
</p>
</div>
</div>
<div id="outline-container-org6fd5062" class="outline-2">
<h2 id="org6fd5062">Appendix: Overview of the Python Implementation</h2>
<div class="outline-text-2" id="text-org6fd5062">
<p>
<a id="orgb898803"></a>
</p>

<p>
The complete code is available at <a href="https://github.com/Dou-Meishi/audi">this github repo</a>.  The overall
framework follows <a href="https://colab.research.google.com/drive/1VpeE6UvEPRz9HmsHh1KS0XxXjYu533EC">this colab notebook</a>.  Basically, I implement the
following data structures.
</p>

<ol class="org-ol">
<li><p>
<code>MyFunction</code>. This models the elementary operations, such as addition,
multiplication, division, subtraction, matrix multiplication and so
on.  Instances of this class have attributes <code>vjp</code> and <code>jvp</code>, which are
responsible for computing vector-Jacobian products and
Jacobian-vector products.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">class</span> <span style="color: #b58900;">MyFunction</span>(<span style="color: #657b83; font-weight: bold;">object</span>):
    <span style="color: #2aa198;">"""Functions with vjp and jvp as attributes."""</span>

    <span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">__init__</span>(<span style="color: #859900; font-weight: bold;">self</span>, name, func, func_vjp=<span style="color: #268bd2; font-weight: bold;">None</span>, func_jvp=<span style="color: #268bd2; font-weight: bold;">None</span>):
        <span style="color: #859900; font-weight: bold;">self</span>.<span style="color: #268bd2;">name</span> = name
        <span style="color: #859900; font-weight: bold;">self</span>.<span style="color: #268bd2;">func</span> = func
        <span style="color: #859900; font-weight: bold;">self</span>.<span style="color: #268bd2;">vjp</span> = func_vjp
        <span style="color: #859900; font-weight: bold;">self</span>.<span style="color: #268bd2;">jvp</span> = func_jvp

    <span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">__call__</span>(<span style="color: #859900; font-weight: bold;">self</span>, *args, **kws):
        <span style="color: #859900; font-weight: bold;">return</span> <span style="color: #859900; font-weight: bold;">self</span>.func(*args, **kws)
</pre>
</div></li>

<li><p>
<code>MyFunctionTracker</code>. This is a decorator class to track inputs and
outputs of elementary operations, designed for the class
<code>MyFunction</code>.  The global instance <code>my_func_tracker</code> decorates the
<code>MyFunction.__call__</code> method. By doing so, each call of the
elementary operations will be recorded in
<code>my_func_tracker.call_tape</code>. Moreover, the method <code>track_func</code> returns
a context manager for conveniently toggling traking functionality.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">class</span> <span style="color: #b58900;">MyFuncTracker</span>(<span style="color: #657b83; font-weight: bold;">object</span>):
    <span style="color: #2aa198;">"""A decorator class to track function inputs and outputs.</span>
<span style="color: #2aa198;">    Designed for MyFunction class.</span>

<span style="color: #2aa198;">    Store recorded calls in attribute `call_tape`, a list of tuples</span>
<span style="color: #2aa198;">    representing (inputs_k, outputs_k, func_k).</span>

<span style="color: #2aa198;">    Args:</span>
<span style="color: #2aa198;">        do_track (bool): A boolean flag to determine whether tracking is enabled.</span>
<span style="color: #2aa198;">    """</span>

    <span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">__init__</span>(<span style="color: #859900; font-weight: bold;">self</span>, do_track: <span style="color: #657b83; font-weight: bold;">bool</span>):
        <span style="color: #859900; font-weight: bold;">self</span>.<span style="color: #268bd2;">do_track</span> = do_track
        <span style="color: #859900; font-weight: bold;">self</span>.<span style="color: #268bd2;">call_tape</span> = []

    <span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">__call__</span>(<span style="color: #859900; font-weight: bold;">self</span>, func):
        <span style="color: #2aa198;">"""Wrap the function to track inputs and outputs in `self.call_tape`.</span>
<span style="color: #2aa198;">        Expect func receive self as its first argument."""</span>
        <span style="color: #859900; font-weight: bold;">pass</span>

    <span style="color: #b58900;">@contextmanager</span>
    <span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">track_func</span>(<span style="color: #859900; font-weight: bold;">self</span>, do_track: <span style="color: #657b83; font-weight: bold;">bool</span>, tape: Union[<span style="color: #268bd2; font-weight: bold;">None</span>, <span style="color: #657b83; font-weight: bold;">list</span>] = <span style="color: #268bd2; font-weight: bold;">None</span>):
        <span style="color: #2aa198;">"""Context manager to enable or disable tracking within a block.  If</span>
<span style="color: #2aa198;">        tape is not None, store records in it. Otherwise, store records in</span>
<span style="color: #2aa198;">        `self.call_tape`."""</span>
        <span style="color: #859900; font-weight: bold;">pass</span>

<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">initialize a global function tracker</span>
<span style="color: #268bd2;">my_func_tracker</span> = MyFuncTracker(do_track=<span style="color: #268bd2; font-weight: bold;">True</span>)
<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">apply it to MyFunction class</span>
MyFunction.<span style="color: #268bd2;">__call__</span> = my_func_tracker(MyFunction.__call__)
</pre>
</div></li>

<li><p>
<code>MyTensor</code>. This models the variables for inputs and outputs of
elementary operations. Instances of this class comprise, <code>value</code> for
a NumPy array and <code>buffer</code> for a dictionary. Arithmetic operators of
this class are overloaded by <code>MyFunction</code> instances following the
instructions mentioned in the Python documentation <a href="https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types">Emulating
numeric types</a>.  By doing so, calculations involving <code>MyTensor</code>
instances will be recorded by <code>my_func_tracker</code> automatically.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">class</span> <span style="color: #b58900;">MyTensor</span>(<span style="color: #657b83; font-weight: bold;">object</span>):
    <span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">__init__</span>(<span style="color: #859900; font-weight: bold;">self</span>, value=0):
        <span style="color: #859900; font-weight: bold;">self</span>.<span style="color: #268bd2;">value</span> = np.asarray(value)
        <span style="color: #859900; font-weight: bold;">self</span>.<span style="color: #657b83; font-weight: bold;">buffer</span> = defaultdict(<span style="color: #859900; font-weight: bold;">self</span>.default_grad)

    <span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">default_grad</span>(<span style="color: #859900; font-weight: bold;">self</span>):
        <span style="color: #859900; font-weight: bold;">return</span> MyTensor(np.zeros_like(<span style="color: #859900; font-weight: bold;">self</span>.value))

    <span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">__add__</span>(<span style="color: #859900; font-weight: bold;">self</span>, other):
        <span style="color: #859900; font-weight: bold;">pass</span>
</pre>
</div></li>
</ol>

<p>
There are two notable things when implementing these classes.
</p>

<ol class="org-ol">
<li><p>
The attributes <code>vjp</code> and <code>jvp</code> of <code>MyFunction</code> instances are
functions. These functions accepts <code>MyTensor</code> inputs and return
<code>MyTensor</code> outputs, utilizing operations exclusively modelded by
<code>MyFunction</code> for tensor manipulation. Therefore, calculations
performed by <code>vjp</code> and <code>jvp</code> can be tracked by <code>my_func_tracker</code>, a
crucial aspect for computing high-order derivatives. Below is the
definition of addition along with its respective JVP/VJP
functionalities. Notably, the JVP propagates gradients of inputs to
gradient of outputs, while the VJP propagates gradient of outputs
to gradient of inputs.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">_add</span>(a: MyTensor, b: MyTensor) -&gt; MyTensor:
    <span style="color: #859900; font-weight: bold;">return</span> MyTensor(a.value + b.value)


<span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">_add_vjp</span>(
    inputs: <span style="color: #657b83; font-weight: bold;">list</span>[MyTensor], outputs: MyTensor, grad_outputs: MyTensor
) -&gt; <span style="color: #657b83; font-weight: bold;">list</span>[MyTensor]:
    <span style="color: #859900; font-weight: bold;">return</span> (grad_outputs <span style="color: #859900; font-weight: bold;">for</span> _ <span style="color: #859900; font-weight: bold;">in</span> inputs)


<span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">_add_jvp</span>(
    inputs: <span style="color: #657b83; font-weight: bold;">list</span>[MyTensor], outputs: MyTensor, grad_inputs: <span style="color: #657b83; font-weight: bold;">list</span>[MyTensor]
) -&gt; MyTensor:
    <span style="color: #859900; font-weight: bold;">return</span> grad_inputs[0] + grad_inputs[1]


<span style="color: #268bd2;">add</span> = MyFunction(<span style="color: #2aa198;">"Add"</span>, _add, func_vjp=_add_vjp, func_jvp=_add_jvp)
</pre>
</div></li>

<li><p>
Broadcast operations should be tracked too. As NumPy arrays can
broadcast their shape automatically, the function <code>_add</code> mentioned
earlier may receive a vector \(x\in\mathbb{R}^3\) and a scalar \(k\in\mathbb{R}\) and return
a new vector \(y=x+k\mathbb{1}\). Indeed, there is a broadcast operation \(k \mapsto
   k\mathbb{1}\in\mathbb{R}^3\) besides addition. Failing to record these broadcast
operations the gradient can lead to incorrect gradient
calculations. Below are the definitions of broadcast and addition
operations in <code>MyTensor</code>.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">class</span> <span style="color: #b58900;">MyTensor</span>(<span style="color: #657b83; font-weight: bold;">object</span>):

    <span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">__add__</span>(<span style="color: #859900; font-weight: bold;">self</span>, other):
        <span style="color: #859900; font-weight: bold;">if</span> <span style="color: #859900; font-weight: bold;">not</span> <span style="color: #657b83; font-weight: bold;">isinstance</span>(other, MyTensor):
            <span style="color: #268bd2;">other</span> = MyTensor(other)
        <span style="color: #268bd2;">a</span>, <span style="color: #268bd2;">b</span> = MyTensor.broadcast(<span style="color: #859900; font-weight: bold;">self</span>, other)
        <span style="color: #859900; font-weight: bold;">return</span> add(a, b)

    @<span style="color: #657b83; font-weight: bold;">staticmethod</span>
    <span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">broadcast</span>(*tensors):
        <span style="color: #268bd2;">shape</span> = np.broadcast_shapes(*[t.shape <span style="color: #859900; font-weight: bold;">for</span> t <span style="color: #859900; font-weight: bold;">in</span> tensors])
        <span style="color: #859900; font-weight: bold;">return</span> <span style="color: #657b83; font-weight: bold;">tuple</span>(t.expand(shape=shape) <span style="color: #859900; font-weight: bold;">for</span> t <span style="color: #859900; font-weight: bold;">in</span> tensors)

<span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">_expand</span>(a: MyTensor, *, shape: <span style="color: #657b83; font-weight: bold;">list</span>[<span style="color: #657b83; font-weight: bold;">int</span>]) -&gt; MyTensor:
    <span style="color: #859900; font-weight: bold;">return</span> MyTensor(np.broadcast_to(a.value, shape))

<span style="color: #268bd2;">expand</span> = MyFunction(<span style="color: #2aa198;">"Expand"</span>, _expand, func_vjp=_expand_vjp, func_jvp=_expand_jvp)
</pre>
</div></li>
</ol>

<p>
Finally, forward-mode AD and reverse-mode AD can be implemented as
follows.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">forwardAD</span>(
    f: Callable[[<span style="color: #657b83; font-weight: bold;">list</span>[MyTensor]], MyTensor],
    inputs: <span style="color: #657b83; font-weight: bold;">list</span>[MyTensor],
    inputs_v: <span style="color: #657b83; font-weight: bold;">list</span>[MyTensor],
    *,
    gradkey: <span style="color: #657b83; font-weight: bold;">str</span> = <span style="color: #2aa198;">"grad"</span>,
) -&gt; MyTensor:
    <span style="color: #2aa198;">"""Use forward-mode AD to compute the Jacobian-vector product of f.</span>
<span style="color: #2aa198;">    Return the gradient of f(dot(v, x)) evaluated at inputs.</span>

<span style="color: #2aa198;">    Args</span>
<span style="color: #2aa198;">    ----</span>
<span style="color: #2aa198;">    - `f`: The function to be differentiated.</span>

<span style="color: #2aa198;">    - `inputs`: Inputs of `f`.</span>

<span style="color: #2aa198;">    - `inputs_v`: A list of tensor matches `inputs`.</span>

<span style="color: #2aa198;">    - `gradkey`: A string used for the dict key. For a given tensor `a`,</span>
<span style="color: #2aa198;">           the grad is stored in `a.buffer[gradkey]`.</span>
<span style="color: #2aa198;">    """</span>
    <span style="color: #268bd2;">tape</span> = []
    <span style="color: #859900; font-weight: bold;">with</span> my_func_tracker.track_func(<span style="color: #268bd2; font-weight: bold;">True</span>, tape=tape):
        <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">do computations and track in tape</span>
        <span style="color: #268bd2;">y</span> = f(*inputs)
    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">forward propagate gradient starting at inputs</span>
    forwardAD_along_tape(inputs, tape, inputs_v, gradkey=gradkey)
    <span style="color: #859900; font-weight: bold;">return</span> y.<span style="color: #657b83; font-weight: bold;">buffer</span>[gradkey]

<span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">reverseAD</span>(
    f: Callable[[<span style="color: #657b83; font-weight: bold;">list</span>[MyTensor]], MyTensor],
    inputs: <span style="color: #657b83; font-weight: bold;">list</span>[MyTensor],
    v: MyTensor,
    *,
    gradkey: <span style="color: #657b83; font-weight: bold;">str</span> = <span style="color: #2aa198;">"grad"</span>,
) -&gt; <span style="color: #657b83; font-weight: bold;">list</span>[MyTensor]:
    <span style="color: #2aa198;">"""Use reverse-mode AD to compute the vector-Jacobian product of f.</span>
<span style="color: #2aa198;">    Return the gradient of dot(f, v) evaluated at inputs.</span>

<span style="color: #2aa198;">    Args</span>
<span style="color: #2aa198;">    ----</span>
<span style="color: #2aa198;">    - `f`: The function to be differentiated.</span>

<span style="color: #2aa198;">    - `inputs`: Inputs of `f`.</span>

<span style="color: #2aa198;">    - `v`: Any tensor matches the dim of `f`.</span>

<span style="color: #2aa198;">    - `gradkey`: A string used for the dict key. For a given tensor `a`,</span>
<span style="color: #2aa198;">           the grad is stored in `a.buffer[gradkey]`.</span>

<span style="color: #2aa198;">    Note</span>
<span style="color: #2aa198;">    ----</span>
<span style="color: #2aa198;">    The gradient of tensor `a` is accumulated in `a.bffer[gradkey]`, which is</span>
<span style="color: #2aa198;">    zero by default. However, this function does not check whether it is zero or</span>
<span style="color: #2aa198;">    not. It simply accumulates all gradient in it.</span>
<span style="color: #2aa198;">    """</span>
    <span style="color: #268bd2;">tape</span> = []
    <span style="color: #859900; font-weight: bold;">with</span> my_func_tracker.track_func(<span style="color: #268bd2; font-weight: bold;">True</span>, tape=tape):
        <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">do computations and track in tape</span>
        <span style="color: #268bd2;">y</span> = f(*inputs)
    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">backpropagate gradient starting at y</span>
    reverseAD_along_tape(y, tape, v, gradkey=gradkey)
    <span style="color: #859900; font-weight: bold;">return</span> [x.<span style="color: #657b83; font-weight: bold;">buffer</span>[gradkey] <span style="color: #859900; font-weight: bold;">for</span> x <span style="color: #859900; font-weight: bold;">in</span> inputs]
</pre>
</div>

<p>
There is also a <code>Test</code> class, which contains various functions and their
derivative functions derived by hand. For example, \[ f(w; X, y) =
L(s(Xw), y), \] where \[s(x) = 1/(1 + e^{-x}), \quad L(p, y) = -\sum[ y_i
\log p_i + (1-y_i)\log (1-p_i)]\] are sigmoid function and binary
cross entropy loss. \(X\in\mathbb{R}^{n\times m}\) is the predictor matrix and \(y\in\mathbb{R}^n\)
is the response. It is not hard to show that \[\partial_w f = (p-y)^\intercal X,
\quad \partial^2_w f = X^\intercal \Omega X,\] where \(p=s(Xw)\) and \(\Omega =
\operatorname{diag}(p(1-p))\).
</p>
</div>
</div>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-ai.html">ai</a> </div>
]]></description>
  <category><![CDATA[ai]]></category>
  <link>https://dou-meishi.github.io/org-blog/2024-09-24-ImplementBP/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2024-09-24-ImplementBP/notes.html</guid>
  <pubDate>Tue, 24 Sep 2024 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Characteristic Functions and Central Limit Theorem]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orga2aeb63">Basics of Characteristic Functions</a>
<ul>
<li><a href="#org1d379a8">Related to Fourier Transform</a></li>
<li><a href="#orgb5d22ac">Related to Moments</a></li>
<li><a href="#org43bdfe0">Related to Weak Convergence</a></li>
</ul>
</li>
<li><a href="#org7c20656">Central Limit Theorem and Gaussian Distribution</a></li>
</ul>
</div>
</nav>
<p>
Strong Law of Large Numbers (SLLN) and Central Limit Theorem (CLT) are
two significant results in probability theory and statistics. Both
theorems concern the asymptotic behavior of the sum of i.i.d. random
variables, but they follow different scaling. SLLN examines the case
when the sum is divided by \(n\), while CLT considers the case when the
sum is divided by \(\sqrt{n}\). Their conclusions are also
different. SLLN asserts that the considered random variable will
converge to a constant <i>almost surely</i>, while CLT ensures that the
<i>distribution</i> of the considered random variable converge to a Gaussian
distribution. With the help of characteristic functions, we are able
to prove the CLT straightforwardly and see how a Gaussian distribution
comes out.
</p>

<p>
In this post, we start by introducing the definition of characteristic
functions (c.f.)  of random variables, and some basic properties like
boundness, uniformly continuity, and usage in computing moments. Once
familiar with the definition, we use c.f.s to obtain many useful
results.
</p>

<ol class="org-ol">
<li>Two random variables follow the same distribution if and only if
they have the same c.f.. This result is particularly useful, as it
ensures that it is possible to deduce the distribution of a random
variable from its c.f.s. For example, if we note a random variable
has the same c.f. as a normal distribution, then we can conclude
that it must follows this normal distribution.</li>

<li>Derivatives of c.f.s are proportional to the moments. This result
is particularly useful in calculating the \(k\)-th moment from the
c.f.. by taking \(k\)-th derivative. Moreover, we are able to give
a Taylor expansion of c.f.s if we know the moments.</li>

<li>Convergence in distribution of random variables is equivalent to
pointwise convergence of c.f.s. This result is particularly useful
in studying asymptotic behaviors. For example, in CLT we want to
study the asymptotic behavior of the sum of i.i.d. random
variables. With the help of c.f.s, we need only to study the limit
of c.f.s. Indeed, we will see that this limit will be the c.f. of a
normal distribution, which concludes the CLT.</li>
</ol>

<p>
<i>Suggested readings:</i> Durrett (2019, pp. 108&#x2013;118), Billingsley (2008,
pp. 342&#x2013;351), and Schilling (2017, pp. 214&#x2013;220).
</p>
<div id="outline-container-orga2aeb63" class="outline-2">
<h2 id="orga2aeb63">Basics of Characteristic Functions</h2>
<div class="outline-text-2" id="text-orga2aeb63">
<p>
<i>Definition.</i> The <i>characterstic function (c.f.)</i> \(\varphi(t)\) of a random
variable \(X\) is defined by \[ \varphi_X(t) = \mathbb{E}[e^{itX}] = \mathbb{E}[\cos tX] +
i\mathbb{E}[\sin tX]. \] In general, the c.f. of a finite measure \(\mu\) is \[
\varphi_\mu(t) = \int e^{itx}\, \mu(dx) = \int \cos tx \, \mu(dx) + i \int \sin tx\,
\mu(dx).\] Naturally, the definition can be applied to integrable
functions, which coincides with the <i>inverse Fourier transform</i>: \[
\varphi_f(t) = \int f(x) e^{itx} \, dx = \int f(x) \cos tx \, dx + i \int f(x) \sin
tx \, dx. \] Clearly, when \(f\) is integrable, \(\varphi(t)\) exists for all
\(t\), though \(\varphi\) might not be integrable.
</p>

<p>
For random variables, c.f.s always exist as \(\cos tX\) and \(\sin tX\)
are bounded and thus integrable. One benefit of introducing c.f.s is
the convenience to handle sum of two independent r.v.s. Let \(X_1\) and
\(X_2\) be two independent r.v.s. Then the sum \(X_1+X_2\) has c.f.  \[
\varphi_{X_1+X_2}(t) = \mathbb{E}[e^{it(X_1+X_2)}] = \mathbb{E}[e^{itX_1}] \cdot \mathbb{E}[e^{itX_2}] =
\varphi_{X_1}(t) \varphi_{X_2}(t). \] This result is known as the <i>convolution
theorem</i>, as the distribution of \(X_1+X_2\) is the convolution of
distributions of \(X_1\) and \(X_2\)<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>.
</p>

<p>
Characteristic functions have many useful properties. For example, it
is always bounded by 1, i.e., \(|\varphi(t)| \leq 1\). Clearly, \(\varphi(0) = 1\).  It
is also uniformly continuous. To see this, \[ \varphi(t+h) - \varphi(t) =
\mathbb{E}[e^{i(t+h)X} - e^{itX}] = \mathbb{E}[e^{itX}(e^{ihX} - 1)]. \] As \(|e^{ihX} -
1| \leq 2\), we can apply the dominated convergence theorem and conclude
that the integral converges to 0 as \(h \to 0\).  Moreover, we observe
that \(|e^{it} - 1| = \bigl| \int_0^t ie^{ix} \, dx \bigr| \leq t\) holds for
all real \(t\). Hence, \(|e^{ihX} - 1| / h \leq X\). If \(X\) is integrable,
then by dominated convergence theorem we can interchange the limit and
and the integral, i.e., \[ \lim_{h\to0} \frac{\varphi(t+h) - \varphi(t)}{h} =
\lim_{h\to0} \mathbb{E}\biggl[ e^{itX} \frac{e^{ihX} - 1}{h} \biggr] = \mathbb{E}\biggl[
e^{itX} \lim_{h\to0} \frac{e^{ihX} - 1}{h} \biggr] = \mathbb{E}[iX e^{itX}]. \]
We conclude that \(\varphi\) is differentiable when \(X\) is
integrable. Similarly, we can show that \(\varphi'(t)\) is uniformly
continuous in this case. Moreover, \(\varphi'(0) = i\mathbb{E}[X]\).
</p>

<p>
<i>Example (Dirac distribution).</i> Let \(X\) be a random variable with Dirac
distribution, i.e., \(\mathbb{P}(X=0) = 1\). By definition, its c.f. is \(\varphi(t) =
\mathbb{E}[e^{itX}] = 1\).
</p>

<p>
<i>Example (Two-point mass distribution).</i> Let \(X\) be the result of
flipping a coin, i.e., \(\mathbb{P}(X=1) = \mathbb{P}(X=-1) = 1/2\). By definition, its
c.f. is \(\varphi(t) = \mathbb{E}[e^{itX}] = (e^{it} + e^{-it})/2 = \cos t\).
</p>

<p>
<i>Example (Uniform distribution).</i> Let \(X\) follow the uniform
distribution on \([-a, a]\). By definition, its c.f. is \[\varphi(t) =
\mathbb{E}[e^{itX}] = \frac{1}{2a} \int e^{itx} \mathbb{1}( -a \leq x \leq a) \, dx = \frac{\sin
at}{at}.\]
</p>

<p>
<i>Example (Poisson distribution).</i> Let \(X\) follow the Poisson
distribution, i.e., \(\mathbb{P}(X=k) = e^{-\lambda}\lambda^k / k!\). By definition, its
c.f. is \[\varphi(t) = \mathbb{E}[e^{itX}] = \sum_{k=0}^\infty e^{itk} e^{-\lambda} \lambda^k / k! =
e^{-\lambda} \sum_{k=0}^\infty (\lambda e^{it})^k / k! = \exp(\lambda( e^{it} - 1)).\]
</p>

<p>
<i>Example (Gaussian distribution).</i> Let \(X\) follow the standard Gaussian
distribution \(\mathcal{N}(0, 1)\). By definition, its c.f. is<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup> \[\varphi(t) =
\mathbb{E}[e^{itX}] = \frac{1}{\sqrt{2\pi}} \int e^{itx} e^{-x^2/2} \, dx =
e^{-t^2/2} \int \frac{1}{\sqrt{2\pi}} \exp(-(x-it)^2/2) \, dx =
e^{-t^2/2}.\]
</p>

<p>
In summary, we can draw the following table.
</p>

<table>


<colgroup>
<col  class="org-left">

<col  class="org-left">

<col  class="org-left">
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Distribution</th>
<th scope="col" class="org-left">Density</th>
<th scope="col" class="org-left">Characteristic Functions</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Dirac</td>
<td class="org-left">\(\delta\)</td>
<td class="org-left">1</td>
</tr>

<tr>
<td class="org-left">Two-point mass</td>
<td class="org-left">\((\delta_{+1} + \delta_{-1})/2\)</td>
<td class="org-left">\(\cos t\)</td>
</tr>

<tr>
<td class="org-left">Uniform</td>
<td class="org-left">\(\mathbb{1}_{[-a,a]}/(2a)\)</td>
<td class="org-left">\(\frac{\sin at}{at}\)</td>
</tr>

<tr>
<td class="org-left">Poisson</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">\(\exp(\lambda(e^{it}-1))\)</td>
</tr>

<tr>
<td class="org-left">Gaussian</td>
<td class="org-left">\(\mathcal{N}(0, 1)\)</td>
<td class="org-left">\(\exp(-t^2/2)\)</td>
</tr>
</tbody>
</table>

<p>
Note that except the c.f. of Gaussian distributions, none of these
c.f.s is integrable.
</p>

<p>
We conclude this subsection by an important lemma, which states that
the c.f. of any integrable function must vanish when \(t\to\infty\). The proof
is simple but irrelevant to our main topic and thus is omitted. For
interesting readers, please see, Billingsley's book (2008, p. 345) or
Schilling's book (2017, pp. 221&#x2013;222).
</p>

<p>
<i>Lemma (Riemann-Lebesgue).</i> If \(\mu\) has a density, then \(|\varphi_\mu(t)|\to0\) when
\(t\to\infty\).
</p>

<p>
<i>Remark.</i> Here \(\mu\) has a density is equivalent to say \(\mu\) is absolutely
continuous with respect to the Lebesgue measure, i.e., \(\mu(dx) = f\,
dx\) where \(f\) is integrable. A counterexample is the Dirac measure,
which is of course not absolutely continuous w.r.t. the Lebesgue
measure.
</p>
</div>
<div id="outline-container-org1d379a8" class="outline-3">
<h3 id="org1d379a8">Related to Fourier Transform</h3>
<div class="outline-text-3" id="text-org1d379a8">
<p>
Why we want to study characteristic functions? One reason is that the
c.f. fully characterizes a finite measure. In fact, any finite measure
can be recovered from its c.f.. Consequently, two finite measures
equal if and only if their c.f.s equal. Hence, it is possible to
determine distributions of random variables by looking at their c.f.s.
</p>

<p>
First, we introduce the <i>inversion theorem</i>, which provides a way to
recover the measure from its c.f..
</p>

<p>
<i>Theorem (Inversion).</i> Let \(\varphi\) be the c.f. of a finite measure
\(\mu\). Then, \[ \lim_{T\to\infty} \frac{1}{2\pi} \int_{-T}^T \frac{e^{-ita} -
e^{-itb}}{it} \varphi(t) \, dt = \mu(a, b) + \frac{1}{2}\mu(\{a\}) +
\frac{1}{2}\mu(\{b\}). \]
</p>

<p>
<i>Remark.</i> The integral on the left-hand side is improper when \(\varphi\) is not
integrable, e.g., when \(\varphi(t)\equiv1\). Nevertheless, the limit exists (this
existence is part of the conclusion). Indeed, take \(\mu\) to be the Dirac
measure, then \(\mu(-c, c)=1\) for all positive number \(c\). The integral
on the left-hand side becomes \(\frac{1}{\pi} \int_{-T}^T \frac{\sin ct}{t}
\, dt\), which converges to 1 as \(T\to \infty\).
</p>

<p>
<i>Proof.</i> The proof is based on the direct calculation of the left-hand
side. Consider \(f(x, t) = (e^{it(x-a)} - e^{it(x-b)}) / (it)\). Noting
that \(|f(x,t)| \leq |e^{it(b-a)}| / |t| \leq |b-a|\), we conclude that \(f(x,
t)\) is integrable on the product measure space \(\mu(dx) \otimes dt\). By
Fubini's theorem, we can interchange the order of integrals
</p>

$$ \begin{aligned}
\frac{1}{2\pi} \int_{-T}^T \frac{e^{-ita} - e^{-itb}}{it} \varphi(t) \, dt
&= \frac{1}{2\pi} \int_{-T}^T dt \int \mu(dx) \,
        \frac{e^{it(x-a)} - e^{it(x-b)}}{it} \\
&= \int \mu(dx) \, \frac{1}{2\pi} \int_{-T}^T dt \,
        \frac{e^{it(x-a)} - e^{it(x-b)}}{it} \\
&=: \int \mu(dx) \, R(x; T).
\end{aligned} $$

<p>
The proof is completed by noting that \(R(x; T)\) is bounded and
converges to<sup><a id="fnr.3" class="footref" href="#fn.3" role="doc-backlink">3</a></sup> \(\mathbb{1}_{(a,b)} + \frac{1}{2}\mathbb{1}_{\{a\}} +
\frac{1}{2}\mathbb{1}_{\{b\}}\) as \(T \to \infty\). By dominated convergence theorem,
the desired conclusion holds.
</p>

<p>
Q.E.D.
</p>

<p>
The inversion theorem implies the uniqueness of c.f.s. Assume two
finite measures \(\mu\) and \(\nu\) have the same c.f.. Then they agree on all
these intervals \((a, b)\) such that \(\mu(\{a\}) = \mu(\{b\}) = \nu(\{a\}) =
\mu(\{b\}) = 0\). As such endpoints are at most countable (otherwise \(\mu\)
and \(\nu\) cannot be finite), these intervals can generate the Borel
\(\sigma\)-algebra, implying that \(\mu\) and \(\nu\) agree on all Borel sets.
</p>

<p>
<i>Corollary (Uniqueness).</i> Two finite measures equal if and only if their
c.f.s equal.
</p>

<p>
Consequently, we can conclude that two random variables follow the
same distribution if and only if they have the same c.f.. In other
words, we can deduce the distribution of a random variable from its
c.f.. The previous subsection shows that the standard normal
distribution \(\mathcal{N}(0, 1)\) has c.f. \(\exp(-t^2/2)\). In general, the normal
distribution \(\mathcal{N}(\mu, \sigma^2)\) has c.f. \(\exp(it\mu - \sigma^2 t^2 / 2)\). Assume
\(X_1\) and \(X_2\) are independent and normally distributed with mean
\(\mu_1, \mu_2\) and variance \(\sigma_1^2\) and \(\sigma_2^2\) respectively. Then \(aX_1 +
bX_2\) has c.f.
</p>

$$ \begin{aligned}
\mathbb{E}[e^{it(aX_1 + bX_2)}]
&= \mathbb{E}[e^{i(at)X_1}] \cdot \mathbb{E}[e^{i(bt)X_2}] \\
&= \exp\Bigl(
    i(at)\mu_1 - \sigma_1^2 (at)^2 / 2 + i(bt)\mu_2 - \sigma_2^2 (bt)^2 / 2 \Bigr) \\
&= \exp\Bigl(
    it(a\mu_1 + b\mu_2) - (a^2\sigma_1^2 + b^2\sigma_2^2) t^2 / 2 \Bigr).
\end{aligned} $$

<p>
This concludes that \(aX_1 + bX_2\) has the same c.f. as \(\mathcal{N}(a\mu_1 + b\mu_2,
a^2\sigma_1^2 + b^2\sigma_2^2)\).
</p>

<p>
<i>Corollary (Normal).</i> Linear combinations of independent normal
variables are normal.
</p>

<p>
Finally, we relate the inversion theorem to <i>Fourier transform</i>. As we
see that the inverse Fourier transform pushes a density function to
its c.f., the Fourier transform recovers the density function from a
c.f..  Assume the c.f. \(\varphi\) of a finite measure \(\mu\) is integrable. Then
the integral on the left-hand side can be extended to the real line as
the integrand is integrable. Moreover, we can apply Fubini's theorem
to rewrite the integral
</p>

$$ \begin{aligned}
\frac{1}{2\pi} \int \frac{e^{-ita} - e^{-itb}}{it} \varphi(t) \, dt
&= \frac{1}{2\pi} \int dt \int_{a}^b \, dx \, e^{-itx} \varphi(t) \\
&= \int_{a}^b dx \, \frac{1}{2\pi} \int dt  \, e^{-itx} \varphi(t).
\end{aligned} $$

<p>
<i>Corollary (Fourier Transform).</i> If the c.f. \(\varphi\) is integrable, then \(\mu\)
has a density function \[ f(x) = \frac{1}{2\pi} \int e^{-itx} \varphi(t) \,
dt. \] Moreover, \(f\) is bounded and uniformly continuous (just like
\(\varphi\)).
</p>
</div>
</div>
<div id="outline-container-orgb5d22ac" class="outline-3">
<h3 id="orgb5d22ac">Related to Moments</h3>
<div class="outline-text-3" id="text-orgb5d22ac">
<p>
Studying c.f.s can also help us determine the moments of a
distribution. We have seen that if a random variable \(X\) is
integrable, then \(\varphi'(0) = i\mathbb{E}[X]\). In this subsection, we will extend
this result to \(k\)-th moment, i.e., \(\varphi^{(k)}(0) = i^k \mathbb{E}[X^k]\) if
\(X^k\) is integrable.
</p>

<p>
First, we need a technical lemma to estimate the remainder of the
Taylor expansion of \(e^{i\xi}\). Recall that according to integration by
parts<sup><a id="fnr.4" class="footref" href="#fn.4" role="doc-backlink">4</a></sup>, \[ e^{i\xi} - 1 - \sum_{k=1}^n \frac{i^k}{k!} \xi^k = \int_0^\xi
\frac{(\xi-t)^n}{n!} i^{n+1} e^{it} \, dt. \] Assume \(\xi > 0\). The
remainder can be bounded by \[ \biggl| e^{i\xi} - 1 - \sum_{k=1}^n
\frac{i^k}{k!} \xi^k \biggr| \leq \int_0^\xi \frac{(\xi -t)^n}{n!} \, dt =
\frac{\xi^{n+1}}{(n+1)!}. \] On the other hand, it can also be bounded
by \[ \biggl| e^{i\xi} - 1 - \sum_{k=1}^n \frac{i^k}{k!} \xi^k \biggr| \leq
\biggl| e^{i\xi} - 1 - \sum_{k=1}^{n-1} \frac{i^k}{k!} \xi^k \biggr| +
\frac{\xi^n}{n!} \leq 2\frac{\xi^{n}}{n!}. \] It is easy to generalize the
bound to the case \(\xi \leq 0\) and obtain the following lemma<sup><a id="fnr.5" class="footref" href="#fn.5" role="doc-backlink">5</a></sup>.
</p>

<p>
<i>Lemma.</i> For any real \(\xi\), the remainder of the \(n\)-th order Taylor
expansion of \(e^{i\xi}\) can be bounded by \[ \biggl| e^{i\xi} - 1 -
\sum_{k=1}^n \frac{i^k}{k!} \xi^k \biggr| \leq \min\biggl( 2\frac{|\xi|^n}{n!},
\frac{|\xi|^{n+1}}{(n+1)!} \biggr). \]
</p>

<p>
We can use this lemma to obtain the Taylor expansion of c.f.s. Let \(X\)
be a random variable such that \(X^{n}\) is integrable. Then,
</p>

$$ \begin{aligned}
\biggl| \mathbb{E}[e^{itX}] - 1 - \sum_{k=1}^n \frac{(it)^k}{k!} \mathbb{E}[X^k] \biggr|
&\leq \mathbb{E}\biggl| e^{itX} - 1 - \sum_{k=1}^n \frac{(it)^k}{k!} X^k \biggr| \\
&\leq |t^n| \mathbb{E}\biggl[ \min\biggl(
   \frac{2|X|^n}{n!}, \frac{|t||X|^{n+1}}{(n+1)!} \biggr) \biggr].
\end{aligned} $$

<p>
Denote by \(c_k = i^k \mathbb{E}[X^k] / k!\). We can show that the remainder has
order \(o(t^n)\).  \[ \lim_{t\to0} \frac{\biggl| \varphi(t) - 1 - \sum_{k=1}^n c_k
t^k \biggr|}{|t^n|} \leq \lim_{t\to0} \mathbb{E}\biggl[ \min\biggl( \frac{2|X|^n}{n!},
\frac{|t||X|^{n+1}}{(n+1)!} \biggr) \biggr]. \] Indeed, the integrand
is bounded by \(2|X|^n/n!\), which is integrable. By dominated
convergence theorem, we can interchange the order of limit and
expectation, concluding that the expectation converges to 0. Note that
this argument does not requires \(X^{n+1}\) is integrable.
</p>

<p>
<i>Theorem.</i> If \(X^n\) is integrable, then in the neighborhood of \(t=0\),
the c.f. has Taylor expansion \[ \varphi(t) = 1 + \sum_{k=1}^n c_k t^k +
o(t^n), \quad\text{where}\quad c_k = i^k \mathbb{E}[X^k] / k!. \]
</p>

<p>
This result inspires us to compute the \(k\)-th moment \(\mathbb{E}[X^k]\) by
taking \(k\)-th derivative of \(\varphi\). We have shown that \(\varphi'(t) =
\mathbb{E}[iXe^{itX}]\) when \(X\) is integrable. Repeating the argument can show
that \(\varphi^{(k)}(t) = \mathbb{E}[(iX)^k e^{itX}]\) if \(X^k\) is integrable<sup><a id="fnr.6" class="footref" href="#fn.6" role="doc-backlink">6</a></sup>.
</p>

<p>
<i>Corollary.</i> If \(|X|^k\) is integrable, then \(\varphi\) is \(k\)-th
differentiable and \(\varphi^{(k)}(0) = i^k \mathbb{E}[X^k]\). Moreover, the \(k\)-th
derivative is bounded, uniformly continuous, and has an explicit form
\(\varphi^{(k)}(t) = \mathbb{E}[(iX)^k e^{itX}]\).
</p>
</div>
</div>
<div id="outline-container-org43bdfe0" class="outline-3">
<h3 id="org43bdfe0">Related to Weak Convergence</h3>
<div class="outline-text-3" id="text-org43bdfe0">
<p>
Finally, c.f.s are useful in studying limiting distributions. This is
due to <i>the continuity theorem</i>. The proof utilizes the concept of
tightness of measures and thus is omitted here; see, e.g.,
Billingsley's book (2008, pp. 349&#x2013;350) or Durrett's book (2019,
pp. 114&#x2013;115).
</p>

<p>
<i>Theorem (Continuity theorem).</i> Let \(\mu_n\) and \(\mu\) be finite measures
with c.f.s \(\varphi_n\) and \(\varphi\). Then \(\mu_n \Rightarrow \mu\) if and only if \(\varphi_n(t) \to
\varphi(t)\) for each \(t\).
</p>

<p>
<i>Remark.</i> The condition requires that the limiting function \(\lim
\varphi_n(t)\) is indeed a c.f. of some finite measure \(\mu\). However, it might
not be true. For example, let \(\varphi_n(t) = \exp(-nt^2/2)\) be the c.f. of
the Gaussian distribution \(\mathcal{N}(0, n)\). Then \(\lim \varphi_n(t) =
\mathbb{1}_{\{0\}}(t)\), which is clearly not a c.f. (as any c.f. must be
uniformly continuous). Thus, \(\mu_n\) does not converge weakly.
</p>

<p>
The continuity theorem relates the pointwise convergence of c.f.s with
the weak convergence of probability measures. In the following
section, we will use it to study the limiting distribution of sum of
i.i.d. random variables through studying the limiting c.f., as it is
much easier to work with product of c.f.s than the convolution of
distributions.
</p>
</div>
</div>
</div>
<div id="outline-container-org7c20656" class="outline-2">
<h2 id="org7c20656">Central Limit Theorem and Gaussian Distribution</h2>
<div class="outline-text-2" id="text-org7c20656">
<p>
With the help of c.f.s, it is not hard to find out the sum of
i.i.d. random variables follows a Gaussian distribution. Let \(X_n\) be
i.i.d. random variables with mean \(\mu\) and finite variance \(\sigma^2 <
\infty\). Let \[ Z_n = \frac{\sum_{k=1}^n X_k - n\mu}{\sqrt{n}\sigma}. \] Now we show
that the limiting distribution of \(Z_n\) is the standard Gaussian
distribution \(\mathcal{N}(0, 1)\).
</p>

<p>
Let \(Y_n = (X_n - \mu) / \sigma\). Then \(Y_n\) are i.i.d. random variables with
 mean 0 and variance 1.  Let \(\varphi_n\) be their c.f.s. Of course, as \(Y_n\)
 are i.i.d., their c.f. are the same \(\varphi_n \equiv \varphi\). By the continuity
 theorem, it is sufficient to show the c.f. of \(Z_n\) converges to
 \(\exp(-t^2/2)\).  \[ \mathbb{E}[\exp(itZ_n)] = \mathbb{E}\biggl[ \exp\biggl( it
 \frac{\sum_{k=1}^n Y_k}{\sqrt{n}} \biggr) \biggr] =
\prod_{k=1}^n \mathbb{E}\biggl[\exp\biggl(it \frac{Y_k}{\sqrt{n}} \biggr) \biggr] =
 \biggl[\varphi\biggl(\frac{t}{\sqrt{n}}\biggr)\biggr]^n. \] As \(Y_k\) has
 mean 0 and finite variance 1, it must be square integrable. Thus, its
 c.f. has Taylor expansion \[ \varphi(t) = 1 - \frac{1}{2}t^2 + o(t^2).\]
 Hence, we can continue to caculate the c.f. of \(Z_n\).  \[
 \mathbb{E}[\exp(itZ_n)] = \biggl[\varphi\biggl(\frac{t}{\sqrt{n}}\biggr)\biggr]^n =
 \biggl[ 1 - \frac{t^2}{2n} + o\biggl(\frac{t^2}{n}\biggr) \biggr]^n \to
 \exp(-t^2/2). \] The final limit exists as \((1 + c/n + o(1/n))^n \to
 e^c\) for all real number \(c\)<sup><a id="fnr.7" class="footref" href="#fn.7" role="doc-backlink">7</a></sup>.
</p>

<p>
<i>Theorem. (Central limit theorem).</i> Let \(X_1, X_2, \ldots\) be
i.i.d. random variables with mean \(\mu\) and positive finite variance
\(\sigma^2\). Then \[ \frac{\sum_{k=1}^nX_k - n\mu}{\sigma\sqrt{n}} \Rightarrow \mathcal{N}(0, 1). \]
</p>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
For any Borel set \(B\), there is (the last equality holds
because of independence) \[ \mu_{X_1+X_2}(B) = \mathbb{P}(X_1 + X_2 \in B) =
\mathbb{E}[\mathbb{1}(X_1 + X_2 \in B)] = \int \mathbb{1}(x_1 + x_2 \in B) \, \mu_{X_1}(dx_1)
\mu_{X_2}(dx_2). \] In general, the <i>convolution</i> of two finite measure is
defined by \[ \mu_1 \star \mu_2 (B) := \int \mathbb{1}_B(x+y) \, \mu_1(dx) \mu_2(dy). \]
The convolution theorem states that the c.f. of \(\mu_1 \star \mu_2\) is
exactly \(\varphi_{\mu_1}(t) \varphi_{\mu_2}(t)\). For a direct proof, see Schilling's
book (2017, p. 221).
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
The normal density function with mean \(it\) and variance 1
indeed integrals to 1 for all real \(t\), but this conclusion requires
proof. The rigorous treatment is showing the c.f. of the standard
normal distribution is indeed \(e^{-t^2/2}\). As \(X\) is integrable, the
c.f. is continuously differentiable and
</p>

$$ \begin{aligned}
\varphi'(t) &= \mathbb{E}[iXe^{itX}] \\
&= \int ix e^{itx} \frac{1}{\sqrt{2\pi}} e^{-x^2/2} \, dx \\
&= \int -i e^{itx} \, d\frac{1}{\sqrt{2\pi}} e^{-x^2/2} \\
&= -\int t e^{itx} \frac{1}{\sqrt{2\pi}} e^{-x^2/2}  \, dx \
&= -t \varphi(t).
\end{aligned} $$

<p class="footpara">
Let \(\xi(t) = \varphi(t) \exp(t^2/2)\). Then \(\xi(0) = 1\) and \(\xi'(t) = [\varphi'(t) +
t\varphi(t)]\exp(t^2/2) \equiv 0\). Hence, \(\xi(t) = \varphi(t) \exp(t^2/2) \equiv 1\).
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3" role="doc-backlink">3</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
In order to see this, we prove the following lemma first.
</p>

<blockquote>
<p>
<i>Lemma.</i> The sinc function \(\frac{\sin x}{x}\) is not integrable but its
improper Riemann integral exists \[\lim_{T\to\infty} \int_{-T}^T \frac{\sin
x}{x} \, dx = \pi.\]
</p>
</blockquote>

<p class="footpara">
This sinc function is a sequence of "bumps" of decreasing size. The
 \(n\)-th "bump" bounds area on the order of \(1/n\), but \(\sum 1/n =
 \infty\). To see this,
</p>

$$ \begin{aligned}
\int \biggl| \frac{\sin x}{x} \biggr| \, dx &= 2\int_0^\infty \frac{|\sin x|}{x} \, dx \\
&= 2\sum_{k=0}^\infty \int_{k\pi}^{(k+1)\pi} \frac{|\sin x|}{x} \, dx \\
&\geq 2\sum_{k=0}^\infty \int_{k\pi}^{(k+1)\pi} \frac{|\sin x|}{(k+1)\pi} \, dx \\
&= \frac{4}{\pi} \sum_{k=0}^\infty \frac{1}{k+1}.
\end{aligned} $$

<p class="footpara">
This concludes that \(\frac{\sin x}{x}\) is not
integrable. Nevertheless, the improper Riemann integral exists and
equals \(\pi\); see, e.g., Schilling's book (2017, <a href="./Schilling-p145.png">p. 145</a>) or
Billingsley's book (2008, <a href="./Billingsley-pp235-236.png">pp. 235&#x2013;236</a>).
</p>

<p class="footpara">
Let \(S(T) = \int_0^T \frac{\sin x}{x} \, dx\). Then \(S(T) \to
\pi/2\). Moreover, there exists a constant \(M\) such that \(|S(T)| \leq
M\). Indeed, as \(S(T) \to \pi/2\), there exists \(T_0 > 0\) such that \(|S(T)|
\leq \pi\) for all \(T \geq T_0\). For \(T < T_0\), there is \(|S(T)| \leq
\int_0^{T_0} |\sin x| / |x| \, dx \leq T_0\). Hence, \(|S(T)| \leq \max(T_0,
\pi)\).
</p>

<p class="footpara">
Now we can discuss the boundness and convergence result of \(R(x;
T)\). Consider \(f(\xi, t) = e^{it\xi} \mathbb{1}(x - b \leq \xi \leq x - a)\). Clearly, \(f(\xi,
t)\) is integrable on the product space \(\mathbb{R} \times [-T, T]\). Hence, we can
apply Fubini's theorem to interchange the order of integrals.
</p>

$$ \begin{aligned}
R(x; T)
&:= \frac{1}{2\pi} \int_{-T}^T \frac{e^{it(x-a)} - e^{it(x-b)}}{it} \, dt \\
&= \frac{1}{2\pi} \int_{-T}^T dt \int d\xi \, e^{it\xi} \mathbb{1}(x - b \leq \xi \leq x - a) \\
&= \frac{1}{2\pi} \int d\xi \int_{-T}^T dt \,  e^{it\xi} \mathbb{1}(x - b \leq \xi \leq x - a) \\
&= \frac{1}{\pi} \int_{x-b}^{x-a} \frac{\sin (T\xi)}{\xi} \, d\xi \\
&= \frac{1}{\pi}[\operatorname{sgn} (x-a) S(T|x-a|)
                - \operatorname{sgn} (x-b) S(T|x-b|)].
\end{aligned} $$

<p class="footpara">
Here we use the fact that for any real number \(c\), the integral \(\int_0^c
\frac{\sin Tx}{x} \, dx = \operatorname{sgn}(c) S(T|c|)\). As \(|S(T)| \leq
M\) for some constant \(M\), we conclude that \(|R(x; T)| \leq
2M/\pi\). Moreover, as \(T \to \infty\), \[ R(x; T) \to \begin{cases} 1, &\quad a <
x < b, \\ 1/2, &\quad x = a \text{ or } x = b, \\ 0, &\quad x < a
\text{ or } x > b. \end{cases} \]
</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4" role="doc-backlink">4</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
In general, by integration by parts
</p>

$$ \begin{aligned}
\int_0^x \frac{(x - t)^n}{n!} f^{(n+1)}(t) \, dt
&= \int_0^x \frac{(x - t)^n}{n!} \, df^{(n)}(t)   \\
&= -\frac{x^n}{n!}f^{(n)}(0)
     + \int_0^x \frac{(x-t)^{n-1}}{(n-1)!} f^{(n)}(t) \, dt \\
&= \cdots \\
&= -\frac{x^n}{n!}f^{(n)}(0) - \cdots - \frac{x^2}{2}f''(0) - xf'(0)
     + \int_0^x f'(t) \, dt \\
&= f(x) - f(0) - \sum_{k=1}^n \frac{f^{(k)}(0)}{k!} x^k.
\end{aligned} $$</div></div>

<div class="footdef"><sup><a id="fn.5" class="footnum" href="#fnr.5" role="doc-backlink">5</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Assume \(\xi < 0\). Then,
</p>

$$ \begin{aligned}
\biggl| e^{i\xi} - 1 - \sum_{k=1}^n \frac{i^k}{k!} \xi^k \biggr|
&= \biggl| \int_0^\xi \frac{(\xi -t)^n}{n!} i^{n+1} e^{it} \, dt \biggr| \\
&=  \biggl| \int_\xi^0 \frac{(\xi -t)^n}{n!} i^{n+1} e^{it} \, dt \biggr| \\
&\leq \int_\xi^0 \frac{|\xi -t|^n}{n!} \, dt \\
&= \int_\xi^0 \frac{(t-\xi)^n}{n!} \, dt \\
&= \frac{(-\xi)^{n+1}}{(n+1)!}.
\end{aligned} $$</div></div>

<div class="footdef"><sup><a id="fn.6" class="footnum" href="#fnr.6" role="doc-backlink">6</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
See, e.g., Billingsley's book (2008, <a href="./Billingsley-pp344-345.png">pp. 344&#x2013;345</a>).
</p></div></div>

<div class="footdef"><sup><a id="fn.7" class="footnum" href="#fnr.7" role="doc-backlink">7</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Taking log on the limit yields \[ \lim_{n\to\infty} \frac{\log(1 +
c/n + o(1/n))}{1/n} = \lim_{n\to\infty} \frac{\log(1 + c/n + o(1/n))}{c/n +
o(1/n)} \cdot \frac{c/n + o(1/n)}{1/n} = c. \]
</p></div></div>


</div>
</div><div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-math.html">math</a> </div>
]]></description>
  <category><![CDATA[math]]></category>
  <link>https://dou-meishi.github.io/org-blog/2024-03-22-CentralLimitTheorem/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2024-03-22-CentralLimitTheorem/notes.html</guid>
  <pubDate>Fri, 22 Mar 2024 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Strong Law of Large Numbers]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org66f764d">Tossing a Coin: Probability as Frequency</a></li>
<li><a href="#org28dad42">Application: Monte Carlo Integration</a></li>
<li><a href="#orgbe52e24">Application: Bernstein's Theorem</a></li>
<li><a href="#org5e7e83b">Other Types of SLLN</a></li>
<li><a href="#org01880ed">Proof Sketch of SLLN (i.i.d. case)</a></li>
</ul>
</div>
</nav>
<p>
We all know that probability can be interpreted as <i>frequency</i>, but
behind it there is an important theorem in probability and statistic
theory, called Strong Law of Large Numbers (SLLN). It states that the
emprical mean, i.e., the mean of samples, will converge to the
expectation of the distribution <i>almost surely</i>. Monte Carlo integration
is actually a direct application of SLLN.
</p>

<p>
In this post, we first review the tossing coin example and prove this
convergence in a probability manner rigorously. Then we review the
SLLN theorem (i.i.d. case) and two applications, Monte Carlo
integration and Bernstein's theorem. The former is an important
numerical method for estimating integrals, and the latter specifies
explicitly an approximating sequence to uniformly approximate any
continuous function on a bounded interval with polynomials.  Finally,
we discuss additional theoretical results relevant to the SLLN, along
with a brief outline of the proof for the SLLN theorem (i.i.d. case).
</p>

<p>
Throughout this post, for a given sequence \((a_n)\) we denote the
cumulative average by \(\bar{a}_n := \frac{1}{n} \sum_{k=1}^n a_k\).
</p>
<div id="outline-container-org66f764d" class="outline-2">
<h2 id="org66f764d">Tossing a Coin: Probability as Frequency</h2>
<div class="outline-text-2" id="text-org66f764d">
<p>
Perhaps all of us start learning about probability by this example: if
we toss a fair coin repeatly then the frequency of heads will tend to
1/2, which is the probability of a head occurring. But can we
characterize this "convergence" behavior?
</p>

<p>
Let the sample space \(\Omega\) be \(\{0, 1\}^\mathbb{N}\), the set of infinite series
with value 0 or 1. Let \(X_n(\omega)\) be the \(n\)-th value of the series
corresponding to \(\omega \in \Omega\). Then clearly \(X_n\) are i.i.d. r.v.s. with mean
\(\mathbb{E}[X_n]=1/2\). By the Strong Law of Large Numbers (SLLN), the empirical
mean \(\bar{X}_n = \frac{1}{n} \sum_{k=1}^n X_k\) (i.e., the frequency of
heads) converges to the expectation <i>almost surely</i>.
</p>

<p>
For the example of tossing a coin, the conclusion can be proved
easily, as the \(k\)-th moment always exists, \(\mathbb{E}[X_n^k] = \mathbb{P}(X_n = 1) =
1/2\). To prove the almost surely convergence, we can estimate the
probability of the event that <i>the difference between the emprical mean
\(\bar{X}_n\) and the expectation 1/2 is larger than any positive</i> \(\epsilon\) by
Markov inequality<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup> \[ \mathbb{P}(|\bar{X}_n - 1/2| > \epsilon) \leq \frac{1}{\epsilon^4}
\mathbb{E}[|\bar{X}_n - 1/2|^4] \leq \frac{3}{(2\epsilon)^4 n^2}. \] By Borel&#x2013;Cantelli
lemma, \(\mathbb{P}(N_\epsilon) = 0\) for any positive \(\epsilon\), where \(N_\epsilon = \{|\bar{X}_n -
1/2| > \epsilon \quad \text{i.o.}\}\). Thus, for any \(\omega \in N^c_\epsilon\),
\(|\bar{X}_n - 1/2| \leq \epsilon\) holds for all but finite many \(n\)'s. Taking
\(N = \bigcup_{\epsilon\in\mathbb{Q}_+} N_\epsilon\) concludes that \(|\bar{X}_n - 1/2| \to 0\) almost
surely.
</p>

<p>
<i>Remark.</i> The sample space \(\Omega\) can be regarded as the interval \((0, 1]\)
like how a real number is represented in base 2. The probability
corresponds to the Lebesgue measure confined on the unit interval.  In
this point of view, the exception set \(N\) is uncountable<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup> but has
measure 0. A number in \(N^c\) is called normal number and the SLLN in
this case is equivalent to \(\mathbb{P}(N) = 0\), which is exactly the Borel's
normal number theorem.
</p>

<p>
The essential condition in this simple proof is the finiteness of the
 4-th moment. However, with advanced techniques, the existence of the
 expectation (possibly infinite) is enough to prove the almost surely
 convergence.
</p>

<p>
<i>Theorem [Strong Law of Large Numbers (i.i.d. case)].</i> Let
\((X_n)_{n=1}^\infty\) be independent and identically distributed and
\(\mathbb{E}[X_1]\) exists (possibly infinite), then \(\bar{X}_n\) converges to
\(\mathbb{E}[X_1]\) almost surely.
</p>
</div>
</div>
<div id="outline-container-org28dad42" class="outline-2">
<h2 id="org28dad42">Application: Monte Carlo Integration</h2>
<div class="outline-text-2" id="text-org28dad42">
<p>
Perhaps Monte Carlo Integration is one of the most promising
application of SLLN. Assume we want to estimate the integral \(\int_A f(x)
\, dx\). Suppose we are able to sample from a reference distribution
\(p\) whose support \(\mathcal{X} \supset A\). Hence, we can rewrite the integral as an
expectation \[ \int_A f(x) \, dx = \int_\mathcal{X} f(x) \frac{\mathbb{1}_A(x)}{p(x)} \, p(x)
dx = \mathbb{E}_{X \sim p}\biggl[f(X) \frac{\mathbb{1}_A(X)}{p(X)} \biggr].\] Then by
sampling from \(p\) to obtain a sequence of i.i.d. observations \((X_n)\),
we can generate a new sequence of i.i.d. observations \((Y_n)\) where
\(Y_n = f(X_n) \mathbb{1}_A(X_n) / p(X_n)\). By SLLN, the empirical mean
\(\bar{Y}_n\) converges almost surely to its expectation, which is
exactly the integral \(\int_A f(x) \, dx\).
</p>

<p>
<i>Example.</i> Compute the integral \(\int_{-\infty}^\infty \frac{\sin^2 x}{x^2} \, dx\).
</p>

<p>
Let the reference distribution \(p\) be a normal distribution. Generate
a sequence of Gaussian noise \((x_n)\). Compute \(y_n = f(x_n) /
p(x_n)\). Then the accumulative average of \((y_n)\) converges to the
integral by SLLN.
</p>

<p>
A simple python code (see <a href="./sinc-square-mc.py">here</a>) can help us visualize the above
calculation. Here is the figure of the convergence of empirical
mean. The horizontal line is the true value of the integral, i.e.,
\(\pi\).
</p>


<figure id="orge73ef67">
<img src="./sinc-square-integral.png" alt="sinc-square-integral.png">

</figure>
</div>
</div>
<div id="outline-container-orgbe52e24" class="outline-2">
<h2 id="orgbe52e24">Application: Bernstein's Theorem</h2>
<div class="outline-text-2" id="text-orgbe52e24">
<p>
According to the famous Weierstrass approximation theorem, any
continuous function \(f\) on the compact set \([0, 1]\) can be uniformly
approximated by polynomials. Interestingly, we can explicitly
construct the approximating sequence with the help of SLLN.
</p>

<p>
For any \(x \in [0, 1]\), let \(p(\cdot; x)\) be Bernoulli distribution with
parameter \(x\). Let \((X_n)\) be a i.i.d. sequence sampled from \(p\). Then
the empirical mean \(\bar{X}_n\) converges to \(x\) almost surely
according to SLLN. By continuity, \(f(\bar{X}_n) \to f(x)\) almost surely
too. Noting that \(f\) is bounded on \([0, 1]\), we conclude that
\(\mathbb{E}[f(\bar{X}_n)] \to f(x)\) by dominated convergence
theorem. Surprisingly, this expectation can be expressed by a
polynomial evaluated at \(x\): \[ \mathbb{E}[f(\bar{X}_n)] = \sum_{k=0}^n
f\biggl(\frac{k}{n}\biggr) \mathbb{P}\biggl(\sum_{i=1}^n X_i = k\biggr) =
\sum_{k=0}^n f\biggl(\frac{k}{n}\biggr) {n \choose k} x^k (1-x)^{n-k} =:
B_n(x; f). \] The polynomial \(B_n(x; f)\) is called the <i>Bernstein
polynomial of degree \(n\) associated with \(f\)</i>.
</p>

<p>
Although the above argument only shows the pointwise convergence, the
following Bernstein's theorem ensures that this convergence is
actually uniform on \([0, 1]\)<sup><a id="fnr.3" class="footref" href="#fn.3" role="doc-backlink">3</a></sup>.
</p>

<p>
<i>Theorem [Bernstein].</i> If \(f\) is continuous, then \(B_n(x; f)\) converges
to \(f\) uniformly on \([0, 1]\).
</p>

<p>
<i>Proof.</i> See <a href="./proof-Bernstein-theorem.png">here</a> (Billingsley, 2008, p. 87).
</p>
</div>
</div>
<div id="outline-container-org5e7e83b" class="outline-2">
<h2 id="org5e7e83b">Other Types of SLLN</h2>
<div class="outline-text-2" id="text-org5e7e83b">
<p>
SLLN states that the existence of the expectation ensures the
convergence of the empirical mean. Interestingly, the converse is also
true if the limit of the empirical mean is finite.
</p>

<p>
<i>Proposition.</i> Let \((X_n)_{n=1}^\infty\) be independent and identically
distributed. If \(\bar{X}_n\) converges almost surely to \(\mu\), which is
finite, then \(\mathbb{E}[|X_1|] < \infty\) and \(\mathbb{E}[X_1] = \mu\).
</p>

<p>
<i>Proof.</i> See Schiling's book (2017, p. 297). See also <a href="https://math.stackexchange.com/questions/1961003/if-x-n-is-i-i-d-and-frac1n-sum-limits-k-1n-x-k-to-y-almost-surel">this discussion</a>.
</p>

<p>
A limitation of SLLN is that it requires the existence of the
expectation, which may not be guaranteed when both expectations of the
positive part and the negative part are infinite. Nevertheless, it can
be proved that in this case the empirical mean may diverge to infinite
too.
</p>

<p>
<i>Proposition [SLLN when mean does not exists ].</i> Let \((X_n)_{n=1}^\infty\) be
independent and identically distributed and \(\mathbb{E}[|X_1|] = \infty\), then
\(\limsup |\bar{X}_n| = \infty\) almost surely.
</p>

<p>
<i>Proof.</i> This is an exercise E4.6 <i>Converse to SLLN</i> in Williams's book
(1991, p. 227). See also <a href="https://math.stackexchange.com/questions/1814813/strong-law-of-large-numbers-converse">this discussion</a> and <a href="https://math.stackexchange.com/questions/4627179/proof-verification-converse-to-strong-law-of-large-numbers">this discussion</a>.
</p>

<p>
<i>Theorem [Strong Law of Large Numbers (independent case)].</i> Let
\((X_n)_{n=1}^\infty\) be independent and \(\sum \frac{\text{Var}[X_n]}{n^2} <
\infty\), then \(\bar{X}_n - \mathbb{E}[\bar{X}_n] \to 0\) almost surely.
</p>

<p>
<i>Proof.</i> See √áinlar's book (2011, p. 127). See also <a href="https://www.math.hkust.edu.hk/~makchen/MATH5411/Chap1Sec7.pdf">this lecture note</a>.
</p>
</div>
</div>
<div id="outline-container-org01880ed" class="outline-2">
<h2 id="org01880ed">Proof Sketch of SLLN (i.i.d. case)</h2>
<div class="outline-text-2" id="text-org01880ed">
<p>
The following arguments are a rephrased version from Billingsley's
book (2008, pp. 282&#x2013;284).
</p>

<p>
Assume \((X_n)\) are nonnegative and \(\mathbb{E}[X_1] < \infty\) (later we can relax
these assumptions).
</p>

<p>
<i>Step I.</i> Let \(Y_n = X_n \mathbb{1}(X_n \leq n)\). Show it holds
almost surely that \[ \bar{Y}_n - \bar{X}_n \to 0 \quad \text{and} \quad
\mathbb{E}[\bar{Y}_n] - \mathbb{E}[X_1] \to 0. \]
</p>

<p>
<i>Step II.</i> Prove \(\bar{Y}_n \to \mathbb{E}[X_1]\) almost surely. (This step is the
most difficult step.)
</p>

<p>
<i>Step III.</i> Conclude that \(\bar{X}_n \to \mathbb{E}[X_1]\) almost surely if \((X_n)\)
are nonnegative and \(\mathbb{E}[X_1] < \infty\).
</p>

<p>
<i>Step IV.</i> Prove that \(\bar{X}_n \to \mathbb{E}[X_1]\) almost surely if \(\mathbb{E}[X_1]
< \infty\) (i.e., removing the nonnegative condition).
</p>

<p>
<i>Step V.</i> Prove that \(\bar{X}_n \to \mathbb{E}[X_1]\) almost surely if \(\mathbb{E}[X_1]
= \infty\) or \(\mathbb{E}[X_1] = -\infty\).
</p>

<p>
In <i>Step II,</i> the following technical lemma is useful: <i>let \((a_n)\) be a
  positive sequence and \((\bar{a}_n)\) be its accumulative average. If
  a subsequence \((\bar{a}_{n_k})\) converges to \(a\) and \(\lim n_{k+1} /
  n_k = r\), then<sup><a id="fnr.4" class="footref" href="#fn.4" role="doc-backlink">4</a></sup></i> \[ \frac{1}{r}a \leq \liminf \bar{a}_n \leq \limsup
  \bar{a}_n \leq r a.\]
</p>

<p>
For the complete proof of <i>Step I-IV</i>, please see <a href="./proof-step-I-II-III-IV.png">here</a>.  For the
complete proof of <i>Step V</i>, please see <a href="./proof-step-V.png">here</a>.
</p>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Actually, we can show that \(\mathbb{E}[|\bar{X}_n - 1/2|^4] =
\frac{3}{16n^2} - \frac{1}{8n^3}\). Let \(Y_n = 2X_n - 1\). Then \((Y_n)\)
are i.i.d., \(\mathbb{E}[Y_n^{2k+1}] = 0\) and \(\mathbb{E}[Y_n^{2k}] = 1\) for all
nonnegative integers \(k\). Now, \[ \mathbb{E}[|\bar{X}_n - 1/2|^4] = \mathbb{E}\biggl|
\frac{1}{n} \sum_{k=1}^n (X_k - 1/2) \biggr|^4 = \frac{1}{16n^4} \mathbb{E}\biggl|
\sum_{k=1}^n Y_k \biggr|^4. \] In order to compute this expectation, we
expand \(|\sum Y_k|^4\) by <a href="https://en.wikipedia.org/wiki/Multinomial_theorem">multinomial theorem</a> \[ \mathbb{E}\biggl| \sum_{k=1}^n Y_k
\biggr|^4 = \sum_{|\alpha| = 4} {4 \choose \alpha} \mathbb{E}[Y^\alpha],\] where \(\alpha\) is a
multiindex \(\alpha = (\alpha_1, \alpha_2, \ldots, \alpha_n)\) and \(Y^\alpha:= \prod_{k=1}^n
Y_k^{\alpha_k}\).  There are five types of \(\alpha\) satisfying \(|\alpha|=4\), i.e., \(\sum
\alpha_k = 4\):
</p>
<ul class="org-ul">
<li>i. \(\alpha\) can be sorted into \((1, 1, 1, 1, 0, \ldots, 0)\)</li>
<li>ii. \(\alpha\) can be sorted into \((2, 1, 1, 0, \ldots, 0)\)</li>
<li>iii. \(\alpha\) can be sorted into \((2, 2, 0, \ldots, 0)\)</li>
<li>iv. \(\alpha\) can be sorted into \((3, 1, 0, \ldots, 0)\)</li>
<li>v. \(\alpha\) can be sorted into \((4, 0, \ldots, 0)\)</li>
</ul>
<p class="footpara">
Clearly, \(\mathbb{E}[Y^\alpha] \neq 0\) only for type iii and type v. In both case,
\(\mathbb{E}[Y^\alpha] = 1\). Type iii contains \({n \choose 2}\) indices and type v
contains \(n\) indices. Hence, \[ \mathbb{E}\biggl| \sum_{k=1}^n Y_k \biggr|^4 = {n
\choose 2}{4 \choose {2, 2}} + n = 3n^2 - 2n. \] We can easily verify
that this result is also true for \(n \leq 3\).
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Indeed, for any \(x \in (0, 1]\), let \(\omega_x = (d_1, d_2, \ldots)\) be
the dyadic expansion of \(x\), i.e., \(x = \sum \frac{d_k}{2^k}\). Let \(\omega' =
(1, 1, d_1, 1, 1, d_2, \ldots)\) be defined by \(\omega'_i = d_i\) if \(i \mod
3 = 0\) and \(\omega'_i = 1\) otherwise. Then clearly \(\bar{X}_n(\omega') \geq 2/3\)
for all \(n\) and thus \(\omega' \in N\). This shows that there is an injection
map from \((0, 1]\) to \(N\).
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3" role="doc-backlink">3</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
The proof is based on Chebyshev's inequality. Let \(M= \sup_{x \in
[0, 1]}|f(x)|\). For any \(\epsilon > 0\), let \(\delta(\epsilon) = \sup_{|x - y| < \epsilon, x, y \in
[0, 1]} |f(x) - f(y)|\). Noting that \(B_n(x; f) = \mathbb{E}[f(\bar{X}_n)]\),
</p>
$$ \begin{aligned}
|B_n(x; f) - f(x) |
& = |\mathbb{E}[f(\bar{X}_n)] - f(x)| \\
& \leq \mathbb{E}|f(\bar{X}_n) - f(x)| \\
& \leq \delta(\epsilon) \mathbb{P}(|\bar{X}_n - x| \leq \epsilon) + 2M \mathbb{P}(|\bar{X}_n - x| > \epsilon) \\
& \leq \delta(\epsilon) + 2M \frac{p(1-p)}{n\epsilon^2}.
\end{aligned} $$
<p class="footpara">
By choosing \(\epsilon = (1/n)^{1/4}\), the uniform norm \(\|B_n - f\|\)
converges to 0 (noting that \(\delta(\epsilon) \to 0\) as \(f\) is uniformly
continuous).
</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4" role="doc-backlink">4</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
For \(n_k \leq n < n_{k+1}\) (noting \(a_n \geq 0\)), there is \[
\frac{n_k}{n_{k+1}} \bar{a}_{n_k} \leq a_n \leq \frac{n_{k+1}}{n_{k}}
\bar{a}_{n_{k+1}}. \]
</p></div></div>


</div>
</div><div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-math.html">math</a> </div>
]]></description>
  <category><![CDATA[math]]></category>
  <link>https://dou-meishi.github.io/org-blog/2024-03-17-LawOfLargeNumbers/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2024-03-17-LawOfLargeNumbers/notes.html</guid>
  <pubDate>Sun, 17 Mar 2024 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Academic Citations and APA Style]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org1f43809">In-text Citations and The Reference List</a></li>
<li><a href="#orgd657323">Formatting Rules for In-text Citations</a></li>
<li><a href="#org29bf979">Formatting Rules for Reference List Entries</a>
<ul>
<li><a href="#org64250b4">The Author Element</a></li>
<li><a href="#orgc201c72">The Date Element</a></li>
<li><a href="#org3a2ebc9">The Title Element</a></li>
<li><a href="#orgc8b7df4">The Source Element</a></li>
<li><a href="#orgf2462c3">Examples</a></li>
</ul>
</li>
<li><a href="#org09eb55e">The Official APA Website</a></li>
<li><a href="#orgc842428">A File Naming Convention for References</a></li>
<li><a href="#org6612221">References&#xa0;&#xa0;&#xa0;<span class="tag"><span class="refs">refs</span></span></a></li>
</ul>
</div>
</nav>
<p>
Citations are important in academic writings, not only for
acknowledging the original authors and avoiding plagiarism, but also
for <i>effective communication</i>. Citations help readers track the
evolution of thought and provide the source where they can retrieve
the original material. Citations can also add strength and authority
to your work. In this post, I will show how to properly cite sources
in academic writings. In particular, I will follow the APA (American
Psychological Association) citation style.
</p>

<p>
You may ask why bother to understand these conceps? There are a plenty of
reference manager software available to help generate nicely formatted
citations in various styles, e.g., Zotero (I use it too), EndNote,
Mendeley, and even google scholar. However, they are only tools and do
not tell us what kind of citation is needed in different
situations. Moreover, it is beneficial to have a brief understanding
of how the citation is formatted so that we can format simple
citations by ourself (it is achievable in APA style), and even
implement a function that suits our need using a programming language.
</p>

<p>
I will begin by discussing the concepts of in-text citation and the
reference list, followed by demonstrations through
examples. Naturally, it is impossible to cover all formatting rules in
a brief post. Therefore, I will direct you to <a href="https://apastyle.apa.org/">the APA style official
website</a> and illustrate how to locate resources there. Finally, I
will introduce my file naming convention for references by following
APA style.
</p>
<div id="outline-container-org1f43809" class="outline-2">
<h2 id="org1f43809">In-text Citations and The Reference List</h2>
<div class="outline-text-2" id="text-org1f43809">
<p>
In APA style, the reference metadata are typically grouped into four
elements:
</p>

<ul class="org-ul">
<li><i>author</i>, i.e., their names;</li>
<li><i>date</i>, in most cases., the publication year;</li>
<li><i>title</i>, i.e., the title of the cited work;</li>
<li>and <i>source</i>, which specifies where to retrieve the cited work.</li>
</ul>

<p>
<i>In-text citation</i> appears within the body of the paper and contain
only <i>author</i> and <i>date</i> of publication, directing readers to locate the
corresponding entry in the <i>reference list</i> at the end of the paper,
which contains additional information include <i>title</i> and <i>source</i>.
</p>
</div>
</div>
<div id="outline-container-orgd657323" class="outline-2">
<h2 id="orgd657323">Formatting Rules for In-text Citations</h2>
<div class="outline-text-2" id="text-orgd657323">
<p>
There are two type of in-text citations:
</p>

<ul class="org-ul">
<li><p>
<i>parenthetical citations</i>, where the author's surname and year appear
in parentheses.  This often appears at the end of a sentence, e.g.,
</p>

<pre class="example" id="org7cc30bc">
Falsely balanced news coverage can distort the public‚Äôs perception
of expert consensus on an issue (Koehler, 2016).

It was found that ... (Anderson &amp; Bratos-Anderson, 1987).

It was found that ... (Reynar et al., 2010).
</pre></li>

<li><p>
<i>narrative citations</i>, where the author's surname appears as part of
the sentence, followed by the year in parentheses. In this case, <code>&amp;</code>
is replaced by the word <code>and</code>. For three or more authors, use <code>et. al</code>.,
This often appears at the beginning of a sentence, e.g.,
</p>

<pre class="example" id="orgdb9ff0c">
Koehler (2016) noted the dangers of falsely balanced news coverage.

Anderson and Bratos-Anderson (1987) found that ...

Reynar et al. (2010) found that ...
</pre></li>
</ul>
</div>
</div>
<div id="outline-container-org29bf979" class="outline-2">
<h2 id="org29bf979">Formatting Rules for Reference List Entries</h2>
<div class="outline-text-2" id="text-org29bf979">
<p>
While in-text citation doesn‚Äôt vary depending on source type,
reference list citations are highly variable depending on the source.
As mentioned before, a reference entry includes the four elements of
the author, date, title, and source; see <a href="https://apastyle.apa.org/style-grammar-guidelines/references/elements-list-entry">Elements of Reference List
Entries</a> for more details.
</p>
</div>
<div id="outline-container-org64250b4" class="outline-3">
<h3 id="org64250b4">The Author Element</h3>
<div class="outline-text-3" id="text-org64250b4">
<p>
<i>Author</i> refers broadly to the person(s) or group(s) responsible for a
work. An author may be an individual, multiple people, or a group
(institution, government agency, organization, etc.). When formatting,
provide the surname first, followed by a comma and the author‚Äôs
initials. Use a comma to separate different authors. Use <code>&amp;</code> before the
final author‚Äôs name. When there are too many, say 21 or more authors,
include the first 19 authors‚Äô names, insert an ellipsis (but no
ampersand), and then add the final author‚Äôs name.
</p>

<pre class="example" id="orgb6b48a8">
Author, A. A.

Author, A. A., &amp; Author, B. B.

Author, A. A., Author, B. B., &amp; Author, C. C.

Author, A. A., Author, B. B., Author, C. C., Author, D. D., Author, E. E.,
        Author, F. F., Author, G. G., Author, H. H., Author, I. I.,
        Author, J. J., Author, K. K., Author, L. L., Author, M. M.,
        Author, N. N., Author, O. O., Author, P. P., Author, Q. Q.,
        Author, R. R., Author, S. S., . . . Author, Z. Z.
</pre>
</div>
</div>
<div id="outline-container-orgc201c72" class="outline-3">
<h3 id="orgc201c72">The Date Element</h3>
<div class="outline-text-3" id="text-orgc201c72">
<p>
<i>Date</i> refers to the date of publication of the work. It can be year
only or an exact date. When formatting, enclose the date in
parentheses, followed by a period.
</p>

<pre class="example" id="org2b1edaa">
(2020).

(2018, July).

(2020, August 26).
</pre>
</div>
</div>
<div id="outline-container-org3a2ebc9" class="outline-3">
<h3 id="org3a2ebc9">The Title Element</h3>
<div class="outline-text-3" id="text-org3a2ebc9">
<p>
<i>Title</i> refers to the title of the work being cited. When formatting,
capitalize the title using sentence case and finish the title with a
period<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>. Italicize the title for stand alone works (e.g., books
and webpages). But do not italicize for works that a part of a great
whole (e.g., journal articles and conference papers).
</p>

<pre class="example" id="org64cc19d">
Happy fish in little ponds: Testing a reference group model of achievement and emotion.
</pre>
</div>
</div>
<div id="outline-container-orgc8b7df4" class="outline-3">
<h3 id="orgc8b7df4">The Source Element</h3>
<div class="outline-text-3" id="text-orgc8b7df4">
<p>
<i>Source</i> indicates where readers can retrieve the cited work. As with
titles, sources fall into two broad categories: works that are part of
a greater whole and works that stand alone:
</p>

<ul class="org-ul">
<li>for books, the source is the publisher;</li>
<li>for webpages, the source is the website;</li>
<li>for journal articles, the source is the journal;</li>
<li>for conference papers, the source is the conference.</li>
</ul>

<p>
The format of the source varies depending on the reference type.
</p>

<table>


<colgroup>
<col  class="org-left">

<col  class="org-left">

<col  class="org-left">
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Reference type</th>
<th scope="col" class="org-left">Component of the source</th>
<th scope="col" class="org-left">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Journal article</td>
<td class="org-left">Italic journal title, volume(issue), pages.</td>
<td class="org-left"><i>Couple and Family Psychology: Research and Practice</i>, 8(3), 137‚Äì151.</td>
</tr>

<tr>
<td class="org-left">Book</td>
<td class="org-left">Publisher name</td>
<td class="org-left">Springer.</td>
</tr>

<tr>
<td class="org-left">Webpage on a website</td>
<td class="org-left">Website name</td>
<td class="org-left">Mayo Clinic.</td>
</tr>

<tr>
<td class="org-left">webpage on a website (when authors are the same as the site name)</td>
<td class="org-left">URL</td>
<td class="org-left"><a href="https://www.cdc.gov/cancer/kinds.htm">https://www.cdc.gov/cancer/kinds.htm</a></td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-orgf2462c3" class="outline-3">
<h3 id="orgf2462c3">Examples</h3>
<div class="outline-text-3" id="text-orgf2462c3">
<p>
A conference paper:
</p>

<p>
Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., &amp;
Poole, B. (2021). Score-Based Generative Modeling through Stochastic
Differential Equations. <i>International Conference on Learning
Representations</i>.
</p>

<p>
A book:
</p>

<p>
Bertsekas, D., Nedic, A., &amp; Ozdaglar, A. (2003). <i>Convex Analysis and Optimization</i>. Athena Scientific.
</p>

<p>
A webpage:
</p>

<p>
Dohmatob, E. (2021). <i>Fenchel-Rockafellar duality theorem, one ring to rule‚Äôem all! - Part 1</i>. <a href="https://dohmatob.github.io/research/2019/10/31/duality.html">https://dohmatob.github.io/research/2019/10/31/duality.html</a>
</p>

<p>
For more examples, see the <a href="https://apastyle.apa.org/style-grammar-guidelines/references/examples">Reference Examples</a> page of APA website.
</p>
</div>
</div>
</div>
<div id="outline-container-org09eb55e" class="outline-2">
<h2 id="org09eb55e">The Official APA Website</h2>
<div class="outline-text-2" id="text-org09eb55e">
<p>
<a href="https://apastyle.apa.org/">The APA style official website</a> contains a comprehensive section of
<a href="https://apastyle.apa.org/style-grammar-guidelines">Style and Grammar Guidelines</a>, an illustrative section of <a href="https://apastyle.apa.org/instructional-aids/handouts-guides">Handouts and
Guides</a>, and a blog section of <a href="https://apastyle.apa.org/blog">Posts</a>. I recommend reading the post
<a href="https://apastyle.apa.org/beginners">APA Style for beginners</a>, in particular, the 2 minutes demonstrating
video introducing the APA style website. The <a href="https://extras.apa.org/apastyle/basics-7e/#/">Academic Writer Tutorial</a>
is also highly recommended. The following cheat sheets are also
useful:
</p>

<ol class="org-ol">
<li><a href="https://apastyle.apa.org/instructional-aids/beginner-student-paper-checklist.pdf">Student Paper Checklist</a>,</li>
<li><a href="https://apastyle.apa.org/instructional-aids/reference-examples.pdf">Common Reference Examples Guide</a>.</li>
</ol>
</div>
</div>
<div id="outline-container-orgc842428" class="outline-2">
<h2 id="orgc842428">A File Naming Convention for References</h2>
<div class="outline-text-2" id="text-orgc842428">
<p>
I always try to follow some file naming convention for electronic
documents. After knowing APA style, I realize that it offers a perfect
structure for naming references. In particular, I create two folders,
<code>~/Books/</code> and <code>~/Papers/</code>, to store my references. Each local copy is
named according to a simple version of the format used in the
reference list:
</p>

<ul class="org-ul">
<li>it does not contain the <i>source</i> element;</li>
<li>a maximum of three authors are represented;</li>
<li>the date element is confined to the year only;</li>
<li>the title element may be shortened.</li>
</ul>

<p>
Here are some examples:
</p>

<pre class="example" id="org061d754">
Anderson, B. D. O. (1982). Reverse-time diffusion equation models.pdf

Polyanskiy, Y., &amp; Wu, Y. (2022). Information theory.pdf

Ho, J., Jain, A., &amp; Abbeel, P. (2020). DDPM.pdf

Goodfellow, I., Pouget-Abadie, J., ... Bengio, Y. (2014). GAN.pdf
</pre>
</div>
</div>
<div id="outline-container-org6612221" class="outline-2">
<h2 id="org6612221">References&#xa0;&#xa0;&#xa0;<span class="tag"><span class="refs">refs</span></span></h2>
<div class="outline-text-2" id="text-org6612221">
<ul class="org-ul">
<li>APA Org (2020). <i>Academic Writer Tutorial: Basics of Seventh Edition APA Style</i>. <a href="https://extras.apa.org/apastyle/basics-7e/#/">https://extras.apa.org/apastyle/basics-7e/#/</a></li>
<li>APA Org (2024). <i>Style and Grammar Guidelines</i>. <a href="https://apastyle.apa.org/style-grammar-guidelines">https://apastyle.apa.org/style-grammar-guidelines</a></li>
<li>Mendeley (2024). <i>APA Format Citation Guide</i>. <a href="https://www.mendeley.com/guides/apa-citation-guide/">https://www.mendeley.com/guides/apa-citation-guide/</a></li>
</ul>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
However, if the title ends with a question mark or exclamation
point, that punctuation mark replaces the period.
</p></div></div>


</div>
</div><div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-tool.html">tool</a> </div>
]]></description>
  <category><![CDATA[tool]]></category>
  <link>https://dou-meishi.github.io/org-blog/2024-03-12-AcademicCitationsandAPA/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2024-03-12-AcademicCitationsandAPA/notes.html</guid>
  <pubDate>Tue, 12 Mar 2024 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Seamless Writing and Rendering with Emacs Org]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgdae0a6f">A new minor mode</a></li>
<li><a href="#orgbf2cda5">A simple local server with autoreload</a></li>
<li><a href="#org291efa7">Replace MathJax with KaTeX</a></li>
</ul>
</div>
</nav>
<p>
One nice feature of <code>.org</code> files is their easy export to HTML for
viewing on any browser. Recently, I realized that it is even possible
to seamlessly integrate writing and exporting processes by telling
Emacs to export the <code>.org</code> file every time I save it. Then by launching
a local server with autoreload, I can immediately preview the HTML
version of the <code>.org</code> file. This is particular useful in writing LaTeX
equations, where it is very hard to imagine what you are writing
without rendering.
</p>

<p>
The basic idea is creating a customized minor mode in which the
exporting function is added to the <code>after-save-hook</code>. Combining with the
<code>prettify-symbols-mode</code> (discussed in <a href="../2024-02-24-EmacsPrettifySymbols/notes.html">the previous post</a>), writing
formulae in Emacs turns out to be enjoyable for me.
</p>

<div style="position: relative; padding-bottom: 62.5%; height: 0;"><iframe src="https://www.loom.com/embed/092c404ffda44c3b9f1855c394bf7a64?sid=a77e1bfb-eb29-4d7b-9389-4a83522b7c79" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe></div>

<p>
Above is the final effect (the vedio was recorded and inserted via
<a href="https://www.loom.com/">Loom</a>). It involves several simple components:
</p>

<ol class="org-ol">
<li>A minor mode called <code>dms/blog-mode</code> in which the file gets exported
to HTML after saving;</li>
<li>A local server which can reload the html page when it changes;</li>
<li><i>(Optional)</i> The KaTeX framework for math rendering to replace
MathJax.</li>
</ol>
<div id="outline-container-orgdae0a6f" class="outline-2">
<h2 id="orgdae0a6f">A new minor mode</h2>
<div class="outline-text-2" id="text-orgdae0a6f">
<p>
In Emacs, creating a minor mode for autosaving is straightforward.
</p>

<div class="org-src-container">
<pre class="src src-elisp">(<span style="color: #859900; font-weight: bold;">define-minor-mode</span> <span style="color: #268bd2;">dms/blog-mode</span>
  <span style="color: #2aa198;">"Automatically export Org files to HTML after saving.</span>

<span style="color: #2aa198;">This mode also enables the prettify-symbols-mode."</span>
  <span style="color: #657b83; font-weight: bold;">:lighter</span> <span style="color: #2aa198;">" dms/blog"</span>
  (<span style="color: #859900; font-weight: bold;">if</span> dms/blog-mode
      (add-hook 'after-save-hook 'org-html-export-to-html 0 'make-it-local)
    (remove-hook 'after-save-hook 'org-html-export-to-html
                 'make-it-local))
  (prettify-symbols-mode 1)
)
</pre>
</div>

<p>
As any function in <code>after-save-hook</code> will be executed after saving, I
add <code>org-html-export-to-html</code> to it when enering this new minor mode and
remove it when exiting. See <a href="https://www.gnu.org/software/emacs/manual/html_node/emacs/Hooks.html">this manual page</a> for a brief introduction
to hooks in Emacs.  In either case, the <code>prettify-symbols-mode</code> is
enabled.
</p>
</div>
</div>
<div id="outline-container-orgbf2cda5" class="outline-2">
<h2 id="orgbf2cda5">A simple local server with autoreload</h2>
<div class="outline-text-2" id="text-orgbf2cda5">
<p>
As the exported file is only a static HTML page, it will not be
updated by the browser when its content changes. So it is necessary to
find a way to refresh the page after updates. Fortunately, there is a
nice python project <a href="https://github.com/lepture/python-livereload">python-livereload</a> which can start a simple local
http server, watch a file/directory and reload pages on changes.
</p>

<p>
After installation, I can use a simple command to launch a server and
host a static HTML page, e.g., <code>/tmp/temporary-drafts.html</code> on the URL
<code>http://localhost:35729//tmp/temporary-drafts.html</code> (35729 is the
default port used by <code>livereload</code>; see <a href="https://livereload.readthedocs.io/en/latest/cli.html">its doc</a> for more details).
</p>

<div class="org-src-container">
<pre class="src src-sh">livereload -t /tmp/temporary-drafts.html
</pre>
</div>

<p>
We integrate these steps into Emacs through a simple command; see also
<a href="https://www.gnu.org/software/emacs/manual/html_node/elisp/Using-Interactive.html#Using-Interactive">the manual</a> for how to use <code>interactive</code>.
</p>

<div class="org-src-container">
<pre class="src src-elisp">(<span style="color: #859900; font-weight: bold;">defun</span> <span style="color: #268bd2;">dms/livereload</span> (arg <span style="color: #b58900;">&amp;optional</span> port)
  <span style="color: #2aa198;">"Launch a http server to autoreload a HTML file and open Edge.</span>

<span style="color: #2aa198;">The server is provided by a python project called python-livereload,</span>
<span style="color: #2aa198;">which uses 35729 as the default port. "</span>
  (<span style="color: #859900; font-weight: bold;">interactive</span>
   (list
    (read-file-name <span style="color: #2aa198;">"Select a file: "</span>)
    (read-number <span style="color: #2aa198;">"Select the Port: "</span> 35729)))
  (<span style="color: #859900; font-weight: bold;">let</span> ((abspath (expand-file-name arg))
        (port (<span style="color: #859900; font-weight: bold;">or</span> port 35729)))
    (shell-command (format
                    <span style="color: #2aa198;">"nohup livereload -t %s -p %d &gt; /dev/null 2&gt;&amp;1 &amp;"</span> abspath port))
    (shell-command (format
                    <span style="color: #2aa198;">"microsoft-edge http://localhost:%d/%s"</span> port abspath))))
</pre>
</div>
</div>
</div>
<div id="outline-container-org291efa7" class="outline-2">
<h2 id="org291efa7">Replace MathJax with KaTeX</h2>
<div class="outline-text-2" id="text-org291efa7">
<p>
By default, Emacs uses MathJax in the exported HTML file to rende math
equations. But personally I feel KaTeX is smoother and faster. So I
use the following snippet to disable MathJax and insert KaTeX scripts
in the head; see also <a href="https://katex.org/docs/browser">the doc</a> for how to included it in HTML.
</p>

<div class="org-src-container">
<pre class="src src-elisp">(<span style="color: #859900; font-weight: bold;">use-package</span> org
  <span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">replace MathJax with KaTeX</span>
  <span style="color: #657b83; font-weight: bold;">:config</span>
  (<span style="color: #859900; font-weight: bold;">setq</span> org-html-mathjax-template <span style="color: #2aa198;">""</span>)
  (<span style="color: #859900; font-weight: bold;">setq</span> org-html-head (concat org-html-head
          <span style="color: #2aa198;">"&lt;!-- Math Support by KaTeX --&gt;</span>
<span style="color: #2aa198;">&lt;link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css\" integrity=\"sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV\" crossorigin=\"anonymous\"&gt;</span>
<span style="color: #2aa198;">&lt;!-- The loading of KaTeX is deferred to speed up page rendering --&gt;</span>
<span style="color: #2aa198;">&lt;script defer src=\"https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js\" integrity=\"sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8\" crossorigin=\"anonymous\"&gt;&lt;/script&gt;</span>
<span style="color: #2aa198;">&lt;!-- To automatically render math in text elements, include the auto-render extension: --&gt;</span>
<span style="color: #2aa198;">&lt;script defer src=\"https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js\" integrity=\"sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05\" crossorigin=\"anonymous\" onload=\"renderMathInElement(document.body);\"&gt;&lt;/script&gt;"</span>))
)
</pre>
</div>

<p>
If you prefer MathJax, I recommend modifying the default value of
<code>org-html-mathjax-template</code> to enable the lazy typesetting feature; see <a href="https://docs.mathjax.org/en/latest/output/lazy.html">the MathJax doc</a>.
</p>
</div>
</div>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-emacs.html">emacs</a> </div>
]]></description>
  <category><![CDATA[emacs]]></category>
  <link>https://dou-meishi.github.io/org-blog/2024-03-06-SeamlessBlogWriting/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2024-03-06-SeamlessBlogWriting/notes.html</guid>
  <pubDate>Wed, 06 Mar 2024 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Display LaTeX Command with Unicode Characters in Emacs]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orga5b4479">Create the list of pretty symbols</a></li>
<li><a href="#orgd0b0895">Decide whether to compose</a></li>
<li><a href="#orgc5f8da6">Tweak into the mode hook</a></li>
<li><a href="#orgf342cea">Choose appropriate fonts</a></li>
<li><a href="#org39cdeb2">Add more symbols</a></li>
<li><a href="#orge1d668c">Alternative implementation: hard replacing</a></li>
<li><a href="#org8640e44">Useful links&#xa0;&#xa0;&#xa0;<span class="tag"><span class="refs">refs</span></span></a></li>
</ul>
</div>
</nav>
<p>
If the minor mode <code>prettify-symbols-mode</code> is enabled, Emacs will display
certain strings with more attractive versions according to
<code>prettify-symbols-alist</code>. For example, <code>$\mathbb{P}(\Omega) \leq 1$</code> might be
displayed as <code>$‚Ñô(Œ©) ‚â§ 1$</code>. This happends without modifying the content
and could be disabled by turning off the <code>prettify-symbols-mode</code> if
necessary. This feature may be very useful in writing LaTeX formulae.
</p>

<p>
This is the effect after incorporating the settings introduced in this
post.
</p>

<p>
Before prettify
<img src="./before-prettify.png" alt="before-prettify.png">
</p>

<p>
After prettify
<img src="./after-prettify.png" alt="after-prettify.png">
</p>
<div id="outline-container-orga5b4479" class="outline-2">
<h2 id="orga5b4479">Create the list of pretty symbols</h2>
<div class="outline-text-2" id="text-orga5b4479">
<p>
Each element <code>prettify-symbols-alist</code> looks like <code>(SYMBOL . CHARACTER)</code>,
where the symbol matching <code>SYMBOL</code> (a string, not a regexp) will be
shown as <code>CHARACTER</code> instead.
</p>

<p>
I create <a href="https://gist.github.com/Dou-Meishi/7c90c9b24fc7d9f6a7cba27cf27b6992">a CSV file</a> to store the symbols and associate pretty
characters. Below is a lisp function which parse such a CSV file and
return a list suitable for <code>prettify-symbols-alist</code>.
</p>

<div class="org-src-container">
<pre class="src src-elisp">(<span style="color: #859900; font-weight: bold;">defun</span> <span style="color: #268bd2;">dms/load-prettify-symbols</span> (file)
  <span style="color: #2aa198;">"Load a CSV file and return a suitable list for `prettify-symbols-alist`.</span>

<span style="color: #2aa198;">The CSV file should be separated by `, `, where the space after</span>
<span style="color: #2aa198;">comma is mandatory. In each line, the string before the comma</span>
<span style="color: #2aa198;">will be displayed by the pretty symbol after the comma."</span>
  (<span style="color: #859900; font-weight: bold;">with-temp-buffer</span>
    (insert-file-contents file)
    (<span style="color: #859900; font-weight: bold;">setq</span> contents (split-string (buffer-string) <span style="color: #2aa198;">"\n"</span> t))
    (<span style="color: #859900; font-weight: bold;">setq</span> loaded-prettify-symbols-alist '())
    (<span style="color: #859900; font-weight: bold;">dolist</span> (line contents loaded-prettify-symbols-alist)
      (<span style="color: #859900; font-weight: bold;">let*</span> ((pair (split-string line <span style="color: #2aa198;">", "</span> t))
             (original-string (car pair))
             (pretty-symbol (cadr pair))
             <span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">Convert string to a char as `prettify-symbols-alist` uses chars not strings</span>
             (pretty-char-symbol (string-to-char pretty-symbol)))
        (<span style="color: #859900; font-weight: bold;">push</span> (cons original-string pretty-char-symbol) loaded-prettify-symbols-alist)))))
</pre>
</div>

<p>
The CSV file looks like
</p>

<pre class="example" id="org32752dd">
\mathbb{P}, ‚Ñô
\leq, ‚â§
\geq, ‚â•
...
</pre>
</div>
</div>
<div id="outline-container-orgd0b0895" class="outline-2">
<h2 id="orgd0b0895">Decide whether to compose</h2>
<div class="outline-text-2" id="text-orgd0b0895">
<p>
The variable <code>prettify-symbols-compose-predicate</code> is a predicate for
deciding if the currently matched symbol is to be composed.  By
default, not all appearance will be prettified. For example, the
string <code>\mathbb{P},</code> will not be prettified as the <code>\mathbb{P}</code> is
followed by a comma.  However, this situation happens frequently in
writing formulae. So I overwrite the rule to allow these cases.
</p>

<div class="org-src-container">
<pre class="src src-elisp">(<span style="color: #859900; font-weight: bold;">defun</span> <span style="color: #268bd2;">dms/lax-prettify-symbols-compose-p</span> (start end _match)
  <span style="color: #2aa198;">"A more lax compose predicate that allows compositing even when</span>
<span style="color: #2aa198;"> the match is followed by digits, parentheses, punctuation,</span>
<span style="color: #2aa198;"> or whitespace characters."</span>
  (<span style="color: #859900; font-weight: bold;">let</span> ((next-char (char-after end)))
    (<span style="color: #859900; font-weight: bold;">or</span>
     (<span style="color: #859900; font-weight: bold;">and</span> next-char (string-match-p <span style="color: #2aa198;">"[[:digit:]()[:punct:][:space:]]"</span> (char-to-string next-char)))
     (prettify-symbols-default-compose-p start end _match))))
</pre>
</div>
</div>
</div>
<div id="outline-container-orgc5f8da6" class="outline-2">
<h2 id="orgc5f8da6">Tweak into the mode hook</h2>
<div class="outline-text-2" id="text-orgc5f8da6">
<p>
As both <code>prettify-symbols-alist</code> and <code>prettify-symbols-compose-predicate</code>
are buffer-local variables, it is recommended to set them in a mode
hook. Below I set them in the org-mode hook. It can also be set in
latex-mode hook if necessary.
</p>

<div class="org-src-container">
<pre class="src src-elisp">(<span style="color: #859900; font-weight: bold;">defun</span> <span style="color: #268bd2;">dms/tweak-prettify-symbols-mode</span> ()
  <span style="color: #2aa198;">"Set values of prettify-symbols-alist and prettify-symbols-compose-predicate"</span>
  (<span style="color: #859900; font-weight: bold;">setq</span> prettify-symbols-alist
        (dms/load-prettify-symbols <span style="color: #2aa198;">"~/.emacs.d/pretty-symbols.csv"</span>))
  (<span style="color: #859900; font-weight: bold;">setq</span> prettify-symbols-compose-predicate
        'dms/lax-prettify-symbols-compose-p))

(add-hook 'org-mode-hook 'dms/tweak-prettify-symbols-mode)
</pre>
</div>
</div>
</div>
<div id="outline-container-orgf342cea" class="outline-2">
<h2 id="orgf342cea">Choose appropriate fonts</h2>
<div class="outline-text-2" id="text-orgf342cea">
<p>
In most cases, many mathematical symbols are not included in the main
font. Fortunately, Emacs has the abilit to display selected characters
with certain fonts, achieving an effect of combining fonts; see the
fontset concept <a href="https://www.gnu.org/software/emacs/manual/html_node/emacs/Fontsets.html">in the doc</a>.
</p>

<p>
Below I patch the default fontset in order to
</p>

<ol class="org-ol">
<li>display unicode characters within the range <code>U2100</code>  to <code>U214F</code> with font DejaVu Sans;</li>
<li>display unicode characters within the range <code>U1D7D8</code> to <code>U1D7#1</code> with font DejaVu Sans;</li>
<li>display unicode characters within the range <code>U1D538</code> to <code>U1D56B</code> with font DeJaVu Sans;</li>
<li>display unicode characters within the range <code>U1D4D0</code> to <code>U1D4E9</code> with font Libertinus Math</li>
</ol>

<div class="org-src-container">
<pre class="src src-elisp"><span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">refer to https://dejavu.sourceforge.net/samples/DejaVuSans.pdf</span>
<span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">Unicode Letterlike symbols</span>
(set-fontset-font <span style="color: #2aa198;">"fontset-default"</span> '(#x2100 . #x214F) <span style="color: #2aa198;">"DejaVu Sans"</span>)
<span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">Blackboard letters 0 to 9</span>
(set-fontset-font <span style="color: #2aa198;">"fontset-default"</span> '(#x1D7D8 . #x1D7E1) <span style="color: #2aa198;">"DejaVu Sans"</span>)
<span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">Blackboard letters A to Z and a to z</span>
(set-fontset-font <span style="color: #2aa198;">"fontset-default"</span> '(#x1D538 . #x1D56B) <span style="color: #2aa198;">"DejaVu Sans"</span>)

<span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">Bold script letters</span>
(set-fontset-font <span style="color: #2aa198;">"fontset-default"</span> '(#x1D4D0 . #x1D4E9) <span style="color: #2aa198;">"Libertinus Math"</span>)
</pre>
</div>
</div>
</div>
<div id="outline-container-org39cdeb2" class="outline-2">
<h2 id="org39cdeb2">Add more symbols</h2>
<div class="outline-text-2" id="text-org39cdeb2">
<p>
In general, the following things are needed to display unicode characters by composition:
</p>

<ol class="org-ol">
<li>the string to be replaced, like <code>\mathscr{L}</code>;</li>
<li>the symbol to be rendered, like <code>ùìõ</code>;</li>
<li>(<i>optional</i>) an appropriate font which can display the symbol.</li>
</ol>

<p>
There is a convenient way to find the unicode symbol, i.e., the second
thing. In Emacs, there is a builtin shortcut <code>C-x 8</code>, which can insert
the unicode character from its codepoint or its name. For example, the
command <code>C-x 8 RET MATHEMATICAL BOLD SCRIPT CAPITAL L</code> will insert the
script letter ùìõ (actually the bold version here as the normal version
is too thin). In fact, if you type <code>C-x 8 RET MATHEMATICAL TAB</code> then
Emacs will pops up a list of mathematical symbols for selection.
</p>


<figure id="org4732ac3">
<img src="./script-letters.png" alt="script-letters.png">

</figure>
</div>
</div>
<div id="outline-container-orge1d668c" class="outline-2">
<h2 id="orge1d668c">Alternative implementation: hard replacing</h2>
<div class="outline-text-2" id="text-orge1d668c">
<p>
The advantage of <code>prettify-symbols-mode</code> is that it is only a way of
rendering. The file content will not be changed when the minor mode is
toggled. However, the disadvantage is that it works on the whole
buffer and, to the best of my knowledge, cannot be restricted to a
region.
</p>

<p>
If necessary, one can choose another implementation to translate these
LaTeX commands to their unicode counterparts, i.e., simply finding and
replacing. One can implement a function named
<code>toggle-unicode-representation</code>, which can replace commands with unicode
characters in a region, or vice versa.
</p>
</div>
</div>
<div id="outline-container-org8640e44" class="outline-2">
<h2 id="org8640e44">Useful links&#xa0;&#xa0;&#xa0;<span class="tag"><span class="refs">refs</span></span></h2>
<div class="outline-text-2" id="text-org8640e44">
<ol class="org-ol">
<li><a href="https://tony-zorman.com/posts/pretty-latex.html">Prettifying LaTeX Buffers</a></li>
<li><a href="https://occasionallycogent.com/emacs_prettify_comments/index.html">Emacs prettify-symbols-mode in Comments</a></li>
<li><a href="https://www.emacswiki.org/emacs/PrettySymbol">Pretty Symbol - Emacs Wiki</a></li>
<li><a href="https://stackoverflow.com/questions/22937393/emacs-lisp-prettify-symbols-mode-for-latex">Emacs-lisp: prettify-symbols-mode for LaTeX - Stack Overflow</a></li>
</ol>
</div>
</div>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-emacs.html">emacs</a> </div>
]]></description>
  <category><![CDATA[emacs]]></category>
  <link>https://dou-meishi.github.io/org-blog/2024-02-24-EmacsPrettifySymbols/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2024-02-24-EmacsPrettifySymbols/notes.html</guid>
  <pubDate>Sat, 24 Feb 2024 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Convergence in Probability]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org0bc40a9">Limiting sets</a></li>
<li><a href="#org217043f">Almost surely convergence and convergence in probability</a></li>
<li><a href="#org9e6d3fa">Convergence in probability and convergence in distribution</a></li>
<li><a href="#org6e3e0a6">Uniqueness of the limit</a></li>
</ul>
</div>
</nav>
<p>
<i>Convergence in probability</i> is a type of convergence of random
variables on the same probability space. It is weaker than <i>almost
surely</i> convergence but stronger than <i>convergence in distribution</i>.
</p>

<p>
In this post, we start by reviewing the concept of <i>infinitely often</i>
set and use it to characterize the complement event of \(\{\lim
X_n=X\}\). Then we give the definition of convergence in probability,
which is by definition weaker than \(‚Ñô(\lim X_n=X)=1\), i.e., the almost
surely convergence. After that a counterexample is given to show that
the converse is not true.  The proof of convergence in probability
implies convergence in distribution is also given. Finally, we give a
necessary and sufficient condition of convergence in probability and
use it to prove 1) the limit of convergence in probability is unique
up to a zero probability set; 2) a continuous mapping preserve the
convergence in probability.
</p>
<div id="outline-container-org0bc40a9" class="outline-2">
<h2 id="org0bc40a9">Limiting sets</h2>
<div class="outline-text-2" id="text-org0bc40a9">
<p>
In order to intuitively introduce this concept, we first recall <i>the
limiting sets</i> of a sequence of sets \(\{A_n\}\).
</p>

$$ \begin{aligned}
\limsup A_n &:= \bigcap_{k=1}^\infty \bigcup_{n=k}^\infty A_n
=: \{A_n\quad\text{i. o.}\}\\
\liminf A_n &:= \bigcup_{k=1}^\infty \bigcap_{n=k}^\infty A_n
=: \{A_n\quad\text{e. a.}\}
\end{aligned} $$

<p>
Clearly, a point \(\omega \in \{A_n\quad\text{i. o.}\}\) if and only if
\(\omega\in A_n\) happens <i>infinitely often</i> as \(n\to\infty\), i.e,
\(\forall k, \exists n ‚â• k, \omega\in A_n\).
</p>

<p>
Similarly, a point \(\omega \in \{A_n\quad\text{e. a.}\}\) if and only if
\(\omega \in A_n\) happens <i>eventually always</i> as \(n\to \infty\), i.e,
\(\exists k, \forall n ‚â• k, \omega \in A_n\).
</p>

<p>
Suppose \(\{A_n\}\) is a sequence of events on a probability space.  It
can be shown that<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>
</p>

$$ \begin{aligned}
‚Ñô(\liminf A_n) ‚â§& \liminf ‚Ñô(A_n) \\
&\limsup ‚Ñô(A_n) ‚â§ ‚Ñô(\limsup A_n).
\end{aligned}$$

<p>
When \(\liminf A_n = \limsup A_n\), we say \(A_n \to A\) and denote by
\(\lim A_n = A\). From above inequalities, it is clear that if \(A_n \to
A\) then \(‚Ñô(A_n) \to ‚Ñô(A)\).
</p>
</div>
</div>
<div id="outline-container-org217043f" class="outline-2">
<h2 id="org217043f">Almost surely convergence and convergence in probability</h2>
<div class="outline-text-2" id="text-org217043f">
<p>
For random variables \(X\) and \((X_n)_{n=1}^\infty\) on the same
probability space, it is natural to consider the event<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup> \(\{\lim
X_n = X\}\). If this set has probability 1, then we say \(X_n\) converges
to \(X\) almost surely, denoted by \(X_n\to X\quad\text{a. s.}\). Sometimes
it is also called \(X_n\) converges to \(X\) with probability 1. Consider
its complement set \(N\). Clearly, by definition \(œâ \in N\) if and only
if there exists some \(œµ > 0\) such that \(|X_n(œâ) - X(œâ)| > œµ\) happens
infinitely often as \(n\to \infty\), i.e., \[ N = \bigcup_{œµ > 0}
\{|X_n(œâ) - X(œâ)| > œµ \quad\text{i. o.}\}. \] Clearly, the union can
be taken over all rational \(œµ\). Then we conclude that \(X_n\to
X\quad\text{a. s.}\) if and only if \[ ‚Ñô(|X_n(œâ) - X(œâ)| > œµ
\quad\text{i. o.}) = 0,\quad\forall œµ > 0.\]
</p>

<p>
<i>Definition.</i> For random variables \(X\) and \((X_n)_{n=1}^\infty\) on the
same probability space, we say \(X_n\) converges to \(X\) in probability,
denoted by \(X_n \to_P X\), if \[\lim_n ‚Ñô(|X_n(œâ) - X(œâ)| > œµ) =
0,\quad\forall œµ > 0.\]
</p>

<p>
Clearly, if \(X_n\) converges to \(X\) almost surely, then \(\limsup
‚Ñô(|X_n(œâ) - X(œâ)| > œµ) = 0\).  Thus, almost surely convergence implies
convergence in probability. However, the converse is not true.
</p>

<p>
<i>Example.</i> Let \(X\equiv0\) and \(X_n=\mathbb{1}(A_n)\). Then \(\{|X_n - X| >
œµ\}=A_n\). Thus, \(X_n \to_P X\) is equivalent to \(‚Ñô(A_n) \to 0\) and \(X_n
\to X\quad\text{a. s.}\) is equivalent to \(‚Ñô(A_n\quad\text{i. o.})=0\).
If we can find \(A_n\) such that \(0= \lim ‚Ñô(A_n) <
‚Ñô(A_n\quad\text{i. o.})\) then we find an example where \(X_n\to_P X\)
but \(X_n\) does not converge to \(X\) almost surely.
</p>

$$ \begin{aligned}
&A_1 = (0, 1/2],\quad A_2 = (1/2, 1],\\
&A_3 = (0, 1/4], \quad A_4 = (1/4, 2/4],
\quad A_5 = (2/4, 3/4], \quad A_6 = (3/4, 1],\\
&\cdots\\
&A_{2^k+i} = ( \frac{i-1}{2^{k+1}}, \frac{i}{2^{k+1}}],\quad
i = 1, 2, \ldots 2^{k+1},\\
&\cdots
\end{aligned}
$$

<p>
Take \(‚Ñô\) be the Lebesgue measure confined on \([0, 1]\). Then \(‚Ñô(A_n)\to
0\) but \(\{A_n\quad\text{i. o.}\}=(0,1]\).
</p>
</div>
</div>
<div id="outline-container-org9e6d3fa" class="outline-2">
<h2 id="org9e6d3fa">Convergence in probability and convergence in distribution</h2>
<div class="outline-text-2" id="text-org9e6d3fa">
<p>
Recall that \(X_n\) is said to converge to \(X\) <i>in distribution</i>, denoted
by \(X_n ‚áí X\), if \(‚Ñô(X_n ‚â§ x) \to ‚Ñô(X ‚â§ x)\) holds for all \(x\) such that
\(‚Ñô(X = x) =0\). It is not hard to show that it is weaker than
<i>convergence in probability</i><sup><a id="fnr.3" class="footref" href="#fn.3" role="doc-backlink">3</a></sup>.
</p>

<p>
<i>Proposition.</i> If \(X_n \to_P X\), then \(X_n ‚áí X\).
</p>

<p>
The converse is clearly not true. Let \(X\) be a uniformly distributed
random variable with range \([0,1]\). Then \(X_n\equiv 1-X\) is also a
uniformly distributed random variable with range \([0,1]\). As \(X_n\) and
\(X\) share the same distribution function \(‚Ñô(X_n ‚â§ x) = ‚Ñô(X ‚â§ x) = x\),
it is clear that \(X_n ‚áí X\). However,
</p>

$$ \begin{aligned}
‚Ñô(|X_n - X| > 1/2) &= ‚Ñô(|1 - 2X| > 1/2)\\
& = ‚Ñô(1 - 2X > 1/2) + ‚Ñô(1 - 2X < -1/2) \\
&= 1/2.
\end{aligned} $$
</div>
</div>
<div id="outline-container-org6e3e0a6" class="outline-2">
<h2 id="org6e3e0a6">Uniqueness of the limit</h2>
<div class="outline-text-2" id="text-org6e3e0a6">
<p>
Finally, we want to discuss the uniqueness of convergence in
probability. This requires the following useful characterization of
convergence in probability, in which a sufficient a necessary
condition is stated as <i>any subsequence contains a further subsequence
which converges almost surely</i>.
</p>

<p>
<i>Theorem.</i> \(X_n \to_P X\) if and only if any subsequence \(\{X_{n_k}\}\)
contains a further subsequence \(\{X_{n_{k(i)}}\}\) such that
\(X_{n_{k(i)}} \to X\) almost surely.
</p>

<p>
See <a href="./proof-to-uniqueness.png">here</a> for the complete proof. Note that in this proof <a href="./first-Borel-Cantelli-lemma.png">the first
Borel-Cantelli lemma</a> is applied.
</p>

<p>
By using this theorem, it is easy to see that if \(X_n ‚áí X\) and \(X_n ‚áí
Y\) then \(X = Y\) almost surely<sup><a id="fnr.4" class="footref" href="#fn.4" role="doc-backlink">4</a></sup>. Moreover, this characterization
asserts that if \(X_n \to_P X\) and \(f\) is continuous then \(f(X_n) \to_P
f(X)\). This is because for any subsequence \(\{f(X_{n_k})\}\) we can
find a further subsequence \(\{f(X_{n_{k(i)}})\}\) such that
\(X_{n_{k(i)}}\) converges to \(X\) almost surely, implying that
\(f(X_{n_{k(i)}})\) converges to \(f(X)\) almost surely as \(f\) is
continuous<sup><a id="fnr.5" class="footref" href="#fn.5" role="doc-backlink">5</a></sup>.
</p>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
For a general measure \(Œº\), the inequality \(Œº(\liminf A_n) ‚â§
\liminf Œº(A_n)\) always holds but \(\limsup Œº(A_n) ‚â§ Œº(\limsup A_n)\)
only holds when \(Œº(\bigcup_{n=k}^\infty A_n) ‚â§ \infty\) for some \(k\).
A counterexample is that: taking \(Œº\) to be the counting measure and
taking \(A_n\) to be the set of integers greater than \(n\). Then \(\limsup
A_n\) is the empty set but \(Œº(A_n)=\infty\) for all \(n\). In fact, these
inequalities follows directly from this lemma:
</p>
<ol class="org-ol">
<li>if \(A_n ‚Üë A\) then \(Œº(A_n) ‚Üë Œº(A)\);</li>
<li>if \(A_n ‚Üì A\) and \(Œº(A_k) < \infty\) for some \(k\) then \(Œº(A_n) ‚Üì
   Œº(A)\).</li>
</ol></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
This is indeed a measurable set when \(X_n\) and \(X\) are
measurable functions. This is because \(\{\lim X_n = X\}\) can be
rewritten as the intersection of \(\{\liminf X_n ‚â• X\}\) and \(\{\limsup
X_n ‚â§ X\}\). Those two sets are measurable because \(\liminf X_n\) and
\(\limsup X_n\) are measurable functions.
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3" role="doc-backlink">3</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Assume \(X_n \to_P X\). Observe that for any \(œµ > 0\) we have
</p>
<ol class="org-ol">
<li>\(\{X_n > x\} \supset \{X > x + œµ\} \cap \{|X_n - X| ‚â§ œµ\}\). Hence,
\(\{X_n ‚â§ x\} \subset \{X ‚â§ x + œµ\} \cup \{|X_n - X| > œµ\}\) and
\(‚Ñô(X_n ‚â§ x) ‚â§ ‚Ñô(X ‚â§ x + œµ) + ‚Ñô(|X_n - X| > œµ)\). Sending
\(n\to\infty\) yields \(\limsup ‚Ñô(X_n ‚â§ x) ‚â§ ‚Ñô(X ‚â§ x + œµ)\) for all \(œµ
   > 0\), implying that \(\limsup ‚Ñô(X_n ‚â§ x) ‚â§ ‚Ñô(X ‚â§ x)\).</li>
<li>\(\{X > x - œµ\} \supset \{X_n > x\} \cap \{|X_n - X| ‚â§ œµ\}\). Hence,
\(\{X ‚â§ x - œµ\} \subset \{X_n ‚â§ x\} \cup \{|X_n - X| > œµ\}\) and \(‚Ñô(X
   ‚â§ x - œµ) ‚â§ ‚Ñô(X_n ‚â§ x) + ‚Ñô(|X_n - X| > œµ)\). Sending \(n\to\infty\)
yields \(‚Ñô(X ‚â§ x - œµ) ‚â§ \liminf ‚Ñô(X_n ‚â§ x)\) for all \(œµ > 0\),
implying that \(‚Ñô(X < x) ‚â§ \liminf ‚Ñô(X_n ‚â§ x)\).</li>
</ol>
<p class="footpara">
Therefore, if \(‚Ñô(X = x) =0\) then \(‚Ñô(X_n ‚â§ x) \to ‚Ñô(X ‚â§ x)\).
</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4" role="doc-backlink">4</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
As \(X_n ‚áí X\), there exists a subsequence \(\{X_{n_k}\}\) which
converges to \(X\) on a probability 1 set \(\Omega_1\). Consider the
subsequence \(\{X_{n_k}\}\).  As \(X_n ‚áí Y\), there exists a futher
subsquence \(\{X_{n_k(i)}\}\) which converges to \(Y\) on a probability 1
set \(\Omega_2\). Clearly, \(Œ©_1 \cap Œ©_2\) has probability 1 and \(X\) and
\(Y\) agree on it.
</p></div></div>

<div class="footdef"><sup><a id="fn.5" class="footnum" href="#fnr.5" role="doc-backlink">5</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
If \(X_n\to X\) at \(\omega\) and \(f\) is continuous at \(X(\omega)\)
then \(f(X_n) \to f(X)\) at \(\omega\).
</p></div></div>


</div>
</div><div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-math.html">math</a> </div>
]]></description>
  <category><![CDATA[math]]></category>
  <link>https://dou-meishi.github.io/org-blog/2024-02-22-ConvinProb/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2024-02-22-ConvinProb/notes.html</guid>
  <pubDate>Thu, 22 Feb 2024 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[MNIST: the Hello World Example in Image Recognition]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org49626cb">Prerequisite</a></li>
<li><a href="#orgc2ea891">About MNIST dataset</a></li>
<li><a href="#org2d1ed3b">Build a CNN clasifier in PyTorch</a>
<ul>
<li><a href="#org9e11c6c">data model</a></li>
<li><a href="#orgdd43a1e">classifier model</a></li>
<li><a href="#org865d192">loss function</a></li>
<li><a href="#org52ee658">optimizer</a></li>
</ul>
</li>
<li><a href="#orgc66492c">Train and test</a></li>
<li><a href="#org05f67bc">Discussion</a></li>
<li><a href="#org0a10618">References</a></li>
</ul>
</div>
</nav>
<p>
In this post we will train a simple CNN (<i>Convolutional Neural
Network</i>) classifier in PyTorch to recognize handwritten digits in
MNIST dataset.
</p>
<div id="outline-container-org49626cb" class="outline-2">
<h2 id="org49626cb">Prerequisite</h2>
<div class="outline-text-2" id="text-org49626cb">
<p>
As we use PyTorch in this post, please ensure it is properly
installed. In addition, we use matploblit to plot figures.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">import</span> matplotlib.pyplot <span style="color: #859900; font-weight: bold;">as</span> plt
<span style="color: #859900; font-weight: bold;">import</span> torch
<span style="color: #859900; font-weight: bold;">import</span> torchvision

<span style="color: #859900; font-weight: bold;">import</span> torch.nn <span style="color: #859900; font-weight: bold;">as</span> nn

<span style="color: #657b83; font-weight: bold;">print</span>(torch.__version__)
</pre>
</div>

<p>
We are referred to <a href="https://pytorch.org/">the PyTorch website</a> for the installation
guide[<a href="#org927c39a">1</a>].  For this post, it is sufficient to use the CPU
version of PyTorch. For Linux, the command looks like
</p>

<div class="org-src-container">
<pre class="src src-shell">pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
</pre>
</div>

<p>
Here we declare hyperparameters for future use[<a href="#org70cb12f">4</a>]. Their meanings
will get clear in the following sections. Besides that, we manually
set the random seed for reproducibility.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #268bd2;">cfg</span> = <span style="color: #657b83; font-weight: bold;">dict</span>(
    n_epochs=3,
    batch_size_train=64,
    batch_size_test=1000,
    learning_rate=0.01,
    momentum=0.5,
    log_interval=10,
)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python">torch.manual_seed(0)
</pre>
</div>
</div>
</div>
<div id="outline-container-orgc2ea891" class="outline-2">
<h2 id="orgc2ea891">About MNIST dataset</h2>
<div class="outline-text-2" id="text-orgc2ea891">
<p>
The MNIST database (<i>Modified National Institute of Standards and
Technology database</i>) is a large database of handwritten digits that
is commonly used for training various image processing systems[<a href="#orgfa19e9a">2</a>].
</p>

<p>
The <code>torchvision</code> package provides a convenient wrapper called
<code>torchvision.datasets.MNIST</code> to access MNIST dataset; see <a href="https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html">its
documentation</a> for more details. For example, the following python
snippet can download the MNIST dataset and load it directly.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #268bd2;">train_data</span> = torchvision.datasets.MNIST(
    root=<span style="color: #2aa198;">"./data/"</span>,
    train=<span style="color: #268bd2; font-weight: bold;">True</span>,
    transform=torchvision.transforms.ToTensor(),
    download=<span style="color: #268bd2; font-weight: bold;">True</span>,
)

<span style="color: #268bd2;">test_data</span> = torchvision.datasets.MNIST(
    root=<span style="color: #2aa198;">"./data/"</span>,
    train=<span style="color: #268bd2; font-weight: bold;">False</span>,
    transform=torchvision.transforms.ToTensor(),
    download=<span style="color: #268bd2; font-weight: bold;">True</span>,
)
</pre>
</div>

<p>
This will download MNIST dataset in the <code>./data/</code> folder if it does not
exist. In addition, the training set and test set will be loaded as
PyTorch tensors.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #657b83; font-weight: bold;">print</span>(train_data)
<span style="color: #657b83; font-weight: bold;">print</span>(f<span style="color: #2aa198;">"The training dataset has shape: </span>{train_data.data.size()}<span style="color: #2aa198;">"</span>)
<span style="color: #657b83; font-weight: bold;">print</span>(test_data)
<span style="color: #657b83; font-weight: bold;">print</span>(f<span style="color: #2aa198;">"The test dataset has shape: </span>{test_data.data.size()}<span style="color: #2aa198;">"</span>)
</pre>
</div>

<p>
From the output, we can see that there are 60,000 training images and
10,000 test images. Each image has \(28 \times 28 = 784\) pixels.
We can also look some images in the training dataset[<a href="#org0358a67">5</a>].
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #268bd2;">fig</span> = plt.figure(figsize=(10, 8))
<span style="color: #268bd2;">cols</span>, <span style="color: #268bd2;">rows</span> = 5, 5
<span style="color: #859900; font-weight: bold;">for</span> i <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(1, cols * rows + 1):
    <span style="color: #268bd2;">sample_idx</span> = torch.randint(<span style="color: #657b83; font-weight: bold;">len</span>(train_data), size=(1,)).item()
    <span style="color: #268bd2;">img</span>, <span style="color: #268bd2;">label</span> = train_data[sample_idx]
    fig.add_subplot(rows, cols, i)
    plt.title(label)
    plt.axis(<span style="color: #2aa198;">"off"</span>)
    plt.imshow(img.squeeze(), cmap=<span style="color: #2aa198;">"gray"</span>)
fig.savefig(<span style="color: #2aa198;">"./sample-images.png"</span>)
</pre>
</div>


<figure id="orgae50a88">
<img src="./sample-images.png" alt="sample-images.png">

</figure>
</div>
</div>
<div id="outline-container-org2d1ed3b" class="outline-2">
<h2 id="org2d1ed3b">Build a CNN clasifier in PyTorch</h2>
<div class="outline-text-2" id="text-org2d1ed3b">
<p>
Let \(f(x; \theta)\) be a classifier with parameters \(\theta\) which
takes the data point \(x\) and predicts its label based on its function
value.
</p>

<p>
By training the classifier on a dataset \(\mathcal{D}\) we roughly mean
solving the optimization problem \[ \min_{\theta}
\operatorname{\mathbb{E}}_{(x_i,y_i)\in \mathcal{D}} [\ell(f(x_i;
\theta), y_i)]. \] Here \(y_i\) is called the label of the data point
\(x_i\). In other words, minimizing the loss function \(\ell(f(x;\theta),
y)\) accross all samples.
</p>

<p>
Solving this problem involves building the following parts:
</p>

<ol class="org-ol">
<li>the data model \(\mathcal{D}\);</li>
<li>the classifier \(f\);</li>
<li>the loss function \(\ell\);</li>
<li>the optimizer to find \(\min_\theta\).</li>
</ol>
</div>
<div id="outline-container-org9e11c6c" class="outline-3">
<h3 id="org9e11c6c">data model</h3>
<div class="outline-text-3" id="text-org9e11c6c">
<p>
The crucial difference between learning and pure optimization is that
the dataset of interest \(\mathcal{D}\) is unknown in a learning
problem. Indeed, we cannot know the data encountered in applications
and their <i>labels</i> before deploying our model. In most cases, we only
have access to a training dataset \(\mathcal{D}_{\text{train}}\) and do
our work with it. As \(\mathcal{D}_{\text{train}}\) may not fit the true
data density, it is often necessary to preserve a part of it to avoid
overfitting, which is called the validation dataset. Nevertheless, for
the sake of simplicity we do not use this technique in this post. As
the MNIST dataset provides the labels of both the training set and
test set, we use the test accuracy to evaluate our model performance
directly. However, it should be keep in mind that if there is no label
data in the test dataset then one has to split the training dataset to
construct a validation dataset manually.
</p>

<p>
We usually do a simple preprocess when loading the data, e.g., a
normalization to scale the data to have mean 0 and std 1. The original
mean and std of MNIST training set can be calculated easily by the
following python statement.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #268bd2;">x</span> = torch.cat([train_data[i][0] <span style="color: #859900; font-weight: bold;">for</span> i <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(<span style="color: #657b83; font-weight: bold;">len</span>(train_data))], dim=0)
<span style="color: #657b83; font-weight: bold;">print</span>(x.mean().item(), x.std().item())
</pre>
</div>

<pre class="example" id="orge251bb1">
0.13066047430038452 0.30810782313346863
</pre>

<p>
As we will use the idea of SGD to optimize the objective function, it
is convenient to create a data loader to iteratively select a <i>batch</i> of
data points.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #268bd2;">train_loader</span> = torch.utils.data.DataLoader(
    torchvision.datasets.MNIST(
        <span style="color: #2aa198;">"./data/"</span>,
        train=<span style="color: #268bd2; font-weight: bold;">True</span>,
        download=<span style="color: #268bd2; font-weight: bold;">True</span>,
        transform=torchvision.transforms.Compose(
            [
                torchvision.transforms.ToTensor(),
                torchvision.transforms.Normalize((0.1307,), (0.3081,)),
            ]
        ),
    ),
    batch_size=cfg[<span style="color: #2aa198;">"batch_size_train"</span>],
    shuffle=<span style="color: #268bd2; font-weight: bold;">True</span>,
)

<span style="color: #268bd2;">test_loader</span> = torch.utils.data.DataLoader(
    torchvision.datasets.MNIST(
        <span style="color: #2aa198;">"./data/"</span>,
        train=<span style="color: #268bd2; font-weight: bold;">False</span>,
        download=<span style="color: #268bd2; font-weight: bold;">True</span>,
        transform=torchvision.transforms.Compose(
            [
                torchvision.transforms.ToTensor(),
                torchvision.transforms.Normalize((0.1307,), (0.3081,)),
            ]
        ),
    ),
    batch_size=cfg[<span style="color: #2aa198;">"batch_size_test"</span>],
    shuffle=<span style="color: #268bd2; font-weight: bold;">True</span>,
)
</pre>
</div>

<p>
Doing so allows us to use for loop to iterate the training dataset
conveniently by writing
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">for</span> batch_x, batch_y <span style="color: #859900; font-weight: bold;">in</span> train_loader:
    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">do SGD with this batch of data</span>
    <span style="color: #859900; font-weight: bold;">pass</span>
</pre>
</div>

<p>
What happens behind this is:
</p>

<ol class="org-ol">
<li>the order of samples in the dataset is shuffled before the iteration;</li>
<li>each sample get preprocessed by normalizing with mean 0.1307 and std 0.3081;</li>
<li>in each step, a fixed number of samples are drown from the dataset
and are stacked into <code>batch_x</code> and <code>batch_y</code>.</li>
</ol>

<p>
An epoch means a whole <i>for loop</i> and a batch means a step in the for
loop.
</p>
</div>
</div>
<div id="outline-container-orgdd43a1e" class="outline-3">
<h3 id="orgdd43a1e">classifier model</h3>
<div class="outline-text-3" id="text-orgdd43a1e">
<p>
From the computational view, a neural network \(f(x;\theta)\) is a
nested function \[ f(\cdot; \theta) = f_{N-1} \circ f_{N-2} \circ
\cdots f_0,\] where each layer \(f_t\) is a parameterized function with
parameter \(\theta_t\). Then the parameter \(\theta\) of the neural
network \(f(x;\theta)\) is actually the collection
\(\{\theta_t,\ t=0,1,\ldots, N-1\}\).
</p>

<p>
In PyTorch, a convolutional layer is a function which accepts a 4D
tensor \(x[\alpha,i,j,k]\) and outputs another 4D tensor \(y[\alpha, i',
j', k']\). The parameter of th layer consists of a bias tensor \(b[i',
j', k']\) and a weight tensor \(w[i', i, j'', k'']\). In particular, \[
y[\alpha, i'] = b[i'] + \sum_{i} w[i', i] \star x[\alpha, i],\] where
\(\star\) is the correlation operator between matrices; see also <a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d">the
documention</a> of <code>torch.nn.Conv2d</code> for more details.  <a href="../2020-12-04-Conv2dNote/notes.html">This post</a> also
gives a review on how <code>torch.nn.Conv2d</code> works.
</p>

<p>
Compared to convolutional layers, fully connected layers, i.e., linear
layers in PyTorch is rather simple. They are just affine
transformations. In the most simple case, a linear layer in PyTorch
accpets a 2D tensor \(x[\alpha, i]\) and outputs another 2D tensor
\(y[\alpha, i']\). The parameter of the layer consists of a bias tensor
\(b[i']\) and a weight tensor \(w[i', i]\). In particular, \[ y[\alpha] =
b + w \circ x[\alpha], \] where \(\circ\) is the matrix-vector product.
In the general case, the index \(\alpha\) might be multiple indices and
the input \(x\) and \(y\) become high order tensors; see also <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">the
documentation</a> of <code>torch.nn.Linear</code>.
</p>

<p>
We use CNN as the basic model of our classifier[<a href="#orgd8a1598">3</a>]. In particular, the
model consists of two 2D convolutional layers followed by two
fully-connected layers. After each convolutional layer, there is a
maximum pooling operation. In addition, the activation function is
called between any two layers. We choose ReLU as our activation
function.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">class</span> <span style="color: #b58900;">Net</span>(nn.Module):
    <span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">__init__</span>(<span style="color: #859900; font-weight: bold;">self</span>):
        <span style="color: #657b83; font-weight: bold;">super</span>().__init__()
        <span style="color: #859900; font-weight: bold;">self</span>.<span style="color: #268bd2;">conv1</span> = nn.Conv2d(1, 10, kernel_size=5)
        <span style="color: #859900; font-weight: bold;">self</span>.<span style="color: #268bd2;">conv2</span> = nn.Conv2d(10, 20, kernel_size=5)
        <span style="color: #859900; font-weight: bold;">self</span>.<span style="color: #268bd2;">fc1</span> = nn.Linear(320, 50)
        <span style="color: #859900; font-weight: bold;">self</span>.<span style="color: #268bd2;">fc2</span> = nn.Linear(50, 10)
        <span style="color: #859900; font-weight: bold;">self</span>.<span style="color: #268bd2;">maxpool</span> = nn.MaxPool2d(kernel_size=2)
        <span style="color: #859900; font-weight: bold;">self</span>.<span style="color: #268bd2;">relu</span> = nn.ReLU()

    <span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">forward</span>(<span style="color: #859900; font-weight: bold;">self</span>, x):
        <span style="color: #268bd2;">x</span> = <span style="color: #859900; font-weight: bold;">self</span>.relu(<span style="color: #859900; font-weight: bold;">self</span>.maxpool(<span style="color: #859900; font-weight: bold;">self</span>.conv1(x)))
        <span style="color: #268bd2;">x</span> = <span style="color: #859900; font-weight: bold;">self</span>.relu(<span style="color: #859900; font-weight: bold;">self</span>.maxpool(<span style="color: #859900; font-weight: bold;">self</span>.conv2(x)))
        <span style="color: #268bd2;">x</span> = x.view(-1, 320)
        <span style="color: #268bd2;">x</span> = <span style="color: #859900; font-weight: bold;">self</span>.relu(<span style="color: #859900; font-weight: bold;">self</span>.fc1(x))
        <span style="color: #268bd2;">x</span> = <span style="color: #859900; font-weight: bold;">self</span>.fc2(x)
        <span style="color: #859900; font-weight: bold;">return</span> x


<span style="color: #268bd2;">clf</span> = Net()
</pre>
</div>
</div>
</div>
<div id="outline-container-org865d192" class="outline-3">
<h3 id="org865d192">loss function</h3>
<div class="outline-text-3" id="text-org865d192">
<p>
As we can see, the output of our model \(f(x; \theta)\) is a vector with
10 components. But how to predict the label of \(x\) and evaluate its
performance? The de facto standard way is interpreting the components
as the logit of the class. For example, in MNIST there are 10 classes,
i.e., 10 labels in total. If the model returns \((t_0, t_1, \ldots,
t_{9})\) for an image, then we say the model predicts that the
probability distribution of the label \(y\) \[ \mathbb{P}(y = i) =
\frac{e^{t_i}}{\sum_i e^{t_i}},\quad i=0,1,\ldots,9. \] Let \(p_i =
\mathbb{P}(y=i)\) be the predicted distribution. We evaluate its
performance by <i>the relative entropy of \(p\) with respect to the true
distribution \(q\)</i>, i.e., \[\ell(f(x;\theta), y) = -\sum_{i}q_i\log
p_i,\] where the true distribution \(q_i\) is, of course, a
deterministic distribution
</p>

$$ q_i = \begin{cases}
1,&\quad \text{$i$ is the true label},\\
0,&\quad \text{otherwise}.
\end{cases}
$$

<p>
Fortunately, PyTorch provides a convenient class <code>CrossEntropyLoss</code> to
carry out above calculations given the predicted logits \(f(x;\theta)\)
and the true label \(y\).
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #268bd2;">loss_func</span> = nn.CrossEntropyLoss()
</pre>
</div>
</div>
</div>
<div id="outline-container-org52ee658" class="outline-3">
<h3 id="org52ee658">optimizer</h3>
<div class="outline-text-3" id="text-org52ee658">
<p>
Given a pair of data \((x^{(i)}, y^{(i)})\), assume the loss of this
pair is \(\phi(\tilde{y}^{(i)}, y^{(i)})\) where
\(\tilde{y}^{(i)}=f(x^{(i)}; \theta)\), we want to compute the gradient
of it w.r.t.  \(\theta\) in order to perform gradient descent. For
example, the mean-square error corresponds to \(\phi(\tilde{y},
y)=\|\tilde{y} - y\|^2\).  Below is a brief summary of <a href="../2021-11-07-BackpropagationFormula/notes.html">this post</a> on
back propagation.
</p>

<p>
Let us slightly overload the notation to denote by \(f_t(\cdot) = f(t,
\cdot, \theta_t)\). Introduce the Hamiltonian \[ H(t, x, u, p) =
p^\intercal f(t, x, u).\] In the calculation of the gradient, the
<i>forward phase</i> is first executed to obtain the <i>state variables</i>
</p>

$$ \begin{aligned}
x_0 &= x^{(i)} \\
x_{t+1} &= \nabla_p H(t, x_k, \theta_k, p)\big\vert_{p=p_{t+1}},\qquad
t = 0, 1, \ldots, N-1.
\end{aligned} $$

<p>
Clearly, this is identical to \(x_{k+1} = f_k\circ f_{k-1}\circ \cdots
f_0(x^{(i)})\). Hence, \(x_{N} = f(x^{(i)}; \theta) = \tilde{y}\) and the
loss is \(\phi(x_N, y)\).  The <i>backward phase</i> is then executed to obtain
the <i>costate variables</i>
</p>

$$ \begin{aligned}
p_N &= \partial_x \phi(x_N, y) \\
p_t &= \nabla_x H(t, x, \theta_t, p_{t+1})\big\vert_{x=x_t},\qquad
t = N-1, \ldots, 1, 0.
\end{aligned} $$

<p>
Clearly, this is identical to \(p_{t} = (\nabla_x f_t(x_t;
\theta_t))^\intercal p_{t+1}\). Here \(\nabla_x f_t\) is a Jacobian
matrix and \((\nabla_x f_t)^\intercal p_{t+1}\) is often computed
efficiently via Jacobian-vector product.
</p>

<p>
Finally, it is not hard to show by induction that the gradient of loss
is \[ \frac{\partial}{\partial \theta_t}\phi(f(x^{(i)}; \theta),
y^{(i)}) = \nabla_u H(t, x_t, u, p_{k+1})\big\vert_{u=\theta_t},\quad t=0, 1, \ldots,
N-1. \]
</p>

<p>
We use the stochastic gradient descent algorithm to find the best net
parameters \(\theta\). There is <i>no need to compute the gradient by
ourselves</i> as PyTorch has implemented the back propagation algorithm
internally and provides various optimizers in <code>torch.optim</code> package. We
choose the simple SGD here.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #268bd2;">optimizer</span> = torch.optim.SGD(
    clf.parameters(), lr=cfg[<span style="color: #2aa198;">"learning_rate"</span>], momentum=cfg[<span style="color: #2aa198;">"momentum"</span>]
)
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-orgc66492c" class="outline-2">
<h2 id="orgc66492c">Train and test</h2>
<div class="outline-text-2" id="text-orgc66492c">
<p>
Finally, we train the model on MNIST dataset. We iterate the training
set and test set several times (called epochs). In each epoch, we
first train the model by going through the whole training set then
test the model performance on the test set. During training process,
we record the training loss after a fixed number of gradient
descents. This progress is then outputed and plotted in a figure.  In
addition, the predicted labels of several examples are visualized.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #268bd2;">train_losses</span> = []
<span style="color: #268bd2;">train_counter</span> = []
<span style="color: #268bd2;">test_losses</span> = []
<span style="color: #268bd2;">test_counter</span> = [i * <span style="color: #657b83; font-weight: bold;">len</span>(train_loader.dataset) <span style="color: #859900; font-weight: bold;">for</span> i <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(cfg[<span style="color: #2aa198;">"n_epochs"</span>] + 1)]


<span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">train</span>(epoch):
    clf.train()
    <span style="color: #859900; font-weight: bold;">for</span> batch_idx, (batch_x, batch_y) <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">enumerate</span>(train_loader):
        optimizer.zero_grad()
        <span style="color: #268bd2;">logits</span> = clf(batch_x)
        <span style="color: #268bd2;">loss</span> = loss_func(logits, batch_y)
        loss.backward()
        optimizer.step()
        <span style="color: #859900; font-weight: bold;">if</span> batch_idx % cfg[<span style="color: #2aa198;">"log_interval"</span>] == 0:
            <span style="color: #657b83; font-weight: bold;">print</span>(
                <span style="color: #2aa198;">"Train Epoch: {} [{}/{} ({:.0f}%)]</span><span style="color: #268bd2; font-weight: bold;">\t</span><span style="color: #2aa198;">Loss: {:.6f}"</span>.<span style="color: #657b83; font-weight: bold;">format</span>(
                    epoch,
                    batch_idx * <span style="color: #657b83; font-weight: bold;">len</span>(batch_x),
                    <span style="color: #657b83; font-weight: bold;">len</span>(train_loader.dataset),
                    100.0 * batch_idx / <span style="color: #657b83; font-weight: bold;">len</span>(train_loader),
                    loss.item(),
                )
            )
            train_losses.append(loss.item())
            train_counter.append(
                (batch_idx * cfg[<span style="color: #2aa198;">"batch_size_train"</span>])
                + ((epoch - 1) * <span style="color: #657b83; font-weight: bold;">len</span>(train_loader.dataset))
            )


<span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">test</span>():
    clf.<span style="color: #657b83; font-weight: bold;">eval</span>()
    <span style="color: #268bd2;">test_loss</span> = 0
    <span style="color: #268bd2;">correct</span> = 0
    <span style="color: #859900; font-weight: bold;">with</span> torch.no_grad():
        <span style="color: #859900; font-weight: bold;">for</span> batch_x, batch_y <span style="color: #859900; font-weight: bold;">in</span> test_loader:
            <span style="color: #268bd2;">logits</span> = clf(batch_x)
            <span style="color: #268bd2;">test_loss</span> += loss_func(logits, batch_y).item()
            <span style="color: #268bd2;">pred</span> = logits.data.<span style="color: #657b83; font-weight: bold;">max</span>(1, keepdim=<span style="color: #268bd2; font-weight: bold;">True</span>)[1]
            <span style="color: #268bd2;">correct</span> += pred.eq(batch_y.data.view_as(pred)).<span style="color: #657b83; font-weight: bold;">sum</span>()
        <span style="color: #268bd2;">test_loss</span> /= <span style="color: #657b83; font-weight: bold;">len</span>(test_loader)
        test_losses.append(test_loss)
        <span style="color: #657b83; font-weight: bold;">print</span>(
            <span style="color: #2aa198;">"</span><span style="color: #268bd2; font-weight: bold;">\n</span><span style="color: #2aa198;">Test set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)</span><span style="color: #268bd2; font-weight: bold;">\n</span><span style="color: #2aa198;">"</span>.<span style="color: #657b83; font-weight: bold;">format</span>(
                test_loss,
                correct,
                <span style="color: #657b83; font-weight: bold;">len</span>(test_loader.dataset),
                100.0 * correct / <span style="color: #657b83; font-weight: bold;">len</span>(test_loader.dataset),
            )
        )


test()
<span style="color: #859900; font-weight: bold;">for</span> epoch <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(1, cfg[<span style="color: #2aa198;">"n_epochs"</span>] + 1):
    train(epoch)
    test()
</pre>
</div>

<pre class="example" id="orgacf39c6">
Test set: Avg. loss: 2.3011, Accuracy: 891/10000 (9%)

Train Epoch: 1 [0/60000 (0%)]	Loss: 2.292757
Train Epoch: 1 [640/60000 (1%)]	Loss: 2.287967
Train Epoch: 1 [1280/60000 (2%)]	Loss: 2.262003
Train Epoch: 1 [1920/60000 (3%)]	Loss: 2.230475
Train Epoch: 1 [2560/60000 (4%)]	Loss: 2.196937
Train Epoch: 1 [3200/60000 (5%)]	Loss: 2.159008
Train Epoch: 1 [3840/60000 (6%)]	Loss: 2.067780
Train Epoch: 1 [4480/60000 (7%)]	Loss: 1.874470
Train Epoch: 1 [5120/60000 (9%)]	Loss: 1.686359
Train Epoch: 1 [5760/60000 (10%)]	Loss: 1.412859
Train Epoch: 1 [6400/60000 (11%)]	Loss: 0.974901
Train Epoch: 1 [7040/60000 (12%)]	Loss: 0.792158
Train Epoch: 1 [7680/60000 (13%)]	Loss: 0.704490
Train Epoch: 1 [8320/60000 (14%)]	Loss: 0.592078
Train Epoch: 1 [8960/60000 (15%)]	Loss: 0.606974
Train Epoch: 1 [9600/60000 (16%)]	Loss: 0.503421
Train Epoch: 1 [10240/60000 (17%)]	Loss: 0.414349
Train Epoch: 1 [10880/60000 (18%)]	Loss: 0.615047
Train Epoch: 1 [11520/60000 (19%)]	Loss: 0.641742
Train Epoch: 1 [12160/60000 (20%)]	Loss: 0.359560
Train Epoch: 1 [12800/60000 (21%)]	Loss: 0.417052
Train Epoch: 1 [13440/60000 (22%)]	Loss: 0.384169
...
Train Epoch: 3 [59520/60000 (99%)]	Loss: 0.129468

Test set: Avg. loss: 0.0577, Accuracy: 9824/10000 (98%)
</pre>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #268bd2;">fig</span> = plt.figure()
plt.plot(train_counter, train_losses, color=<span style="color: #2aa198;">"blue"</span>)
plt.scatter(test_counter, test_losses, color=<span style="color: #2aa198;">"red"</span>)
plt.legend([<span style="color: #2aa198;">"Train Loss"</span>, <span style="color: #2aa198;">"Test Loss"</span>], loc=<span style="color: #2aa198;">"upper right"</span>)
plt.xlabel(<span style="color: #2aa198;">"number of training examples seen"</span>)
plt.ylabel(<span style="color: #2aa198;">"negative log likelihood loss"</span>)
fig.savefig(<span style="color: #2aa198;">"./training-curve.png"</span>)
</pre>
</div>


<figure id="org337e5b3">
<img src="./training-curve.png" alt="training-curve.png">

</figure>

<div class="org-src-container">
<pre class="src src-python">clf.<span style="color: #657b83; font-weight: bold;">eval</span>()

<span style="color: #268bd2;">fig</span> = plt.figure(figsize=(10, 8))
<span style="color: #268bd2;">cols</span>, <span style="color: #268bd2;">rows</span> = 5, 5
<span style="color: #859900; font-weight: bold;">for</span> i <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(1, cols * rows + 1):
    <span style="color: #268bd2;">sample_idx</span> = torch.randint(<span style="color: #657b83; font-weight: bold;">len</span>(train_data), size=(1,)).item()
    <span style="color: #268bd2;">img</span>, <span style="color: #268bd2;">label</span> = train_data[sample_idx]
    <span style="color: #859900; font-weight: bold;">with</span> torch.no_grad():
        <span style="color: #268bd2;">logits</span> = clf(img.unsqueeze(0))
        <span style="color: #268bd2;">pred</span> = logits.data.<span style="color: #657b83; font-weight: bold;">max</span>(1, keepdim=<span style="color: #268bd2; font-weight: bold;">True</span>)[1].item()
    fig.add_subplot(rows, cols, i)
    plt.title(f<span style="color: #2aa198;">"</span>{label}<span style="color: #2aa198;"> (predict: </span>{pred}<span style="color: #2aa198;">)"</span>)
    plt.axis(<span style="color: #2aa198;">"off"</span>)
    plt.imshow(img.squeeze(), cmap=<span style="color: #2aa198;">"gray"</span>)

fig.savefig(<span style="color: #2aa198;">"./pred-sample-images.png"</span>)
</pre>
</div>


<figure id="org33a2429">
<img src="./pred-sample-images.png" alt="pred-sample-images.png">

</figure>
</div>
</div>
<div id="outline-container-org05f67bc" class="outline-2">
<h2 id="org05f67bc">Discussion</h2>
<div class="outline-text-2" id="text-org05f67bc">
<p>
In this post, we trained a CNN classifier to recognize handwritten
digits in MNIST dataset.  The final test accuracy is approximately
98%. There are many ways to improve this result. For example,
</p>

<ol class="org-ol">
<li>adjust hyperparameters to select a better model, including learning
rate, number of training epochs, batch size, etc;</li>
<li>adjust the classifier model, including adding batch normalization layer,
adding dropout layer, manually initializing network parameters, etc[<a href="#orgd114d8d">6</a>, <a href="#org34125b3">7</a>, <a href="#org26d9de8">8</a>];</li>
<li>adjust the optimizer, including using other optimizing algorithm like
Adam and explore their hyperparameters[<a href="#org69e86af">9</a>].</li>
</ol>
</div>
</div>
<div id="outline-container-org0a10618" class="outline-2">
<h2 id="org0a10618">References</h2>
<div class="outline-text-2" id="text-org0a10618">
<ol class="org-ol">
<li><a id="org927c39a"></a> Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., ‚Ä¶ Chintala, S. (2019). PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Advances in Neural Information Processing Systems 32 (pp. 8024‚Äì8035)</li>
<li><a id="orgfa19e9a"></a> LeCun, Y. (1998). The MNIST database of handwritten digits. <a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a></li>
<li><a id="orgd8a1598"></a> Schmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural networks, 61, 85-117.</li>
<li><a id="org70cb12f"></a> Koehler, G. (2020). MNIST Handwritten Digit Recognition in PyTorch. <a href="https://nextjournal.com/gkoehler/pytorch-mnist">https://nextjournal.com/gkoehler/pytorch-mnist</a></li>
<li><a id="org0358a67"></a> Nutan (2021). PyTorch Convolutional Neural Network With MNIST Dataset. <a href="https://medium.com/@nutanbhogendrasharma/pytorch-convolutional-neural-network-with-mnist-dataset-4e8a4265e118">https://medium.com/@nutanbhogendrasharma/pytorch-convolutional-neural-network-with-mnist-dataset-4e8a4265e118</a></li>
<li><a id="orgd114d8d"></a> Ioffe, S., &amp; Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning (pp. 448-456). pmlr.</li>
<li><a id="org34125b3"></a> Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., &amp; Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1), 1929-1958.</li>
<li><a id="org26d9de8"></a> He, K., Zhang, X., Ren, S., &amp; Sun, J. (2015). Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision (pp. 1026-1034).</li>
<li><a id="org69e86af"></a> Kingma, D. P., &amp; Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.</li>
</ol>
</div>
</div>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-ai.html">ai</a> </div>
]]></description>
  <category><![CDATA[ai]]></category>
  <link>https://dou-meishi.github.io/org-blog/2024-02-19-MNISTwithPytorch/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2024-02-19-MNISTwithPytorch/notes.html</guid>
  <pubDate>Mon, 19 Feb 2024 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Quantile Functions]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org9039827">Examples</a></li>
<li><a href="#org252985d">Properties</a></li>
</ul>
</div>
</nav>
<p>
For a distribution function \(F:\mathbb{R}\to [0,1]\), its quatile
function \(q:(0,1)\to\mathbb{R}\) is defined by<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup> \[
q(u):=\inf\{x:F(x)\geq u\}. \] Noting that \(F\) is right continuous, it
is clear that \(\{x: F(x)\geq u\}\) is closed and bounded below for any
\(0 < u < 1\).  Hence, \(q(u)=\min\{x:F(x) \geq u\}\).  Interestingly,
there is also (see the section <a href="#org252985d">Properties</a>) \[ F(x) = \sup\{u: q(u)
\leq x\}.\]
</p>


<p>
The graph of a quantile function can be conveniently derived from the graph of the corresponding
distribution function.
</p>

<ol class="org-ol">
<li>Plot the graph \((x, F(x))\) for \(x\in(-\infty,+\infty)\).</li>
<li>Connect all discontinuity points of \(F\) by adding vertical lines.</li>
<li>Swap the horizontal axis with the vertical axes.</li>
<li>Shrink vertical lines to their lowest points to avoid multiple values.</li>
</ol>

<p>
By the definition of quantile functions, in the last step we should
choose the lowest point as the proper function value while removing a
vertical line. This reveals that, unlike distribution functions,
quantile functions are left continuous.
</p>
<div id="outline-container-org9039827" class="outline-2">
<h2 id="org9039827">Examples</h2>
<div class="outline-text-2" id="text-org9039827">
<p>
<i>Example 1.</i> The distribution function of a exponentially distributed random variable is
</p>

$$
F(x) = \begin{cases}
1 - e^{-x},&\quad x \geq 0,\\
0,&\quad x < 0.
\end{cases}
$$

<p>
Its quantile function is
\[ q(u) = -\ln (1-u),\quad u\in(0,1). \]
</p>

<p>
In this case,
</p>

$$
q(F(x)) = \begin{cases}
x,&\quad x \geq 0,\\
0,&\quad x < 0,
\end{cases}
\quad\text{and}\quad
F(q(u)) = u, \quad 0 < u < 1.
$$

<p>
<i>Example 2.</i> The distribution function of a unit mass is
</p>

$$
F(x) = \begin{cases}
1 ,&\quad x \geq 0,\\
0,&\quad x < 0.
\end{cases}
$$

<p>
Its quantile function is
\[ q(u) = 0,\quad u\in(0,1).\]
</p>

<p>
In this case, \(q(F(x))\) is void and
\[ F(q(u)) = 1, \quad 0 < u < 1.\]
</p>

<p>
<i>Example 3.</i> The distribution function of two equal mass is
</p>

$$
F(x) = \begin{cases}
1 ,&\quad x \geq 1,\\
1/2, &\quad -1 \leq x < 1,\\
0,&\quad x < -1.
\end{cases}
$$

<p>
Its quantile function is
</p>

$$
q(u) = \begin{cases}
1, &\quad 1/2 < u < 1 \\
-1, &\quad 0 < u \leq 1/2.
\end{cases}
$$

<p>
In this case,
</p>

$$
q(F(x)) = -1,\quad -1\leq x < 1\quad\text{and}\quad
F(q(u)) = \begin{cases}
1,&\quad 1/2 < u < 1,\\
1/2,&\quad 0 < u \leq 1/2.
\end{cases}
$$

<p>
Note that in this example, the quantile function is left continuous.
</p>
</div>
</div>
<div id="outline-container-org252985d" class="outline-2">
<h2 id="org252985d">Properties</h2>
<div class="outline-text-2" id="text-org252985d">
<p>
By definition, a quantile function is clearly nondecreasing due to the
nondecreasing nature of distribution functions. However, a quantile
function is left continuous, while distribution functions are right
continuous. Indeed, as \(q\) is nondecreasing, we can show that
\(q(u-) = q(u)\)<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>.
</p>

<p>
By definition, \[ q(F(x_0)) = \inf\{x: F(x) \geq F(x_0)\} \leq x_0,
\quad\forall x_0\in\mathbb{R}.\] This inequality even holds when
\(F(x_0)\) equals 0 or 1<sup><a id="fnr.1.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>.  This inequality leads to the equation
\(F(x)=\sup\{u:q(u) \leq x\}\)<sup><a id="fnr.3" class="footref" href="#fn.3" role="doc-backlink">3</a></sup>. Thus, \[ F(q(u_0)) = \sup\{u: q(u)
\leq q(u_0)\} \geq u_0,\quad\forall u_0\in(0,1).\] Similarly, the
supremum can be replaced by maximum as \(q\) is left continuous.
</p>

<p>
As a result, there are \[ x \geq q(u) ‚áí F(x) \geq F(q(u)) \geq u \]
and \[ F(x) \geq u ‚áí x \geq q(F(x)) \geq q(u),\] which means \(x \geq
q(u)\) if and only if \(F(x) \geq u\).  It is also true that \(x < q(u)\)
if and only if \(F(x) < u\). However, it is possible that \(x \leq q(u)\)
holds but \(F(x) \leq u\) fails. This happens when \(F(q(u)) > u\), where
\(x=q(u)\) but \(F(x) > u\).
</p>

<p>
Assume \(U\) is a uniformly distributed variable with distribution
\(\mathbb{P}(U \leq u)=u\) for \(u\in(0,1)\). Then the random variable
\(X=q(U)\) has distribution \[ \mathbb{P}(X \leq x) = \mathbb{P}(q(U)
\leq x) = \mathbb{P}(U \leq F(x)) = F(x).\]
</p>

<p>
<i>Summary of properties of quantile functions.</i>
</p>

<ol class="org-ol">
<li>Domain is \((0, 1)\) and range is \(\mathbb{R}\).</li>
<li>Nondecreasing and left continuous.</li>
<li>\(q(F(x)) \leq x\) and \(F(q(u)) \geq u\)</li>
<li>\(q(u) \leq x\) if and only if \(u \leq F(x)\)</li>
<li>\(q(u) > x\) if and only if \(u > F(x)\)</li>
<li>\(q(U)\) has distribution function \(F\)</li>
</ol>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Define that \(F(-\infty)=0\) and \(F(\infty)=1\).  Then \(F\) becomes
a function from \(\overline{\mathbb{R}}\) to \([0,1]\) and \(q\) becomes a
function from \([0,1]\) to \(\overline{\mathbb{R}}\).
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
For any \(œµ > 0\), we have \(F(q(u-œµ)) \leq
F(q(u-))\). Applying \(F(q(u)) \geq u\) yields \(u-œµ \leq F(q(u-))\).
Hence, \(F(q(u-)) \geq u\) and by the
definition of quantile functions \(q(u-) \geq q(u)\).
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3" role="doc-backlink">3</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
As \(q(F(x)) \leq x\), we know that \(F(x) \in \{u: q(u) \leq
x\}\), which means \(F(x) \leq \sup\{u:q(u) \leq x\}\).  For the converse
direction, it suffices to show that if \(q(u) \leq x\) then \(u \leq F(x)\).
However, this is true by definition.
</p></div></div>


</div>
</div><div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-math.html">math</a> </div>
]]></description>
  <category><![CDATA[math]]></category>
  <link>https://dou-meishi.github.io/org-blog/2024-02-13-QuantileFunction/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2024-02-13-QuantileFunction/notes.html</guid>
  <pubDate>Tue, 13 Feb 2024 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Convergence in Distribution and Weak Convergence]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org6114a94">Definitions</a></li>
<li><a href="#org98f837c">Skorohod's theorem</a></li>
<li><a href="#orgaba8d75">Equivalent definition of weak convergence</a></li>
</ul>
</div>
</nav>
<p>
In statistics, we often want to study the asymptotic behavior of
random variables, or in other words, the limit of their
distributions. This concept appears in the central limit theorem which
asserts that the empirical mean of random observations always converges
to a normal distribution, regardless the distribution of the observed
random variable.
</p>

<p>
There are several concepts of the convergence of random variables,
e.g, almost surely convergence, convergence in probability and
convergence in distribution. <i>Convergence in distribution</i> (also called
<i>convergence in law</i>) is the weakest type of convergence among them. It
is actually defined in terms of the pointwise convergence of
distribution functions. Hence, it is also a type of convergence of
probability measures, as there is a one-to-one correspondence between
distribution functions and probability measures. The concept of weak
convergence of probability measures is therefore introduced.
</p>

<p>
Below, we first give the definitions of convergence in distribution of
random variables and weak convergence of probability measures.  Then
the Skorhod's theorem is proved and a direct application of it is
given. Finally, we discuss equivalent characterization of weak
convergence without referring to distribution functions.  The result
is helpful in understanding the weak topology in the space of
probability measures. For complete discussions, please refer to
Billingsley's book and Parthasarathy's book.
</p>

<p>
Random variables mentioned below are all real-valued.  The
distribution function \(F\) of a random variable \(X\) is thereby \(F(x)=
\mathbb{P}(X \leq x)\).  The theory of random variables valued in
\(\mathbb{R}^n\), and or more general in a Polish space, is a bit
complicated and left untouched.
</p>
<div id="outline-container-org6114a94" class="outline-2">
<h2 id="org6114a94">Definitions</h2>
<div class="outline-text-2" id="text-org6114a94">
<p>
<i>Definition.</i> A sequence of real-valued random variables \(\{X_n\}\) (with
distribution functions \(\{F_n\}\)) is said to <i>converge in distribution</i>
or <i>in law</i> to \(X\) (with distribution function \(F\)) if \(F_n(x)\)
converges to \(F(x)\) at continuity points \(x\) of \(F\), i.e., \[\lim_n
F_n(x) = F(x), \quad\text{for every continuity point $x$ of $F$}.\]
</p>

<p>
<i>Remark.</i> In this definition, it is the distribution function which
matters, and there is nothing to do with the underlying probability
space. So, random variables \(\{X_n\}\) may be defined on entirely
different probability space.
</p>

<p>
As we all know, there is a one-to-one correspondence between
distribution functions and probability measures<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>.  Therefore, we
can define the convergence of probability measures in the same vain.
</p>

<p>
<i>Definition.</i> A sequence of probability measures \(\{\mu_n\}\) on the real
line is said to <i>converge weakly</i> to a probability measure \(\mu\) if \[
\lim_n \mu_n((-\infty, x]) = \mu((-\infty, x]), \quad \text{for which
$\mu(\{x\}) = 0$}.\]
</p>

<p>
<i>Remark.</i> Like the continuity condition in the definition of convergence
in distributions, here the condition \(\mu(\{x\})=0\) is also essential.
In fact, the set \((\infty, x])\) is called a \(\mu\)-continuity set if
\(\mu(\{x\})=0\). If a set \(A\) is not a \(\mu\)-continuity set, then
\(\lim_n\mu_n(A)\) may fail to converge to \(\mu(A)\).
</p>

<p>
<i>Notation.</i> We write \(X_n ‚áí X\) if \(X_n\) converge in law to \(X\).  In this
case, we may also write \(F_n ‚áí F\).  Similarly, we write \(\mu_n ‚áí \mu\)
if \(\mu_n\) converge weakly to \(\mu\).
</p>

<p>
Obviously, random variables \(X_n\) converge in law to \(X\) if
and only if probability measures \(\mathbb{P}_{ X_n}\) converge weakly
to \(\mathbb{P}_{X}\), i.e.,
\[\lim_n \mathbb{P}_{X_n}((-\infty, x]) =
\mathbb{P}_{X}((-\infty, x]), \quad\text{for every $x$ such that
$\mathbb{P}_X(\{x\})=0$}.\]
</p>

<p>
<i>Example.</i> Let \(\mu_n\) corresponds to a mass of \(1/n\) at each point \(0,
\frac{1}{n}, \frac{2}{n}, \ldots, \frac{n-1}{n}\). Then it is clear
that \(\mu_n\) converges weakly to \(\mu\), the Lebesgue measure confined
on \([0,1]\).  However, \(\mu_n(\mathbb{Q}\cap[0,1])=1\) for all \(n\) but
\(\mu(\mathbb{Q}\cap[0,1])=0\).
</p>

<p>
<i>Example.</i> Let \(X_n\equiv a_n\) and \(X\equiv a\). Then
 \(F_n(x)=\mathbb{1}(x\geq a_n)\) and \(F(x)=\mathbb{1}(x\geq a)\).
 Clearly, \(X_n\) converges in distribution to \(X\) if and only if
 \(a_n\to a\). However, if \(a_n > a\) for infinite many \(n\), then
 \(F_n(a)\) fails to converge to \(F(a)\).
</p>

<p>
Finally, we should mention that the limit of convergence in law and
weak convergence is unique. If \(F_n ‚áí F\) and \(F_n ‚áí G\), then
\(F(x)=G(x)\) holds for every \(x\), including their discontinuities.
Indeed, by the definition of weak convergence, \(F\) and \(G\) agree on
the real line except countable points. As \(F\) and \(G\) are right
continuous, they must agree on those countable points. See <a href="./proof-to-uniqueness-of-weak-limit.png">here</a> for a
complete proof.
</p>
</div>
</div>
<div id="outline-container-org98f837c" class="outline-2">
<h2 id="org98f837c">Skorohod's theorem</h2>
<div class="outline-text-2" id="text-org98f837c">
<p>
For any probability measure \(\mu_n\) on the real line, we can construct
a probability space and a random variable \(Y_n\) on it such that \(Y_n\)
induces \(\mu_n\). For probability measures \(\mu_n ‚áí \mu\), the following
theorem states that \(Y_n\) and \(Y\) can be constructed on the same
probability space, and even in such a way that \(Y_n(\omega) \to
Y(\omega)\) fo every \(\omega\), which is a much stronger condition than
\(Y_n ‚áí Y\).
</p>

<p>
<i>Theorem [Skorohod].</i> Suppose that \(\mu_n ‚áí \mu\). Then there exists a
probability space \((\Omega,\mathcal{F},\mathbb{P})\) and random
variables \(Y_n\) and \(Y\) on it such that \(Y_n\) has distribution
\(\mu_n\), \(Y\) has distribution \(\mu\), and \(Y_n(\omega)\to Y(\omega)\)
for every \(\omega\).
</p>

<p>
<i>Proof.</i> The proof is constructive and based on quantile functions. See
<a href="./proof-Skorohod-theorem.png">here</a> for the complete proof. You may also want to look at <a href="../2024-02-13-QuantileFunction/notes.html">this post</a>
for a brief review of quantile functions. Below is the sketch of
constructing those random variables.
</p>

<p>
Let \(F_n\) and \(F\) be the distribution functions corresponding to
\(\mu_n\) and \(\mu\).  Let \(q_n\) and \(q\) be the corresponding quantile
functions.  Take the probability space on which there exists a random
variable \(U\) which follows the uniform distribution on \((0,1)\). Then
\(q_n(U)\) and \(q(U)\) have distribution \(\mu_n\) and \(\mu\) respectively.
It can be proved that if \(F_n(x)\to F(x)\) at continuity points of \(F\)
then \(q_n(u)\to q(u)\) at continuity points of \(q\). Let
\(N\subset(0,1)\) be the set of discontinuity points of \(q\).
Define
</p>

$$
Y_n(\omega) = \begin{cases}
q_n(U(\omega)),&\quad \omega\not\in N,\\
0,&\quad\omega\in N,
\end{cases}
\quad
Y(\omega) = \begin{cases}
q(U(\omega)),&\quad \omega\not\in N,\\
0,&\quad\omega\in N.
\end{cases}
$$

<p>
Then \(Y_n\) and \(Y\) satisfy the desired properties.
</p>

<p>
Q.E.D.
</p>

<p>
<i>Corollary 1.</i> If \(X_n ‚áí X\) and \(\mathbb{P}(X \in D_h)=0\), then \(h(X_n)
‚áí h(X)\).  Here, \(D_h\) is the set of discontinuity points of the
measurable function \(h\).
</p>

<p>
<i>Proof.</i> See <a href="./proof-to-corollary1.png">here</a>.
</p>

<p>
<i>Corollary 2.</i> If \(X_n ‚áí a\) and \(h\) is continuous at \(a\), then \(h(X_n) ‚áí
h(a)\).
</p>
</div>
</div>
<div id="outline-container-orgaba8d75" class="outline-2">
<h2 id="orgaba8d75">Equivalent definition of weak convergence</h2>
<div class="outline-text-2" id="text-orgaba8d75">
<p>
<i>Definition.</i> A set \(A\) is a \(\mu\)-continuity set if it is a Borel set
and \(\mu(\partial A)=0\).  Here, the boundary \(\partial A\) is the
closure of \(A\) minus its interior.
</p>

<p>
<i>Theorem.</i> The following conditions are equivalent.
</p>

<ol class="org-ol">
<li>\(\mu_n ‚áí\mu\);</li>
<li>\(\int f\,d\mu_n \to \int f\,d\mu\) for every bounded and continuous
real function \(f\);</li>
<li>\(\int f\,d\mu_n \to \int f\,d\mu\) for every bounded and uniformly
continuous real function \(f\);</li>
<li>\(\mu_n(A)\to \mu(A)\) for every \(\mu\)-continuity set \(A\).</li>
</ol>

<p>
Those equivalent statements can be used to define the weak convergence
of probability measures on general Polish spaces, not limited to the
real line. See Parthasarathy's book <i>Probability Measures on Metric
Spaces</i> for a complete discussion
</p>

<p>
<i>Proof.</i> See <a href="./proof-equivalent-definition-weak-convergence.png">here</a>. The basic ideas in the proof are listed below.
</p>

<ol class="org-ol">
<li>Clearly 4) ‚áí 1) and 2) ‚áí 3).</li>
<li>If 1) holds, then we can prove <i>for any bounded real function on
\(\mathbb{R}\) such that the discontinuity points of \(f\) is a
\(\mu\)-null set there is \(\int f\,d\mu_n\to\int f\,d\mu\)</i>.
Hence, 1) ‚áí 2), 1) ‚áí 3), 1) ‚áí 4).</li>
<li>Then it remains to show 3) ‚áí 1). For any \(x\in\mathbb{R}\) and \(œµ >
   0\), pick a bounded and uniformly continuous function \(f_œµ\) such
that \(\mathbb{1}_{(-\infty, x]} \leq f_œµ \leq \mathbb{1}_{(-\infty,
   x+\epsilon]}\) on the real line (e.g., the function \(f_œµ\) can be
constructed by piecewise linear function). Then \(\int
   f_œµ\,d\mu_n\to\int f_œµ\,d\mu\) implies \(\limsup \mu_n((-\infty,x])
   \leq \mu((-\infty,x+œµ])\). Similarly, there is \(\mu((-\infty, x-œµ])
   \leq \liminf \mu_n((-\infty,x])\). Then
\(\mu_nu((-\infty,x])\to\mu((-\infty,x])\) when \(\mu(\{x\})=0\).</li>
</ol>

<p>
Q.E.D.
</p>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
For a given probability measure \(\mu\) on the real line, the
function \(F(x):=\mu((-\infty,x])\) is nondecreasing, right-continuous,
and satisfies \(F(-\infty)=0\) and \(F(\infty)=1\), and thus is a
distribution function. Conversely, for a given distribution function
\(F\), let \(q:(0,1)\to\mathbb{R}\) be the quantile function of \(F\): \(q(u)
= \inf\{x: u\leq F(x)\}\). Then \(q(u) \leq x\) if and only if \(u \leq
F(x)\). Hence, the random variable \(q\) has the distribution \(F\) and
induces a probability measure \(\mu\). See also <a href="../2024-02-13-QuantileFunction/notes.html">this post</a> for a brief
discussion on quantile functions.
</p></div></div>


</div>
</div><div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-math.html">math</a> </div>
]]></description>
  <category><![CDATA[math]]></category>
  <link>https://dou-meishi.github.io/org-blog/2024-02-09-ConvinDistandWeakConv/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2024-02-09-ConvinDistandWeakConv/notes.html</guid>
  <pubDate>Fri, 09 Feb 2024 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[How I create backups]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org05d52d0">Different focus of syncing and backing up</a></li>
<li><a href="#orgfa08e9b">Presync</a></li>
<li><a href="#org45ddb2c">Backup</a></li>
</ul>
</div>
</nav>
<p>
In <a href="https://dou-meishi.github.io/org-blog/2023-09-16-UltimateFileManagement/events-oriented-file-storage-framework">the previous post</a>, I discussed the how I organize the folder
<code>~/Documents/</code>. In my home directory <code>~/</code>, there are another two folders
with special purposes: <code>~/Assets/</code> and <code>~/AAA/</code>.
</p>

<p>
Generally speaking, <code>AAA/</code> severs as a trash bin; <code>Assets/</code> severs as a
repo for large files; <code>Documents/</code> severs as the main workspace,
containing all event folders; see <a href="https://dou-meishi.github.io/org-blog/2023-09-16-UltimateFileManagement/events-oriented-file-storage-framework">this post</a> for what is an event
folder.  During backups, <code>AAA/</code> is ignored, and only <code>Assets/</code> and
<code>Documents/</code> are considered.
</p>

<p>
In general, if any folder or file outside these three folders is of
interest, they should have a local backup in <code>Assets/</code> or <code>Documents/</code>,
maintaining by a special script <code>dms-presync</code>. This script should be
executed before the actual backup process begins.
</p>
<div id="outline-container-org05d52d0" class="outline-2">
<h2 id="org05d52d0">Different focus of syncing and backing up</h2>
<div class="outline-text-2" id="text-org05d52d0">
<p>
First, I would like to give some difference between syncing and
backing up based on personal experiences.
</p>

<p>
In synchronization, it is generally advised to treat folder as the
smallest unit and avoid excluding some files or subfolders within the
folder. Doing so ensures a seamless experience in switching machines
to work on the same folder. When a folder is integrated into the
synchronization framework, one should expect to have an identical
experience across different machines.
</p>

<p>
On the other hand, backup procedures differ. In the context of backup,
the primary goal is to prevent any data loss, rather than emphasizing
a seamless experience when switching between machines. Consequently,
certain unnecessary files could and should be ignored.
</p>
</div>
</div>
<div id="outline-container-orgfa08e9b" class="outline-2">
<h2 id="orgfa08e9b">Presync</h2>
<div class="outline-text-2" id="text-orgfa08e9b">
<p>
The purpose of this stage is to ensure that it is sufficient to
consider only the two folders <code>Assets/</code> and <code>Documents/</code> in creating
backups.
</p>

<ol class="org-ol">
<li>Normalize event folders in <code>Documents/</code>.</li>

<li><p>
Ensure all files of interest have an up-to-date version in <code>Assets/</code> or <code>Documents/</code>.
</p>

<p>
Some folders involved in this stage include, e.g., <code>~/.local/share/fonts/</code>,
<code>~/org/</code> and <code>~/.themes/</code>.
</p></li>
</ol>
</div>
</div>
<div id="outline-container-org45ddb2c" class="outline-2">
<h2 id="org45ddb2c">Backup</h2>
<div class="outline-text-2" id="text-org45ddb2c">
<p>
The actual backup process can be generally split into two steps:
backup <code>.git/</code> folders and backup other folders.
</p>

<p>
For each git repos, I manually back them up to the special upstream
<code>backup</code>, which could be a remote repo in some cloud server or a local
repo in the backup drive.<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>
</p>

<p>
Suppose then I want to backup all necessary files in <code>/home/dou</code> to
<code>/media/dou/BACKUP/dou/</code>, I go through the following steps (in this
stage, all <code>.git/</code> folders are ignored).
</p>

<ol class="org-ol">
<li value="0">Ensure <code>/media/dou/BACKUP/dou/</code> is a git repo.</li>

<li>Run <code>unison backup</code> to bakcup <code>~/Assets/</code> and <code>~/Documents/</code>.</li>

<li>Commit in <code>/media/dou/BACKUP/dou/</code>.</li>
</ol>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
The command <code>find . -name ".git"</code> can list all git repos.
</p></div></div>


</div>
</div><div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-think.html">think</a> </div>
]]></description>
  <category><![CDATA[think]]></category>
  <link>https://dou-meishi.github.io/org-blog/2024-02-01-TopLevelFolders/how-i-sync-and-backup-files.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2024-02-01-TopLevelFolders/how-i-sync-and-backup-files.html</guid>
  <pubDate>Sat, 03 Feb 2024 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Event-oriented File Storage Framework]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org457cc1b">Organize files into events</a>
<ul>
<li><a href="#org0f2a17f">Groupping files into categories is not I want</a></li>
<li><a href="#orgbacb960">Tagging every file is not I want</a></li>
<li><a href="#orgfe56530">Event directory is all I need</a></li>
</ul>
</li>
<li><a href="#orgdbda09c">Further discussion on the event-oriented approach</a>
<ul>
<li><a href="#orgb8ace20">Share assets between events</a></li>
<li><a href="#org2bcaae5">Write a descriptive journal for each event</a></li>
<li><a href="#orge18f22c">Generate an index page listing all events</a></li>
<li><a href="#orgfadfa26">How to transform a folder to an event folder</a></li>
<li><a href="#org90375f9">Tips</a></li>
</ul>
</li>
</ul>
</div>
</nav>
<p>
Every day, as I create, receive or update a lot of files (like notes,
blogs, papers, books, assets, slides, git repositories, experimental
codes, and various other types), I find myself pondering the
possibility of having a cohesive method to store, access, and back
them up.
</p>

<p>
After years of thinking and trying, I guess it is time to settle down
my framework of <i>storing</i> files. This framework aims to be simple and
clean, and serve as a solid foundation for scripting access and backup
functionalities that meet my specific requirements..
</p>
<div id="outline-container-org457cc1b" class="outline-2">
<h2 id="org457cc1b">Organize files into events</h2>
<div class="outline-text-2" id="text-org457cc1b">
<p>
The central idea of this file storage framework is organizing files
into different <i>events</i>. As I described in <a href="../2023-04-09-ManageFiles/notes.html">this post</a>, an event is simple
a folder with name pattern <code>YYYY-mm-dd-BriefName/</code>.  Almost all my files
are placed in <code>~/Documents/</code> folder, under which I put all my event
folders.
</p>

<pre class="example" id="orga31df52">
drwxrwxr-x  2 dou dou   4096 Jan 29 10:53 2023-04-09-ManageFiles/
drwxrwxr-x  2 dou dou   4096 Jan 28 13:10 2023-04-30-OptimalityandKKTCondition/
drwxrwxr-x  2 dou dou   4096 Jan 31 00:07 2023-09-16-UltimateFileManagement/
drwxrwxr-x  2 dou dou   4096 Jan 28 13:02 2023-09-19-Compactness/
drwxrwxr-x  2 dou dou   4096 Jan 28 13:02 2023-10-23-BanachSpaceExample/
drwxrwxr-x  2 dou dou   4096 Jan 28 13:02 2024-01-07-ReviewUnison/
drwxrwxr-x  2 dou dou   4096 Jan 28 13:02 2024-01-11-CodeBlockinLaTeX/
drwxrwxr-x  2 dou dou   4096 Jan 28 13:02 2024-01-14-TryOrgPublish/
drwxrwxr-x  2 dou dou   4096 Jan 28 13:02 2024-01-22-TryOrgStaticBlog/
</pre>

<p>
When I review or share my works, event is the smallest unit.
</p>

<p>
Before elaborating on reasons why I choose this <i>event</i>-oriented
approach, let me first discuss two approaches that I tried but gave up
eventually.
</p>
</div>
<div id="outline-container-org0f2a17f" class="outline-3">
<h3 id="org0f2a17f">Groupping files into categories is not I want</h3>
<div class="outline-text-3" id="text-org0f2a17f">
<p>
The first approach I tried is <i>category</i>-oriented. It is not surprised,
since we all have those pre-created folders named <i>Documents</i>, <i>Videos</i>,
<i>Musics</i> etc. To better classifying my files, I created new folders
named <i>References</i>, <i>Slides</i>, <i>Codes</i>, <i>Subjects</i>, and later on more specific
folders such as <i>Manuscripts</i>, <i>Notes</i>, <i>Templates</i>, <i>Plugins</i>.
</p>

<p>
Some of those folders work well, like <i>Musics</i>, <i>Books</i> and <i>Plugins</i>.  They
are well defined and I am very confident on whether a new file or
folder should be placed inside them and whether some file or folder I
am looking for will be there. However, some folders quickly become too
deep and their positions overlap with other folders. For example, the
<i>Subjects</i> folder is created with subfolders named <i>Math</i>, <i>Physics</i> and <i>CS</i>,
and each subfolder contains course subsubfolders like
<i>RealAnalysis</i>, <i>Probability</i> and <i>Mechanics</i>.
</p>

<pre class="example" id="orgffb7ebc">
Subjects/
‚îú‚îÄ‚îÄ CS/
‚îú‚îÄ‚îÄ Math/
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Probability/
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ RealAnalysis/
‚îî‚îÄ‚îÄ Physics/
    ‚îî‚îÄ‚îÄ Mechanics/
</pre>

<p>
Looks like neatly organized, right? But when it comes to practice, it
is a totally different story. Imagine that I attend one lecture of
probability, downloaded professor's slide and the homework. After
several days of self-studying, I probably finished the homework and
wrote a small note on what I had learnt. Now, how do I deal with those
files?
</p>

<p>
If I insist on organizing files according to their categories, the
slide and note should be placed in the top level <i>Slides</i> folder and
<i>Notes</i> folder respectively. It seems that only the homework will be
placed in the <i>Probability</i> folder, which is clearly not a folder meant
for homeworks however.  Moreover, when I move that slide into the
<i>Slides</i> folder, should I follow the original hierarchy like
<code>Slides/Math/Probability/</code>? Should I also move that note into
<code>Notes/Math/Probability/</code>?  What if I also wrote some experimental
code?
</p>

<p>
Manually enforcing that a file should be placed inside a certain
category folder is boring. Doing so also breaks the connection between
a group of files. In this scenario, these files are produced within a
small period of time and have strong connections to each other. In
addition, storing them in different folders is a bad idea for
synchronization. Imagine that I have to switch between different
machines during writing my note and that homework. Comparing with a
single folder, storing in several folders with complex hierarchy
structures is clearly more troublesome.
</p>

<p>
There are also other problems with this category-oriented approach,
like the inefficiency introduced by very deep folder structures, and
unbalanced folder sizes.
</p>

<p>
While I may continue to use folders like <i>Musics</i>, <i>Videos</i> and <i>Books</i>, I
will certainly not continue to create new top level folders and
enforce that any file matching a category should be placed inside the
corresponding top level folder.
</p>
</div>
</div>
<div id="outline-container-orgbacb960" class="outline-3">
<h3 id="orgbacb960">Tagging every file is not I want</h3>
<div class="outline-text-3" id="text-orgbacb960">
<p>
After the failure of category-based approach, I was still looking for
a way to organize my files in a logical structure.  Soon or later, I
realized that one of the crucial drawbacks of the previous approach is
the exclusive nature of categories, i.e., a file must belong to one
category (one top-level folder) or another one, but not both.
</p>

<p>
Indeed, it is quite natural to think a file can only sit in one place
on the disk. However, in terms of the various attributes of a
particular file, we may want to find it in different locations. Take
the example mentioned before of attending a lecture. It would be very
natural to assume the note should appear in both
<code>Notes/Math/Probability/</code> and <code>Subjects/Math/Probability/</code>.  When I am
looking for a slide, I may consult the top-level folder <i>Slides</i>. When I
am looking for all staffs related to a particular subject, I may
consult the top-level folder <i>Subjects</i>.
</p>

<p>
Following this line of thinking, I then realized that the (sub)folder
name acts like tags. A file in <code>Notes/Math/Probability/</code> are expected to
have tags <i>Notes</i>, <i>Math</i> and <i>Probability</i>. In this point of view, the
deep hierarchy structure is actually not important. It is meaningless
to differentiate between folder <code>Notes/Math/Probability/</code> and folder
<code>Math/Probability/Notes/</code>.
</p>

<p>
Then I imagined a tag-based approach of organizing files. A file may
have arbitrary tags, e.g., <code>tagA, tagB, tagC</code>.  For each tag, I create a
top-level folder with the same name. The true location of a file does
not matter. I can put it at anywhere. However, as long as I give a tag
to the file, I will create a symbolic link of this file in the
corresponding tag folder. Then it is not hard to write a small script
which can list all files having a particular collection of tags.
</p>

<p>
Of course, there are other ways of implementing a tag-based file
system. Besides the way of symbolic links, one can also use
</p>

<ol class="org-ol">
<li>hard links;</li>
<li>database, keeping records of file paths and their tags;</li>
<li>special name convention, similar to database, but tags are
embedded in the file name.</li>
</ol>

<p>
Well, this approach sounds very nice theoretically too.  But I never
seriously try it in practice.
</p>

<ol class="org-ol">
<li>It is actually a framework of file access, not file storage.  It
does not answer how to organize files in the disk.  Indeed, all
current file systems are tree/folder based, not tag based.</li>

<li>Too sophisticated to maintain. Links in tag folders, database and
special words in filenames are all too complicated to manipulate.</li>

<li>Tagging every file is tedious, especially since the need to search
by tags doesn't arise frequently.</li>
</ol>

<p>
I want a simple solution to <i>store</i> my files. Assigning tags to files
might be useful for viewing and searching, but does not solve my
problem. For special type of files, like books and notes, I may try to
manage them by tags, but I will not try to put every file in this
framework.
</p>
</div>
</div>
<div id="outline-container-orgfe56530" class="outline-3">
<h3 id="orgfe56530">Event directory is all I need</h3>
<div class="outline-text-3" id="text-orgfe56530">
<p>
In practice, after I abandoned those category folders, I went to the
event-oriented approach to organize files. Actually, I adopted this
approach even before I notice the concept of event directory.  At the
beginning, I simply put all files I need for a particular task in a
separate folder. Then I had so many those folders and I decided to add
a date prefix to sort them antichronologically. That's it. I found
myself so comfortable with this file structure.
</p>

<ol class="org-ol">
<li><i>Self-contained.</i> An event folder contains all files I need to work
on this task. I can work on it without boring myself on other
folders most of the time.  When I switch machines, I need only to
ensure this event folder is synchronized, without wasting time on
syncing unnecessary files.</li>

<li><i>Flexible.</i> I can put anything inside an event folder and organize
them in the way I like. For example, I can put pictures required by
a latex manuscript, a git repo to track some experimental scripts,
some assets collected from the internet, etc. In fact, I just treat
an event folder as the workspace for it and put any necessary files
in it.</li>

<li><i>Flat strcture.</i> All even folders are placed in the same level.  No
intermediate folders like <code>2023/</code> or <code>2024/</code>. Flat structure is more
efficient to browse and work with. Moreover, by prepending date,
all folders are neatly sorted. Events in the same month come to
close by default, both in file explorer and terminal output of <code>ls</code>.
Adding intermediate folders is meaningless.</li>

<li><i>Archive automatically.</i> Thanks to the nature of self-containing,
moving old event folders to other place does not influence my
workflow.  In parctice, most event folders are rarely needed after
a short period of time. Though from time to time I may want to
review what I have done in the past month, I rarely visit an event
folder created years ago. Even when I want to visit, I typically do
not want to change the content. This fact make it very convenient
to archive event folders and backup them. At any time, the number
of event folders I need pay attention to is generally not larger
than 20.</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-orgdbda09c" class="outline-2">
<h2 id="orgdbda09c">Further discussion on the event-oriented approach</h2>
<div class="outline-text-2" id="text-orgdbda09c">
<p>
Now I summarize some properties of an event folder should have.
</p>

<ol class="org-ol">
<li>Its name starts with a date string in the format <code>YYYY-mm-dd-</code> and
ends with the event name.</li>

<li>It should be self-contained and include necessary staffs for
working on.</li>
</ol>

<p>
Below I want to add two more properties:
</p>

<ol class="org-ol">
<li value="3">It should occupied less than 500MB disk space.</li>
<li>All files with base name <code>notes</code> are reserved for storing metadata of
the event. (This rule does not apply to subfolders in the event
directory.)</li>
</ol>
</div>
<div id="outline-container-orgb8ace20" class="outline-3">
<h3 id="orgb8ace20">Share assets between events</h3>
<div class="outline-text-3" id="text-orgb8ace20">
<p>
The second property is crucial but sometimes can be troublesome.  For
example, if an event involves working with a lot of large immutable
assets, like a lot of data files or a lot of pictures, the event
folder might grow too large, say larger than 4GB.  In addition, if
there is another event involves working with the same assets, copying
them to the new folder does not seem to be a good idea.
</p>

<p>
My resolution is creating another top-level folder, say <code>~/Assets/</code>,
which acts like a repo for large files. For example, if an event
involves accessing to the famous MNIST dataset, I can move the dataset
to folder <code>~/Assets/MNIST/</code> and leave a symbolic link in the event
folder. The folder <code>~/Assets/</code> is also a good place to store data
outputs, like model weights of neural networks.
</p>

<p>
The folder <code>~/Assets/</code> is synced across machines.  In order to avoid
name conflict, I often add the same date prefix when allocating new
asset folders.
</p>
</div>
</div>
<div id="outline-container-org2bcaae5" class="outline-3">
<h3 id="org2bcaae5">Write a descriptive journal for each event</h3>
<div class="outline-text-3" id="text-org2bcaae5">
<p>
I always create a <code>notes.org</code> in each event folder, which serves like a
private README and journal for this event whose audience is future
myself.
</p>

<p>
Generally, I add meta data of the event in the front matter,including
<i>TITLE</i> and <i>DATE</i>. In <a href="../2024-01-14-TryOrgPublish/notes.html">this post</a>, I introduced how I use <code>org-publish</code> to
generate a sitemap of all events based on those notes files. In the
near future, I may add the <i>KEYWORDS</i> field for searching. The body may
contain journal of working on the event, links to useful resources and
anything I want to write down. In general, this file can possibly
contain descriptions to
</p>

<ol class="org-ol">
<li>metadata of the event, like tags, title, date and so on;</li>
<li>purpose and state of the event, like in what circumstance I create it and what is going on;</li>
<li>git repos related to the event;</li>
<li>notes/blogs related to the event;</li>
<li>papers/books related to the event;</li>
<li>assets related to the event, like resources, large files and so on;</li>
<li>file/folder structure of the event; represented as org entries, possibly tagged;</li>
</ol>

<p>
Different from the README file of a git repo, <code>notes.org</code> is always
private and never gets public. If I want to publish some content of it
to my blog, I just create a new post, cut and paste from it and leave
a link in the notes which looks like <i>see my post here</i>.
</p>
</div>
</div>
<div id="outline-container-orge18f22c" class="outline-3">
<h3 id="orge18f22c">Generate an index page listing all events</h3>
<div class="outline-text-3" id="text-orge18f22c">
<p>
One advantage of adopting the event-oriented framework is the
convenient reviewing experience. As all event directories follows a
particular naming pattern and has a descriptive journal <code>notes.org</code>, it
is not hard to recognize all these journals and create an index page;
see the figure.
</p>


<figure id="orge7a983e">
<img src="./demo-index.png" alt="demo-index.png">

</figure>

<p>
Clicking an event in this index page leads to its journal <code>notes.html</code>,
which can give me a quick review about what I have done.
</p>

<p>
See my post <a href="https://dou-meishi.github.io/org-blog/2024-01-14-TryOrgPublish/notes.html">Review org-publish Utility</a> for how I create this index
page via emacs org-publish utility.
</p>
</div>
</div>
<div id="outline-container-orgfadfa26" class="outline-3">
<h3 id="orgfadfa26">How to transform a folder to an event folder</h3>
<div class="outline-text-3" id="text-orgfadfa26">
<p>
Given an existed folder <code>dirname/</code>, I go through these steps to transform
it into an event folder.
</p>

<ol class="org-ol">
<li>Normalize its name to ensure it matches the format <code>YYYY-mm-dd-EventName</code>.
Here the date may be inferred from the folder content.</li>

<li><p>
Normalize the journal file <code>notes.org</code>. Ensure there are two metadata
entry <code>#+TITLE</code> and <code>#+DATE</code>. The latter is recommended to be
consistent with the folder name, but not strictly required.
</p>

<p>
In addition, check the content of <code>notes.org</code>. Ensure it can remind
me of the purpose of this folder.
</p></li>

<li>Normalize the folder size to be smaller than 500MB.  If not,
reorganize files inside this folder and move large assets to
<code>~/Assets/</code>.</li>
</ol>
</div>
</div>
<div id="outline-container-org90375f9" class="outline-3">
<h3 id="org90375f9">Tips</h3>
<div class="outline-text-3" id="text-org90375f9">
<ol class="org-ol">
<li><p>
This approach may not be suitable to organize context-free assets.
</p>

<p>
However, for me, most assets have context. For example, books on probability theory
are most refered in writing notes of the subject. So these books are placed in the
same event directory as these notes.
</p></li>

<li><p>
Create a new event and refer to the old event, instead of enlarge the old event folder.
</p>

<p>
Remember to briefly conclude what you obtained from the old event.
</p></li>
</ol>
</div>
</div>
</div>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-think.html">think</a> </div>
]]></description>
  <category><![CDATA[think]]></category>
  <link>https://dou-meishi.github.io/org-blog/2023-09-16-UltimateFileManagement/events-oriented-file-storage-framework.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2023-09-16-UltimateFileManagement/events-oriented-file-storage-framework.html</guid>
  <pubDate>Wed, 31 Jan 2024 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Org Static Blog: A Simple Static Site Generator]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org3672fc3">Go through the manual</a></li>
<li><a href="#org9d35bb7">Minimal Configuration</a></li>
<li><a href="#org3917ddd">Question</a></li>
<li><a href="#orgc90aa40">Build My Blog Site</a></li>
<li><a href="#orga8b39dd">External Links&#xa0;&#xa0;&#xa0;<span class="tag"><span class="refs">refs</span></span></a></li>
</ul>
</div>
</nav>
<p>
I picked up most of my computer skills from reading posts online, and
it got me thinking about having my own blog where I can share what
I've learned. But I've always been a bit hesitant about learning all
the complicated stuff in web development.  Recently, I came across a
third-party emacs package called <code>org-static-blog</code> while searching for
the built-in functionality <code>org-publish</code>.  After browsing a few blog
sites created with this package, I was impressed by its simplicity and
the ability to blog using org files. It became clear that this package
is exactly what I need at this stage.
</p>

<p>
After a few minites of survey, I observed the following facts.
</p>

<ol class="org-ol">
<li>The source code is simple enough. There is only a single elisp
script with less than 1000 lines.  The code is well-organized, with
customization variables appearing first, followed by commands. It
doesn't involve any complex or cryptic syntax.</li>

<li>The developer maintains this project over 8 years and he continues to use it in his blog site.</li>

<li>There are a few appealing blog sites are created by org static blog,
e.g.,  <a href="https://bastibe.de/about.html">Bastian</a>,  <a href="https://jao.io/blog/about.html">Jose Antonio Ortega Ruiz</a>,  <a href="https://justin.abrah.ms/">Justin Abrahms</a>, <a href="http://alhassy.com/about">Musa Al-hassy</a>&#x2026;</li>
</ol>

<p>
Finally, I decide to give it a try. I hope it can give me a way to
share my thoughts on the internet.
</p>
<div id="outline-container-org3672fc3" class="outline-2">
<h2 id="org3672fc3">Go through the manual</h2>
<div class="outline-text-2" id="text-org3672fc3">
<p>
The <a href="https://github.com/bastibe/org-static-blog/blob/master/README.org">README</a> contains a brief introduction to the usage and a minimal working example.
</p>

<p>
First, let us review the introduction.
</p>

<blockquote>
<p>
Static blog generators are a dime a dozen. This is one more, which focuses on being simple. All files are simple org-mode files in a directory. The only requirement is that every org file must have a <code>#+TITLE</code> and a <code>#+DATE</code>, and optionally, <code>#+FILETAGS</code>, <code>#+DESCRIPTION</code> and <code>#+IMAGE</code>.
</p>
</blockquote>

<p>
After collecting the appropriate org files, publishing them by org
static blog needs only two steps: 1) customizing org static blog
parameters; 2) calling <code>org-static-blog-publish</code> to publish all posts,
or calling <code>org-static-blog-publish-filename</code> to publish only a
particular post.
</p>

<p>
The blog site generated by org static blog consists of six parts.
</p>

<ol class="org-ol">
<li><i>Posts.</i> Every org file in <code>org-static-blog-posts-directory</code> is one blog post.
Each blog post is rendered as its own HTML page.</li>

<li><i>Index.</i> The index page contains the last few blog posts on a single page.
The number of entries on  the index page can be customized
using <code>org-static-blog-index-length</code>.</li>

<li><i>Tags.</i> Each blog post can be tagged, and each tag links to a page
that lists all other posts of the same tag. This feature is only
enabled when <code>org-static-blog-enable-tags</code> is <code>t</code>.</li>

<li><i>Archives.</i> This page lists the publishing dates and headlines of every blog post.</li>

<li><i>RSS Feeds.</i> This is a machine-readable XML file that contains every blog
post. It is not meant to be consumed by humans. Instead RSS readers
can use the RSS feed to aggregate entries from multiple blogs.</li>

<li><i>Drafts.</i> They are rendered like regular blog posts, but are not included
in the index, the archive, or the RSS feed.</li>
</ol>

<p>
Every HTML page in org static blog can be customized in the following aspects.
</p>

<ol class="org-ol">
<li><code>org-static-blog-page-header</code>. This is inserted into the <code>&lt;head&gt;</code> of every page.
Use this to include custom CSS and JavaScript for your blog.</li>

<li><code>org-static-blog-page-preamble</code>. This is inserted just before the
content of every page. This is a good place to put the header or
menus for your blog.</li>

<li><code>org-static-blog-page-postamble</code>. This is inserted after the content
of every generated page: after any blog post page, after the index
page, the tag pages and the archive. This is where you can include
copyright notices.</li>

<li><code>org-static-blog-post-preamble</code> and
<code>org-static-blog-post-postamble</code>. The returned values are prepended
and appended to every blog post. If you want to change the
formatting of dates, titles, or the tag list, overwrite these
functions. In particular the content of
<code>org-static-blog-post-comments</code> is inserted at the end of each blog
post. Use this to add a comment box.</li>
</ol>

<p>
Other features:
</p>

<ol class="org-ol">
<li>Optionally show a preview of the post (instead of the full post) on
the index page setting <code>org-static-blog-use-preview</code> to <code>t</code>. The region
of the post used as a preview is, by default, its first paragraph,
but can be fine-tuned using <code>org-static-blog-preview-start</code> and
<code>org-static-blog-preview-end</code>.</li>

<li><p>
Activate a few convenience key bindings by
</p>

<div class="org-src-container">
<pre class="src src-elisp">(add-to-list 'auto-mode-alist
             (cons (concat
                    org-static-blog-posts-directory <span style="color: #2aa198;">".*\\.org\\'"</span>)
                   'org-static-blog-mode))
</pre>
</div>

<p>
These key bindings are:
</p>

<ol class="org-ol">
<li><code>C-c C-f</code> / <code>C-c C-b</code> to open next/previous post.</li>
<li><code>C-c C-p</code> to open the matching published HTML file of a post.</li>
<li><code>C-c C-n</code> to create a new blog post.</li>
</ol></li>
</ol>
</div>
</div>
<div id="outline-container-org9d35bb7" class="outline-2">
<h2 id="org9d35bb7">Minimal Configuration</h2>
<div class="outline-text-2" id="text-org9d35bb7">
<p>
Following the example in its manual, I try to build a blog site at
<code>./org-blog/</code> with a folder <code>./org-blog/org/</code> containing several org
files.
</p>

<p>
The first step is, of course, installing the package.
</p>

<div class="org-src-container">
<pre class="src src-elisp">(package-install 'org-static-blog)
</pre>
</div>

<p>
The next step is setting up variables. I want to do the following things.
</p>

<ol class="org-ol">
<li>Set the site title to <i>Hello, Org Static Blog</i></li>
<li>Set the url of the site to <code>file:///home/dou/Documents/2024-01-22-TryOrgStaticBlog/org-blog/</code></li>
<li>Set the directory that holds all html files to <code>./org-blog/</code></li>
<li>Set the directory that holds all org files to be published to  <code>./org-blog/org/</code></li>
<li>Set the directory that holds all drafts to  <code>./org-blog/drafts/</code></li>
<li>Set the page header as the content of the file <code>./org-blog/static/header.html</code></li>
<li>Set the page preamble as the content of the file <code>./org-blog/static/preamble.html</code></li>
<li>Set the page postamble as the content of the file <code>./org-blog/static/postamble.html</code></li>
<li>Enable tags</li>
<li>Enable preview</li>
</ol>

<div class="org-src-container">
<pre class="src src-elisp">(<span style="color: #859900; font-weight: bold;">require</span> '<span style="color: #268bd2; font-weight: bold;">org-static-blog</span>)

(<span style="color: #859900; font-weight: bold;">setq</span> dms/org-static-blog-root-dir <span style="color: #2aa198;">"/home/dou/Documents/2024-01-22-TryOrgStaticBlog/org-blog/"</span>)

(<span style="color: #859900; font-weight: bold;">setq</span> org-static-blog-publish-title <span style="color: #2aa198;">"Hello, Org Static Blog"</span>)
(<span style="color: #859900; font-weight: bold;">setq</span> org-static-blog-publish-url (format <span style="color: #2aa198;">"file://%s"</span> dms/org-static-blog-root-dir))
(<span style="color: #859900; font-weight: bold;">setq</span> org-static-blog-publish-directory (format <span style="color: #2aa198;">"%s"</span> dms/org-static-blog-root-dir))
(<span style="color: #859900; font-weight: bold;">setq</span> org-static-blog-posts-directory (format <span style="color: #2aa198;">"%sorg"</span> dms/org-static-blog-root-dir))
(<span style="color: #859900; font-weight: bold;">setq</span> org-static-blog-drafts-directory (format <span style="color: #2aa198;">"%sdrafts"</span> dms/org-static-blog-root-dir))
(<span style="color: #859900; font-weight: bold;">setq</span> org-static-blog-page-header (<span style="color: #859900; font-weight: bold;">with-temp-buffer</span>
  (insert-file-contents (format <span style="color: #2aa198;">"%sstatic/header.html"</span> dms/org-static-blog-root-dir))
  (buffer-string)))
(<span style="color: #859900; font-weight: bold;">setq</span> org-static-blog-page-preamble (<span style="color: #859900; font-weight: bold;">with-temp-buffer</span>
  (insert-file-contents (format <span style="color: #2aa198;">"%sstatic/preamble.html"</span> dms/org-static-blog-root-dir))
  (buffer-string)))
(<span style="color: #859900; font-weight: bold;">setq</span> org-static-blog-page-postamble (<span style="color: #859900; font-weight: bold;">with-temp-buffer</span>
  (insert-file-contents (format <span style="color: #2aa198;">"%sstatic/postamble.html"</span> dms/org-static-blog-root-dir))
  (buffer-string)))
(<span style="color: #859900; font-weight: bold;">setq</span> org-static-blog-enable-tags t)
(<span style="color: #859900; font-weight: bold;">setq</span> org-static-blog-use-preview t)
</pre>
</div>

<p>
Contents of <code>header.html</code>, <code>preamble.html</code> and <code>postamble.html</code> are given as follows.
Assets <code>static/style.css</code> and <code>static/favicon.ico</code> are downloaded from <a href="https://github.com/bastibe/bastibe.github.com/tree/master/static">Bastian</a>.
</p>

<div class="org-src-container">
<pre class="src src-html">&lt;<span style="color: #268bd2;">meta</span> <span style="color: #268bd2;">name</span>=<span style="color: #2aa198;">"author"</span> <span style="color: #268bd2;">content</span>=<span style="color: #2aa198;">"Dou Meishi"</span>&gt;
&lt;<span style="color: #268bd2;">meta</span> <span style="color: #268bd2;">name</span>=<span style="color: #2aa198;">"referrer"</span> <span style="color: #268bd2;">content</span>=<span style="color: #2aa198;">"no-referrer"</span>&gt;
&lt;<span style="color: #268bd2;">link</span> href= <span style="color: #2aa198;">"static/style.css"</span> <span style="color: #268bd2;">rel</span>=<span style="color: #2aa198;">"stylesheet"</span> <span style="color: #268bd2;">type</span>=<span style="color: #2aa198;">"text/css"</span> /&gt;
&lt;<span style="color: #268bd2;">link</span> <span style="color: #268bd2;">href</span>=<span style="color: #2aa198;">"static/favicon.ico"</span> <span style="color: #268bd2;">rel</span>=<span style="color: #2aa198;">"icon"</span> /&gt;
</pre>
</div>

<div class="org-src-container">
<pre class="src src-html">&lt;<span style="color: #268bd2;">div</span> <span style="color: #268bd2;">class</span>=<span style="color: #2aa198;">"header"</span>&gt;
  Hello, Org Static Blog
&lt;/<span style="color: #268bd2;">div</span>&gt;
</pre>
</div>

<div class="org-src-container">
<pre class="src src-html">Created by &lt;<span style="color: #268bd2;">a</span> <span style="color: #268bd2;">href</span>=<span style="color: #2aa198;">"https://github.com/bastibe/org-static-blog/"</span>&gt;Org Static Blog&lt;/<span style="color: #268bd2;">a</span>&gt;
</pre>
</div>

<p>
Currently, the project layout looks like
</p>

<pre class="example" id="orgefb6cca">
org-blog/
‚îú‚îÄ‚îÄ drafts
‚îú‚îÄ‚îÄ org
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ notes.org
‚îî‚îÄ‚îÄ static
    ‚îú‚îÄ‚îÄ favicon.ico
    ‚îú‚îÄ‚îÄ header.html
    ‚îú‚îÄ‚îÄ postamble.html
    ‚îú‚îÄ‚îÄ preamble.html
    ‚îî‚îÄ‚îÄ style.css

3 directories, 6 files
</pre>

<p>
Finally, call  <code>org-static-blog-publish</code> to generate the site. At this time, the project layout becomes
</p>

<pre class="example" id="org39009eb">
org-blog/
‚îú‚îÄ‚îÄ archive.html
‚îú‚îÄ‚îÄ drafts
‚îú‚îÄ‚îÄ index.html
‚îú‚îÄ‚îÄ notes.html
‚îú‚îÄ‚îÄ org
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ notes.org
‚îú‚îÄ‚îÄ rss.xml
‚îú‚îÄ‚îÄ static
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ favicon.ico
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ header.html
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ postamble.html
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ preamble.html
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ style.css
‚îî‚îÄ‚îÄ tags.html

3 directories, 11 files
</pre>
</div>
</div>
<div id="outline-container-org3917ddd" class="outline-2">
<h2 id="org3917ddd">Question</h2>
<div class="outline-text-2" id="text-org3917ddd">
<ol class="org-ol">
<li><p>
Does it scans all org files in <code>org-static-blog-posts-directory</code> recursively or not?
</p>

<p>
Yes. For example, a folder named <code>2024/</code> in it will be published to the folder <code>2024/</code> in
<code>org-static-blog-publish-directory</code>.
</p></li>

<li><p>
Does it support following symbolic links when checking org files?
</p>

<p>
Yes but no. I test the following based on the project layout in the <i>Minimal Configuration</i> section.
I renamed the ordinary file <code>notes.org</code> inside <code>org-blog/org/</code> to <code>org-blog/../</code> but leave
a symbolic link. So the project layout becomes.
</p>

<pre class="example" id="org6f6d278">
org-blog/
‚îú‚îÄ‚îÄ drafts
‚îú‚îÄ‚îÄ org
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ notes.org -&gt; /home/dou/Documents/2024-01-22-TryOrgStaticBlog/notes.org
‚îî‚îÄ‚îÄ static
    ‚îú‚îÄ‚îÄ favicon.ico
    ‚îú‚îÄ‚îÄ header.html
    ‚îú‚îÄ‚îÄ postamble.html
    ‚îú‚îÄ‚îÄ preamble.html
    ‚îî‚îÄ‚îÄ style.css

3 directories, 6 files
</pre>

<p>
However, calling <code>org-static-blog-pulish</code> exports <code>org-blog/org/notes.org</code> to <code>~/Documents/notes.html</code>.
</p>

<p>
This issue comes function <code>org-static-blog-get-post-public-path</code>. Running either
</p>

<div class="org-src-container">
<pre class="src src-elisp">(org-static-blog-get-post-public-path <span style="color: #2aa198;">"~/Documents/2024-01-22-TryOrgStaticBlog/notes.org"</span>)
</pre>
</div>

<p>
or
</p>

<div class="org-src-container">
<pre class="src src-elisp">(org-static-blog-get-post-public-path <span style="color: #2aa198;">"~/Documents/2024-01-22-TryOrgStaticBlog/org-blog/org/notes.org"</span>)
</pre>
</div>

<p>
gives the result <code>../../notes.html</code>.
</p></li>

<li><p>
What will happen if <code>org-static-blog-publish-directory</code> is the same
as <code>org-static-blog-posts-directory</code>?  In this case, what will happen
if <code>org-static-blog-drafts-directory</code> is a subfolder?
</p>

<p>
It behaves like exporting all org files to the same directory, and posts inside
the drafts directory will not be included in the index.
</p></li>
</ol>
</div>
</div>
<div id="outline-container-orgc90aa40" class="outline-2">
<h2 id="orgc90aa40">Build My Blog Site</h2>
<div class="outline-text-2" id="text-orgc90aa40">
<p>
It is also not hard to extend the minimal configuration to build a
real blog site.  But before tweaking these scripts and assets, I need,
of course, get a public URL for hosting the site. Fortunately, GitHub
Pages allows hosting directly from a GitHub repository. What I need to
do is create a publich repo, say <code>org-blog</code>, and go to <code>Settings -&gt; Pages</code>
and set the deploy target to <code>https://dou-meishi.github.io/blog/</code>.  Now
I can replace the previous local URL to this one in the script and
assets.
</p>

<p>
Besides changing the publish URL, there are a few other things to make
the site a slightly more visual appealing.
</p>

<ol class="org-ol">
<li><p>
Add a top bar at each page, showing links to the homepage and the archive page.
</p>

<p>
This can be easily done by modifying the <code>preamble.html</code>
</p>

<div class="org-src-container">
<pre class="src src-html">&lt;<span style="color: #268bd2;">div</span> <span style="color: #268bd2;">class</span>=<span style="color: #2aa198;">"header"</span>&gt;
  &lt;<span style="color: #268bd2;">div</span> <span style="color: #268bd2;">class</span>=<span style="color: #2aa198;">"sitelinks"</span>&gt;
    &lt;<span style="color: #268bd2;">a</span> <span style="color: #268bd2;">href</span>=<span style="color: #2aa198;">"https://dou-meishi.github.io/org-blog/index.html"</span>&gt;Home&lt;/<span style="color: #268bd2;">a</span>&gt;
    |
    &lt;<span style="color: #268bd2;">a</span> <span style="color: #268bd2;">href</span>=<span style="color: #2aa198;">"https://dou-meishi.github.io/org-blog/archive.html"</span>&gt;All Posts&lt;/<span style="color: #268bd2;">a</span>&gt;
  &lt;/<span style="color: #268bd2;">div</span>&gt;
&lt;/<span style="color: #268bd2;">div</span>&gt;
</pre>
</div></li>

<li><p>
Render math formulae. This can also be achieved by adding appropriate javascript
and stylesheets. Currently, I use <a href="https://katex.org/docs/browser">KaTeX</a>, which seems to be faster than MathJax.
</p>

<div class="org-src-container">
<pre class="src src-html"><span style="color: #93a1a1;">&lt;!-- </span><span style="color: #93a1a1;">Math Support by KaTeX</span><span style="color: #93a1a1;"> --&gt;</span>
&lt;<span style="color: #268bd2;">link</span> <span style="color: #268bd2;">rel</span>=<span style="color: #2aa198;">"stylesheet"</span> <span style="color: #268bd2;">href</span>=<span style="color: #2aa198;">"https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"</span> <span style="color: #268bd2;">integrity</span>=<span style="color: #2aa198;">"sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV"</span> <span style="color: #268bd2;">crossorigin</span>=<span style="color: #2aa198;">"anonymous"</span>&gt;
<span style="color: #93a1a1;">&lt;!-- </span><span style="color: #93a1a1;">The loading of KaTeX is deferred to speed up page rendering</span><span style="color: #93a1a1;"> --&gt;</span>
&lt;<span style="color: #268bd2;">script</span> defer <span style="color: #268bd2;">src</span>=<span style="color: #2aa198;">"https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"</span> <span style="color: #268bd2;">integrity</span>=<span style="color: #2aa198;">"sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8"</span> <span style="color: #268bd2;">crossorigin</span>=<span style="color: #2aa198;">"anonymous"</span>&gt;&lt;/<span style="color: #268bd2;">script</span>&gt;
<span style="color: #93a1a1;">&lt;!-- </span><span style="color: #93a1a1;">To automatically render math in text elements, include the auto-render extension:</span><span style="color: #93a1a1;"> --&gt;</span>
&lt;<span style="color: #268bd2;">script</span> defer <span style="color: #268bd2;">src</span>=<span style="color: #2aa198;">"https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"</span> <span style="color: #268bd2;">integrity</span>=<span style="color: #2aa198;">"sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05"</span> <span style="color: #268bd2;">crossorigin</span>=<span style="color: #2aa198;">"anonymous"</span> <span style="color: #268bd2;">onload</span>=<span style="color: #2aa198;">"renderMathInElement(document.body);"</span>&gt;&lt;/<span style="color: #268bd2;">script</span>&gt;
</pre>
</div></li>

<li><p>
Customize the top of the index page. This is done by setting the
<code>org-static-blog-index-front-matter</code> variable in the lisp script.
</p>

<div class="org-src-container">
<pre class="src src-elisp">(<span style="color: #859900; font-weight: bold;">setq</span> org-static-blog-index-front-matter
      <span style="color: #2aa198;">"&lt;h1 class=title&gt; Recent Posts &lt;/h1&gt;"</span>)
</pre>
</div></li>

<li><p>
Change the default ellipsis <code>(...)</code> used in preview to  <code>...</code>
</p>

<div class="org-src-container">
<pre class="src src-elisp">(<span style="color: #859900; font-weight: bold;">setq</span> org-static-blog-preview-ellipsis <span style="color: #2aa198;">"..."</span>)
</pre>
</div></li>

<li><p>
Use a different CSS stylesheet. I have been using <a href="http://gongzhitaao.org/orgcss">this stylesheet</a>
for three years and always appreciating its clean design. It is
designed for HTML exported from org files by <code>org-publish</code>. To adapt
it to files exported by <code>org-static-blog</code>, I add several additional
rules in <a href="https://dou-meishi.github.io/org-blog/static/dou-org-blog.css">a patched CSS</a>.
</p>

<div class="org-src-container">
<pre class="src src-html">&lt;<span style="color: #268bd2;">link</span> href= <span style="color: #2aa198;">"https://gongzhitaao.org/orgcss/org.css"</span> <span style="color: #268bd2;">rel</span>=<span style="color: #2aa198;">"stylesheet"</span> <span style="color: #268bd2;">type</span>=<span style="color: #2aa198;">"text/css"</span> /&gt;
&lt;<span style="color: #268bd2;">link</span> href= <span style="color: #2aa198;">"https://dou-meishi.github.io/org-blog/static/dou-org-blog.css"</span> <span style="color: #268bd2;">rel</span>=<span style="color: #2aa198;">"stylesheet"</span> <span style="color: #268bd2;">type</span>=<span style="color: #2aa198;">"text/css"</span> /&gt;
</pre>
</div></li>

<li><p>
Sync posts from my document folder. As I mentioned in <a href="../2024-01-14-TryOrgPublish/notes.html">the previous post</a>, my notes resides in different event directories
in the document folder, and, of course, I do not want to share the whole <code>~/Documents/</code> folder.
So I list all files I want to share in <code>~/.unison/syncpost.prf</code>, which looks like
</p>

<div class="org-src-container">
<pre class="src src-text">source default.prf

root = /home/dou/Documents
root = /home/dou/Documents/2024-01-24-MyOrgBlog/

nodeletion = /home/dou/Documents

path = 2023-09-19-Compactness/notes.org
path = 2023-10-23-BanachSpaceExample/notes.org
path = 2024-01-07-ReviewUnison/basics.org
path = 2024-01-07-ReviewUnison/advanced.org
path = 2024-01-11-CodeBlockinLaTeX/notes.org
path = 2024-01-11-CodeBlockinLaTeX/simple-code.png
path = 2024-01-11-CodeBlockinLaTeX/tcolorbox-listings.png
path = 2024-01-14-TryOrgPublish/notes.org
path = 2024-01-22-TryOrgStaticBlog/notes.org
</pre>
</div>

<p>
Whenever I want to post something, I just check this file, run <code>unison-gui syncpost</code>,
and execute all lisp script in the <code>build-blog.el</code>.
</p></li>
</ol>

<p>
The lisp script and static assets are all included in <a href="https://github.com/Dou-Meishi/org-blog">my git repo</a> for this blog.
</p>
</div>
</div>
<div id="outline-container-orga8b39dd" class="outline-2">
<h2 id="orga8b39dd">External Links&#xa0;&#xa0;&#xa0;<span class="tag"><span class="refs">refs</span></span></h2>
<div class="outline-text-2" id="text-orga8b39dd">
<p>
Below are other users' configuration on org static blog.
</p>

<ol class="org-ol">
<li>a simple setup: <a href="https://jao.io/blog/simplicity.html">simplicity - programming (and other) musings</a></li>
<li>another simple setup <a href="https://justin.abrah.ms/dotfiles/emacs.html#orgc787b23">Justin's emacs configuration - Writting - Blogging</a></li>
<li>an extensive setup: <a href="http://alhassy.com/AlBasmala#Why-not-use-an-existing-blogging-platform">AlBasmala: Blogging with Emacs &amp; Org-mode (‚Ä¢ÃÄ·¥ó‚Ä¢ÃÅ)Ÿà</a></li>
</ol>
</div>
</div>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-emacs.html">emacs</a> </div>
]]></description>
  <category><![CDATA[emacs]]></category>
  <link>https://dou-meishi.github.io/org-blog/2024-01-22-TryOrgStaticBlog/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2024-01-22-TryOrgStaticBlog/notes.html</guid>
  <pubDate>Mon, 22 Jan 2024 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Use YASnippet to Input Greek Characters]]></title>
  <description><![CDATA[
<p>
Sometimes, single characters like <code>Œ±</code> and <code>Œ≤</code> are more recognizable than words like <code>alpha</code> and <code>beta</code>.
I think it is particularly useful when writing formulae in LaTeX or other text environments.
There are many general ways to input unicode characters quickly. For instance,
most intelligent input methods will suggest Œ± as a candidate when you input alpha. In addition,
Emacs includes a built-in function <code>insert-char</code>, which is bound to <code>C-x 8 RET</code> by default.
<a href="https://github.com/joaotavora/yasnippet">YASnippet</a> is another way I think is more convenient for frequently used symbols.
</p>

<p>
For example, put the following content in the file <code>~/.emacs.d/snippets/text-mode/char-alpha</code>.
</p>

<div class="org-src-container">
<pre class="src src-text"># -*- mode: snippet -*-
# name: char-alpha
# key: \alpha
# --
&#945;
</pre>
</div>

<p>
Then, when you type <code>\alpha&lt;TAB&gt;</code>, the string expands to <code>Œ±</code>. Of course,
this only works in text mode and its derived mode. But you can easily
enable this feature in any mode, say <code>prog-mode</code>, by adding a line
</p>

<div class="org-src-container">
<pre class="src src-text">text-mode
</pre>
</div>

<p>
to the file <code>~/.emacs.d/snippets/prog-mode/.yas-parents</code>.  Remember to
remove the last newline character in the file <code>char-alpha</code>.  Otherwise,
it will expand to <code>\alpha&lt;NewLine&gt;</code>.
</p>

<p>
I create <a href="https://gist.github.com/Dou-Meishi/6db3e58138714198da7095a8f4ac82cd">a csv file</a> whose rows are greek letters and their name in latex, which looks like
</p>

<div class="org-src-container">
<pre class="src src-csv">Œ±, alpha
Œ≤, beta
Œ≥, gamma
Œ¥, delta
œµ, epsilon
Œµ, varepsilon
...
</pre>
</div>

<p>
Based on this list, the following simple Python script can produce all the required snippets for greek letters.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #268bd2;">filename</span> = <span style="color: #2aa198;">'greek.csv'</span>
<span style="color: #859900; font-weight: bold;">with</span> <span style="color: #657b83; font-weight: bold;">open</span>(filename, <span style="color: #2aa198;">'r'</span>) <span style="color: #859900; font-weight: bold;">as</span> fpr:
    <span style="color: #268bd2;">greeklines</span> = fpr.readlines()

<span style="color: #268bd2;">snippetTemplate</span> = r<span style="color: #2aa198;">"""# -*- mode: snippet -*-</span>
<span style="color: #2aa198;"># name: char-{name}</span>
<span style="color: #2aa198;"># key: \{name}</span>
<span style="color: #2aa198;"># --</span>
<span style="color: #2aa198;">{char}"""</span>

<span style="color: #268bd2;">charNameTuples</span> = [line.strip().split(<span style="color: #2aa198;">", "</span>) <span style="color: #859900; font-weight: bold;">for</span> line <span style="color: #859900; font-weight: bold;">in</span> greeklines]
<span style="color: #859900; font-weight: bold;">for</span> char, name <span style="color: #859900; font-weight: bold;">in</span> charNameTuples:
    <span style="color: #859900; font-weight: bold;">with</span> <span style="color: #657b83; font-weight: bold;">open</span>(f<span style="color: #2aa198;">"char-</span>{name}<span style="color: #2aa198;">"</span>, <span style="color: #2aa198;">"w"</span>, encoding=<span style="color: #2aa198;">"utf8"</span>) <span style="color: #859900; font-weight: bold;">as</span> fpr:
        fpr.write(snippetTemplate.<span style="color: #657b83; font-weight: bold;">format</span>(name=name, char=char))
</pre>
</div>

<p>
Of couse, this method can be used to input any unicode letters, not
limited to greek letters.
</p>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-emacs.html">emacs</a> </div>
]]></description>
  <category><![CDATA[emacs]]></category>
  <link>https://dou-meishi.github.io/org-blog/2024-01-18-YasforGreekLetters/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2024-01-18-YasforGreekLetters/notes.html</guid>
  <pubDate>Thu, 18 Jan 2024 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Review org-publish Utility]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgbc38b1e">Go through the manual</a></li>
<li><a href="#orgce53576">Practice</a></li>
<li><a href="#org8d21373">Questions</a></li>
<li><a href="#org4476275">Further consideration</a></li>
<li><a href="#org808d7fc">External Links&#xa0;&#xa0;&#xa0;<span class="tag"><span class="refs">refs</span></span></a></li>
</ul>
</div>
</nav>
<p>
Recently, I finally decided to create my personal blog site. I
researched a few tools and then I suddenly remembered the <code>org-publish</code>
function in Emacs. Although I have been using Emacs and Org mode for
three years, I never wrote a script in elisp. But since my interest in
Emacs is continuously growing, I think it's a good time to play around
with it. It is also a good practice for me to understand the source code
of <a href="https://github.com/bastibe/org-static-blog/tree/master">Org Static Blog</a>, which is the actual blogging tool I want to use.
</p>
<div id="outline-container-orgbc38b1e" class="outline-2">
<h2 id="orgbc38b1e">Go through the manual</h2>
<div class="outline-text-2" id="text-orgbc38b1e">
<p>
According to <a href="https://orgmode.org/manual/Publishing.html">the manual</a>, publishing in org-mode is configured almost
entirely through setting the value of one variable, called
<code>org-publish-project-alist</code>. Each element of the list configures one
project, and may be in one of the two following forms:
</p>

<ol class="org-ol">
<li><code>("project-name" :property value :property value ...)</code></li>
<li><code>("project-name" :components ("project-name" "project-name" ...))</code></li>
</ol>

<p>
After properly configuring the variable, calling <code>org-publish</code> will prompt for a project name
and publish all files that belong to it. Calling <code>org-publish-all</code> will publish all projects.
</p>

<p>
Publishing means that a file is copied to the destination directory and possibly transformed in the process.
The transformation is controlled by the property <code>publishing-function</code>. Typical values include
</p>

<ol class="org-ol">
<li><code>org-html-publish-to-html</code>, which calls the HTML exporter to export org files to HTML files;</li>
<li><code>org-publish-attachment</code>, which does not modify files but simply copy them.</li>
</ol>

<p>
We may also generate a sitemap for a given project by customizing following properties;
see <a href="https://orgmode.org/manual/Site-map.html">Section 14.1.7</a> in the org manual. Interesting properties include:
</p>

<ol class="org-ol">
<li><code>sitemap-format-entry</code>: tell how a published entry is formatted in the sitemap;</li>
<li><code>sitemap-sort-folders</code>: where folders should appear in the sitemap;</li>
<li><code>sitemap-sort-files</code>: how the files are sorted in the sitemap.</li>
</ol>
</div>
</div>
<div id="outline-container-orgce53576" class="outline-2">
<h2 id="orgce53576">Practice</h2>
<div class="outline-text-2" id="text-orgce53576">
<p>
<i>A simple setting:</i> given a folder <code>./content</code> with several org files in it, we want to publish them
into a different folder <code>./public</code>. Assets should be copied too.
</p>

<blockquote>
<p>
It is convenient to put publishing related source in a standalone <code>build.el</code> file.
Visit it in Emacs and call <code>eval-buffer</code> to publish projects defined it.
</p>
</blockquote>

<p>
First, we define our <code>sitemap-format-entry</code> function, which will format an entry into
a timestamp followed by a URL whose description is the title of the entry.
</p>

<div class="org-src-container">
<pre class="src src-elisp">(<span style="color: #859900; font-weight: bold;">defun</span> <span style="color: #268bd2;">dms/org-sitemap-format-entry</span> (entry style project)
  <span style="color: #2aa198;">"Format ENTRY in org-publish PROJECT Sitemap as [date] [[file][title]]."</span>
  (<span style="color: #859900; font-weight: bold;">let</span> ((filetitle (org-publish-find-title entry project)))
    (<span style="color: #859900; font-weight: bold;">if</span> (= (length filetitle) 0)
        (format <span style="color: #2aa198;">"*%s*"</span> entry)
      (format <span style="color: #2aa198;">"[%s] [[file:%s][%s]]"</span>
              (format-time-string <span style="color: #2aa198;">"%Y-%m-%d"</span>
                                  (org-publish-find-date entry project))
              entry
              filetitle))))
</pre>
</div>

<p>
Then, we set <code>org-publish-project-alist</code>. We create two projects, one for exporting org files
and other one for copying assets. Both projects recursively search files based on a particular REGEXP on
file extension. In addition, we require to generate a sitemap and format entries by our
<code>dms/org-sitemap-format-entry</code> function. In addition, entries  are sorted by date
and organized as a plain list, instead of nested list containing subfolders.
</p>

<div class="org-src-container">
<pre class="src src-elisp"><span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">Define the publishing project</span>
(<span style="color: #859900; font-weight: bold;">setq</span> org-publish-project-alist
      (list
       (list <span style="color: #2aa198;">"try-org-publish-org"</span>
             <span style="color: #657b83; font-weight: bold;">:base-directory</span> <span style="color: #2aa198;">"./content"</span>
             <span style="color: #657b83; font-weight: bold;">:base-extension</span> <span style="color: #2aa198;">"org"</span>
             <span style="color: #657b83; font-weight: bold;">:publishing-directory</span> <span style="color: #2aa198;">"./public"</span>
             <span style="color: #657b83; font-weight: bold;">:publishing-function</span> 'org-html-publish-to-html
             <span style="color: #657b83; font-weight: bold;">:recursive</span> t
             <span style="color: #657b83; font-weight: bold;">:auto-sitemap</span> t
             <span style="color: #657b83; font-weight: bold;">:sitemap-title</span> <span style="color: #2aa198;">"Doumeishi's Mainpage"</span>
             <span style="color: #657b83; font-weight: bold;">:sitemap-format-entry</span> 'dms/org-sitemap-format-entry
             <span style="color: #657b83; font-weight: bold;">:sitemap-sort-files</span> 'anti-chronologically
             <span style="color: #657b83; font-weight: bold;">:sitemap-style</span> 'list
             )
       (list <span style="color: #2aa198;">"try-org-publish-assets"</span>
             <span style="color: #657b83; font-weight: bold;">:base-directory</span> <span style="color: #2aa198;">"./content"</span>
             <span style="color: #657b83; font-weight: bold;">:base-extension</span> <span style="color: #2aa198;">"css</span><span style="color: #859900; font-weight: bold;">\\</span><span style="color: #b58900; font-weight: bold;">|</span><span style="color: #2aa198;">js</span><span style="color: #859900; font-weight: bold;">\\</span><span style="color: #b58900; font-weight: bold;">|</span><span style="color: #2aa198;">png</span><span style="color: #859900; font-weight: bold;">\\</span><span style="color: #b58900; font-weight: bold;">|</span><span style="color: #2aa198;">jpg</span><span style="color: #859900; font-weight: bold;">\\</span><span style="color: #b58900; font-weight: bold;">|</span><span style="color: #2aa198;">gif</span><span style="color: #859900; font-weight: bold;">\\</span><span style="color: #b58900; font-weight: bold;">|</span><span style="color: #2aa198;">pdf</span><span style="color: #859900; font-weight: bold;">\\</span><span style="color: #b58900; font-weight: bold;">|</span><span style="color: #2aa198;">mp3</span><span style="color: #859900; font-weight: bold;">\\</span><span style="color: #b58900; font-weight: bold;">|</span><span style="color: #2aa198;">ogg</span><span style="color: #859900; font-weight: bold;">\\</span><span style="color: #b58900; font-weight: bold;">|</span><span style="color: #2aa198;">swf</span><span style="color: #859900; font-weight: bold;">\\</span><span style="color: #b58900; font-weight: bold;">|</span><span style="color: #2aa198;">mov"</span>
             <span style="color: #657b83; font-weight: bold;">:publishing-directory</span> <span style="color: #2aa198;">"./public"</span>
             <span style="color: #657b83; font-weight: bold;">:publishing-function</span> 'org-publish-attachment
             <span style="color: #657b83; font-weight: bold;">:recursive</span> t
             )
       )
)
</pre>
</div>

<p>
Finally, we publish all projects.
</p>

<div class="org-src-container">
<pre class="src src-elisp"><span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">Generate the site output</span>
(org-publish-all t)

(message <span style="color: #2aa198;">"Publish complete!"</span>)
</pre>
</div>
</div>
</div>
<div id="outline-container-org8d21373" class="outline-2">
<h2 id="org8d21373">Questions</h2>
<div class="outline-text-2" id="text-org8d21373">
<ol class="org-ol">
<li><p>
Can I customize the way of Emacs searching for intended org files rather than
a base dir + extension?
</p>

<p>
Yes, we can first exclude all files by setting the base extension to <code>"dummy"</code> and then use <code>:include</code>
to include a list of files we want to publish.
</p></li>

<li><p>
Aware of privacy, can I customize the exporting scheme to exclude publishing particular files?
</p>

<p>
Yes, we can set the <code>exclude</code> property. Or we can set the <code>:exclude-tags</code> property.
</p></li>

<li><p>
Can I adjust publication settings for particular subfolders?
</p>

<p>
Yes, we can exclude the subfolder from existing projects, then create a new project for it
and apply different rules for this subfolder.
</p></li>

<li><p>
How the <i>last modified</i> time is set? I want it to be set by the mtime of org files.
</p>

<p>
I am not sure about this. With some test I found that if I run the script in Emacs
then everything work as expected. But if I run the script in terminal by
<code>emacs -Q --script</code> then every exported file will update the modification time
to the current time.
</p></li>
</ol>
</div>
</div>
<div id="outline-container-org4476275" class="outline-2">
<h2 id="org4476275">Further consideration</h2>
<div class="outline-text-2" id="text-org4476275">
<p>
<i>A slightly complicated setting:</i> my document folder consists of event directories and looks like
</p>

<pre class="example" id="orgfb3636c">
.
‚îú‚îÄ‚îÄ 2023-09-03-CustomizePrompt/
‚îú‚îÄ‚îÄ 2023-11-18-ContentManagementSystem/
‚îú‚îÄ‚îÄ 2024-01-03-ReviewPham/
‚îú‚îÄ‚îÄ 2024-01-07-ReviewUnison/
‚îú‚îÄ‚îÄ 2024-01-11-CodeBlockinLaTeX/
</pre>

<p>
In each event directory, there is an org file <code>notes.org</code> which contains my notes on this event.
I want to generate a sitemap for my document folder (or some folder with the same
strcture) such that I can review what I have done in browser. In particular, I want to
publish only those event notes, i.e., no other org files are exported during the creation of
my sitemap. Moreover, I want to publish those notes in-place, i.e., the generated html should
be placed in the its event directory.
</p>

<p>
In order to do this, we can first define two variables. One is the root directory to be considered,
and is set to <code>~/Document</code>. The other one is a textual file, in which every line specifies a event
name that should not be published.
</p>

<div class="org-src-container">
<pre class="src src-elisp">(<span style="color: #859900; font-weight: bold;">defcustom</span> <span style="color: #268bd2;">dms/org-publish-event-root-dir</span> <span style="color: #2aa198;">"~/Documents"</span>
  <span style="color: #2aa198;">"The directory contains a list of event directories."</span>)

(<span style="color: #859900; font-weight: bold;">defcustom</span> <span style="color: #268bd2;">dms/org-publish-nopublish-events-fp</span> <span style="color: #2aa198;">"~/org/nopublish-events.txt"</span>
  <span style="color: #2aa198;">"The file path whose content is a list of event names</span>
<span style="color: #2aa198;">which should not be considered when do publishing.</span>
<span style="color: #2aa198;">This file should be a textual file and each line corresponds to</span>
<span style="color: #2aa198;">an event name."</span>)
</pre>
</div>

<p>
Then we define a function to generate the list of event notes to be published.
In this function I first filtered the event directory under the root folder with
the content of that nopublish file, then I concat the filename <code>notes.org</code>
for each event and check the existence of such file.
</p>

<div class="org-src-container">
<pre class="src src-elisp">(<span style="color: #859900; font-weight: bold;">defun</span> <span style="color: #268bd2;">dms/org-publish-get-event-notes</span> ()
  <span style="color: #2aa198;">"Return a list of event notes to be published according to the value</span>
<span style="color: #2aa198;">of dms/org-publish-event-root-dir and dms/org-publish-nopublish-events-fp.</span>

<span style="color: #2aa198;">An event is a directory whose name has the format YYYY-MM-DD-EventName.</span>
<span style="color: #2aa198;">A event note is the file named notes.org under the event directory."</span>
  (<span style="color: #859900; font-weight: bold;">let*</span> ((events (directory-files dms/org-publish-event-root-dir nil
                    <span style="color: #2aa198;">"^[0-9]\\{</span><span style="color: #268bd2;">4\\</span><span style="color: #2aa198;">}-[0-9]\\{</span><span style="color: #268bd2;">2\\</span><span style="color: #2aa198;">}-[0-9]\\{</span><span style="color: #268bd2;">2\\</span><span style="color: #2aa198;">}-.+"</span>))
         (nopublish-event-alist
          (<span style="color: #859900; font-weight: bold;">if</span> dms/org-publish-nopublish-events-fp
              (<span style="color: #859900; font-weight: bold;">with-temp-buffer</span>
                (insert-file-contents dms/org-publish-nopublish-events-fp)
                (split-string (buffer-string) <span style="color: #2aa198;">"\n"</span> t))))
         (filtered-events (seq-difference events nopublish-event-alist))
         (event-notes-to-publish
          (mapcar (<span style="color: #859900; font-weight: bold;">lambda</span> (event) (concat
                                   (file-name-as-directory event)
                                   <span style="color: #2aa198;">"notes.org"</span>)) <span style="color: #cb4b16; font-weight: bold;">filtered-events)))</span>
    (seq-filter (<span style="color: #859900; font-weight: bold;">lambda</span> (event-note)
                  (file-exists-p (concat (file-name-as-directory
                                          dms/org-publish-event-root-dir)
                                         event-note)))
                event-notes-to-publish)))
</pre>
</div>

<p>
After that we define the way to format the event note in the sitemap, i.e.,
formatting as <code>=date= [[path][title]]</code>.
</p>

<div class="org-src-container">
<pre class="src src-elisp">(<span style="color: #859900; font-weight: bold;">defun</span> <span style="color: #268bd2;">dms/org-sitemap-format-event-note-entry</span> (entry style project)
  <span style="color: #2aa198;">"Format an event note ENTRY in org-publish PROJECT Sitemap as</span>
<span style="color: #2aa198;">=date= [[file][title]]."</span>
  (<span style="color: #859900; font-weight: bold;">let</span> ((filetitle (org-publish-find-title entry project)))
    (<span style="color: #859900; font-weight: bold;">if</span> (= (length filetitle) 0)
        (format <span style="color: #2aa198;">"*%s*"</span> entry)
      (format <span style="color: #2aa198;">"=%s= [[file:%s][%s]]"</span>
              (format-time-string <span style="color: #2aa198;">"%Y-%m-%d"</span>
                                  (org-publish-find-date entry project))
              entry
              filetitle))))
</pre>
</div>

<p>
Finally, we set up the project alist variable and publish. By the way, we can always check the returned value
of <code>dms/org-publish-get-event-notes</code> to see the list of files to be published.
</p>

<div class="org-src-container">
<pre class="src src-elisp"><span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">Define the publishing project</span>
(<span style="color: #859900; font-weight: bold;">setq</span> org-publish-project-alist
      (list
       (list <span style="color: #2aa198;">"event-notes"</span>
             <span style="color: #657b83; font-weight: bold;">:base-directory</span> dms/org-publish-event-root-dir
             <span style="color: #657b83; font-weight: bold;">:base-extension</span> <span style="color: #2aa198;">"dummy"</span>
             <span style="color: #657b83; font-weight: bold;">:include</span> (dms/org-publish-get-event-notes)
             <span style="color: #657b83; font-weight: bold;">:publishing-directory</span> dms/org-publish-event-root-dir
             <span style="color: #657b83; font-weight: bold;">:publishing-function</span> 'org-html-publish-to-html
             <span style="color: #657b83; font-weight: bold;">:recursive</span> nil
             <span style="color: #657b83; font-weight: bold;">:auto-sitemap</span> t
             <span style="color: #657b83; font-weight: bold;">:sitemap-title</span> <span style="color: #2aa198;">"Event Notes"</span>
             <span style="color: #657b83; font-weight: bold;">:sitemap-filename</span> <span style="color: #2aa198;">"index.org"</span>
             <span style="color: #657b83; font-weight: bold;">:sitemap-format-entry</span> 'dms/org-sitemap-format-event-note-entry
             <span style="color: #657b83; font-weight: bold;">:sitemap-sort-files</span> 'anti-chronologically
             <span style="color: #657b83; font-weight: bold;">:sitemap-style</span> 'list
             )))

<span style="color: #93a1a1;">;; </span><span style="color: #93a1a1;">Generate the site output</span>
(org-publish-all t)

(message <span style="color: #2aa198;">"Publish complete!"</span>)
</pre>
</div>

<p>
We can place this script in our <code>.emacs.d/</code> directory.
Whenever we want to rebuild the index page of the document folder,
simply visit it and run <code>eval-buffer</code>.
</p>
</div>
</div>
<div id="outline-container-org808d7fc" class="outline-2">
<h2 id="org808d7fc">External Links&#xa0;&#xa0;&#xa0;<span class="tag"><span class="refs">refs</span></span></h2>
<div class="outline-text-2" id="text-org808d7fc">
<ol class="org-ol">
<li><a href="https://orgmode.org/manual/Publishing.html">Publishing - The Org Mode Manual</a></li>
<li><a href="https://taingram.org/blog/org-mode-blog.html">Building a Emacs Org-Mode Blog</a></li>
<li><a href="https://systemcrafters.net/publishing-websites-with-org-mode/building-the-site/">Build Your Website with Org Mode - System Crafaters</a></li>
<li><a href="https://orgmode.org/worg/org-tutorials/org-publish-html-tutorial.html">Publishing Org-mode files to HTML</a></li>
<li><a href="https://remacs.cc/posts/%E9%9D%A2%E5%90%91%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86%E7%9A%84emacs%E6%95%99%E7%A8%8B17.-%E9%80%9A%E8%BF%87org-mode%E5%86%99%E5%8D%9A%E5%AE%A2/">Èù¢Âêë‰∫ßÂìÅÁªèÁêÜÁöÑEmacsÊïôÁ®ãÔºö17. ÈÄöËøáOrg modeÂÜôÂçöÂÆ¢</a></li>
</ol>
</div>
</div>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-emacs.html">emacs</a> </div>
]]></description>
  <category><![CDATA[emacs]]></category>
  <link>https://dou-meishi.github.io/org-blog/2024-01-14-TryOrgPublish/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2024-01-14-TryOrgPublish/notes.html</guid>
  <pubDate>Sun, 14 Jan 2024 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Insert Code Block in LaTeX]]></title>
  <description><![CDATA[
<p>
The most simple way of inserting code blocks in LaTeX is the built-in
command <code>\verb</code> and environment <code>verbatim</code>.
</p>

<p>
The command <code>\verb</code> can produce inline code. It is better than
<code>\texttt</code> for source code because characters in <code>\verb</code> will be treated
literally. For example, <code>\verb+\textbf{a}+</code> produces <code>\textbf{a}</code>
in monospace font, but <code>\texttt{\textbf{a}}</code> produces a single
character <b><code>a</code></b>, in bold monospace font. Here, the <code>+</code> character after <code>\verb</code>
acts as a delimiter. Any character except letters and <code>*</code> can be used as a delimiter
in <code>\verb</code>. For example, <code>\verb|...|</code> is equivalent to <code>\verb+...+</code>.
</p>

<p>
Similar to <code>\verb</code>, texts enclosed inside the environment <code>verbatim</code> are printed literally.
</p>

<div class="org-src-container">
<pre class="src src-latex"><span style="color: #859900; font-weight: bold;">\documentclass</span>{<span style="color: #657b83; font-weight: bold;">article</span>}
<span style="color: #859900; font-weight: bold;">\begin</span>{<span style="color: #268bd2;">document</span>}

The command <span style="color: #859900; font-weight: bold;">\verb</span>|\verb| can produce inline code.

The <span style="color: #859900; font-weight: bold;">\verb</span>+\verb|\textbf{a}|+ produces <span style="color: #859900; font-weight: bold;">\verb</span>|\textbf{a}|,
but <span style="color: #859900; font-weight: bold;">\verb</span>+\texttt{\textbf{a}}+ produces <span style="color: #859900; font-weight: bold;">\texttt</span>{<span style="color: #859900; font-weight: bold;">\textbf</span>{<span style="font-weight: bold;">a</span>}}.

<span style="color: #859900; font-weight: bold;">\begin</span>{<span style="color: #268bd2;">verbatim</span>}
Text enclosed inside verbatim environment
is printed directly
and all \LaTeX{} commands are ignored.

Blank lines           and spaces are preserved.
<span style="color: #859900; font-weight: bold;">\end</span>{<span style="color: #268bd2;">verbatim</span>}
<span style="color: #859900; font-weight: bold;">\end</span>{<span style="color: #268bd2;">document</span>}
</pre>
</div>

<p>
The LaTeX code above produces the following output.
</p>


<figure id="org12f0c78">
<img src="./simple-code.png" alt="simple-code.png">

</figure>

<p>
Besides the built-in commands, there are various packages providing more features for redering code blocks.
These packages generally are more powerful and can add syntax highlighting, line numbers, background color
and etc. Usual choices include <code>listings</code> package,  <code>minted</code> package
and <code>tcolorbox</code> package. While  <code>listings</code> and  <code>minted</code> are mainly focusing on redering source codes,
<code>tcolorbox</code> is a general package for redering contents in a box.
Actually, <code>tcolorbox</code> internally loads <code>listings</code> or <code>minted</code> packages.
</p>
<div id="outline-container-org3fb72ca" class="outline-2">
<h2 id="org3fb72ca">Create Own Code Block Environment with tcolorbox</h2>
<div class="outline-text-2" id="text-org3fb72ca">
<p>
When <code>tcolorbox</code> is loaded with the option <code>listings</code>, it provides various useful predefined commands
for rendering source blocks by loading <code>listings</code> package automatically. Besides the <code>listings</code> option,
we can also toggle the <code>breakable</code> option to allow a box to span across pages.
</p>

<p>
In the following example, we create two code block envrionments with <code>tcolorbox</code> and <code>listings</code> in preamble.
These two envrionments are basically identical, except that one can read code from external
files and we do no need to copy the code from source files into our latex manuscript.
</p>

<div class="org-src-container">
<pre class="src src-latex"><span style="color: #859900; font-weight: bold;">\documentclass</span>{<span style="color: #657b83; font-weight: bold;">article</span>}

<span style="color: #859900; font-weight: bold;">\usepackage</span>{<span style="color: #657b83; font-weight: bold;">xcolor</span>}
<span style="color: #93a1a1;">% </span><span style="color: #93a1a1;">define colors</span>
  <span style="color: #859900; font-weight: bold;">\definecolor</span>{codebg}{RGB}{253, 246, 227}
  <span style="color: #859900; font-weight: bold;">\definecolor</span>{codefg}{RGB}{101, 123, 131}
  <span style="color: #859900; font-weight: bold;">\definecolor</span>{codegreen}{RGB}{133, 153, 0}
  <span style="color: #859900; font-weight: bold;">\definecolor</span>{codegray}{RGB}{147, 161, 161}
  <span style="color: #859900; font-weight: bold;">\definecolor</span>{codecyan}{RGB}{42, 161, 152}

<span style="color: #859900; font-weight: bold;">\usepackage</span>[listings,breakable,skins]{<span style="color: #657b83; font-weight: bold;">tcolorbox</span>}
<span style="color: #93a1a1;">% </span><span style="color: #93a1a1;">declare our code block environment</span>
  <span style="color: #859900; font-weight: bold;">\newtcblisting</span>{tcbcodeblock}[1]{<span style="color: #93a1a1;">%</span>
    enhanced,
    sharp corners,
    colframe=black,
    coltext=codefg,
    colback=codebg,
    breakable,
    size=fbox,
    listing only,
    listing options={<span style="color: #93a1a1;">%</span>
      style=tcblatex,
      language={#1},
      showspaces=false,
      showstringspaces=false,
      commentstyle=<span style="color: #859900; font-weight: bold;">\color</span>{codegray},
      keywordstyle=<span style="color: #859900; font-weight: bold;">\color</span>{codegreen},
      stringstyle=<span style="color: #859900; font-weight: bold;">\color</span>{codecyan},
      basicstyle=<span style="color: #859900; font-weight: bold;">\ttfamily\footnotesize</span>
    }
  }

<span style="color: #93a1a1;">% </span><span style="color: #93a1a1;">like tcbcodeblock, but read code from files</span>
  <span style="color: #859900; font-weight: bold;">\newtcbinputlisting</span>{<span style="color: #859900; font-weight: bold;">\tcbinputcodeblock</span>}[2]{<span style="color: #93a1a1;">%</span>
    listing file={#2},
    enhanced,
    sharp corners,
    colframe=black,
    coltext=codefg,
    colback=codebg,
    breakable,
    size=fbox,
    listing only,
    listing options={<span style="color: #93a1a1;">%</span>
      style=tcblatex,
      language={#1},
      showspaces=false,
      showstringspaces=false,
      commentstyle=<span style="color: #859900; font-weight: bold;">\color</span>{codegray},
      keywordstyle=<span style="color: #859900; font-weight: bold;">\color</span>{codegreen},
      stringstyle=<span style="color: #859900; font-weight: bold;">\color</span>{codecyan},
      basicstyle=<span style="color: #859900; font-weight: bold;">\ttfamily\footnotesize</span>
    }
  }
</pre>
</div>

<p>
In the main document, we can use <code>tcbcodeblock</code> as a normal environment
with one mandatory argument, which specifies the language of source code,
and use <code>tcbinputcodeblock</code> as a normal command with two mandatory arguments,
which specify the language of source code and the name of the source file.
See the table in <a href="https://mirror-hk.koddos.net/CTAN/macros/latex/contrib/listings/listings.pdf">the manual</a> of <code>listings</code> for a complete list of supported languages.
</p>

<div class="org-src-container">
<pre class="src src-latex"><span style="color: #859900; font-weight: bold;">\begin</span>{<span style="color: #268bd2;">document</span>}
After declaring our own environment <span style="color: #859900; font-weight: bold;">\verb</span>|tcbcodeblock|.
we can enclose source codes in it and render them in <span style="color: #859900; font-weight: bold;">\LaTeX</span>.

This is a code block of <span style="color: #859900; font-weight: bold;">\TeX</span>.
<span style="color: #859900; font-weight: bold;">\begin</span>{<span style="color: #268bd2;">tcbcodeblock</span>}{TeX}
Hello, <span style="color: #859900; font-weight: bold;">\TeX</span>!
<span style="color: #859900; font-weight: bold;">\end</span>{<span style="color: #268bd2;">tcbcodeblock</span>}

This is a code block of Python.
<span style="color: #859900; font-weight: bold;">\begin</span>{<span style="color: #268bd2;">tcbcodeblock</span>}{Python}
# python code
print("Hello, world!")
<span style="color: #859900; font-weight: bold;">\end</span>{<span style="color: #268bd2;">tcbcodeblock</span>}

This is a code block of C.
<span style="color: #859900; font-weight: bold;">\begin</span>{<span style="color: #268bd2;">tcbcodeblock</span>}{C}
// C code
#include &lt;stdio.h&gt;
int main() {
   printf("Hello, World!");
   return 0;
}
<span style="color: #859900; font-weight: bold;">\end</span>{<span style="color: #268bd2;">tcbcodeblock</span>}

The <span style="color: #859900; font-weight: bold;">\verb</span>|\tcbinputcodeblock| can read codes from a file
and render them in a source block like <span style="color: #859900; font-weight: bold;">\verb</span>|\tcbcodeblock|.
This is the <span style="color: #859900; font-weight: bold;">\LaTeX</span>\ source code of this manuscript.
<span style="color: #859900; font-weight: bold;">\tcbinputcodeblock</span>{[LaTeX]TeX}{./tcolorbox-listings.tex}
<span style="color: #859900; font-weight: bold;">\end</span>{<span style="color: #268bd2;">document</span>}
</pre>
</div>


<figure id="org6312be1">
<img src="./tcolorbox-listings.png" alt="tcolorbox-listings.png">

</figure>
</div>
</div>
<div id="outline-container-org89d10b8" class="outline-2">
<h2 id="org89d10b8">Explanation of the Created Code Environment</h2>
<div class="outline-text-2" id="text-org89d10b8">
<p>
First, we load <code>tcolorbox</code> with appropriate options. In addition, we load the <code>xcolor</code> package for color support.
</p>

<div class="org-src-container">
<pre class="src src-latex"><span style="color: #859900; font-weight: bold;">\usepackage</span>{<span style="color: #657b83; font-weight: bold;">xcolor</span>}
<span style="color: #859900; font-weight: bold;">\usepackage</span>[listings,breakable,skins]{<span style="color: #657b83; font-weight: bold;">tcolorbox</span>}
</pre>
</div>

<p>
Then, we create our own code environment for furture uses.
</p>

<div class="org-src-container">
<pre class="src src-latex"><span style="color: #93a1a1;">% </span><span style="color: #93a1a1;">define colors</span>
  <span style="color: #859900; font-weight: bold;">\definecolor</span>{codebg}{RGB}{253, 246, 227}
  <span style="color: #859900; font-weight: bold;">\definecolor</span>{codefg}{RGB}{101, 123, 131}
  <span style="color: #859900; font-weight: bold;">\definecolor</span>{codegreen}{RGB}{133, 153, 0}
  <span style="color: #859900; font-weight: bold;">\definecolor</span>{codegray}{RGB}{147, 161, 161}
  <span style="color: #859900; font-weight: bold;">\definecolor</span>{codecyan}{RGB}{42, 161, 152}
</pre>
</div>

<p>
After that, we create a new envrionment <code>tcbcodeblock</code> with one argument, which is used to specify the code language.
This envrionment can render source code in a box with following features.
</p>

<ol class="org-ol">
<li>Use the <code>enhanced</code> skin.</li>
<li>Box corners are squared.</li>
<li>Box borders are drawn with black lines.</li>
<li>Text color is set to <code>codefg</code>, which is defined previously by RGB(101, 123, 131).</li>
<li>Background color is set to <code>codebg</code>, which is defined previously by RGB(253, 246, 227).</li>
<li>This box can span across multiple pages.</li>
<li>Minimal margin layout with <code>\fbox</code> style.</li>
<li>Show code only (if the source code is latex manuscript, you can remove the <code>listing only</code> option to show
the expected latex output of the latex code enclosed in this envrionment).</li>
<li>Use the predefined style <code>tcblatex</code>. This overwirtes options of the <code>listings</code> package by
options of <code>tcolorbox</code> package. If this line is absent, some of your settings of <code>tcolorbox</code> will not take
effects in this environment.</li>
<li>Set the language of the source code. This is used to identify strings, comments and keywords in the code.</li>
<li>Does not render space as character <code>‚ê£</code>.</li>
<li>Does not render space in strings as character <code>‚ê£</code>.</li>
<li>Text color of comments, keywords and strings are set to  <code>codegray</code>, <code>codegreen</code> and <code>codecyan</code> respectively.</li>
<li>Set text font to monospace and footnote size.</li>
</ol>

<div class="org-src-container">
<pre class="src src-latex"><span style="color: #93a1a1;">% </span><span style="color: #93a1a1;">declare our code block environment</span>
  <span style="color: #859900; font-weight: bold;">\newtcblisting</span>{tcbcodeblock}[1]{<span style="color: #93a1a1;">%</span>
    enhanced,
    sharp corners,
    colframe=black,
    coltext=codefg,
    colback=codebg,
    breakable,
    size=fbox,
    listing only,
    listing options={<span style="color: #93a1a1;">%</span>
      style=tcblatex,
      language={#1},
      showspaces=false,
      showstringspaces=false,
      commentstyle=<span style="color: #859900; font-weight: bold;">\color</span>{codegray},
      keywordstyle=<span style="color: #859900; font-weight: bold;">\color</span>{codegreen},
      stringstyle=<span style="color: #859900; font-weight: bold;">\color</span>{codecyan},
      basicstyle=<span style="color: #859900; font-weight: bold;">\ttfamily\footnotesize</span>
    }
  }
</pre>
</div>

<p>
In addition, we provide a command <code>tcbinputcodeblock</code>, which functions like <code>tcbcodeblock</code> but
read code from external files. It accepts two arguments, one is the language and the other one is the
name of the source file.
</p>

<div class="org-src-container">
<pre class="src src-latex"><span style="color: #93a1a1;">% </span><span style="color: #93a1a1;">like tcbcodeblock, but read code from files</span>
  <span style="color: #859900; font-weight: bold;">\newtcbinputlisting</span>{<span style="color: #859900; font-weight: bold;">\tcbinputcodeblock</span>}[2]{<span style="color: #93a1a1;">%</span>
    listing file={#2},
    enhanced,
    sharp corners,
    colframe=black,
    coltext=codefg,
    colback=codebg,
    breakable,
    size=fbox,
    listing only,
    listing options={<span style="color: #93a1a1;">%</span>
      style=tcblatex,
      language={#1},
      showspaces=false,
      showstringspaces=false,
      commentstyle=<span style="color: #859900; font-weight: bold;">\color</span>{codegray},
      keywordstyle=<span style="color: #859900; font-weight: bold;">\color</span>{codegreen},
      stringstyle=<span style="color: #859900; font-weight: bold;">\color</span>{codecyan},
      basicstyle=<span style="color: #859900; font-weight: bold;">\ttfamily\footnotesize</span>
    }
  }
</pre>
</div>
</div>
</div>
<div id="outline-container-orgf3e69d0" class="outline-2">
<h2 id="orgf3e69d0">External Links&#xa0;&#xa0;&#xa0;<span class="tag"><span class="refs">refs</span></span></h2>
<div class="outline-text-2" id="text-orgf3e69d0">
<ol class="org-ol">
<li><a href="https://mirror-hk.koddos.net/CTAN/macros/latex/contrib/tcolorbox/tcolorbox.pdf">the official manual of tcolorbox - CTAN</a></li>
<li><a href="https://mirror-hk.koddos.net/CTAN/macros/latex/contrib/listings/listings.pdf">the official manual of listings - CTAN</a></li>
<li><a href="https://liam.page/2016/07/22/using-the-tcolorbox-package-to-create-a-new-theorem-environment/">LaTeX ÈªëÈ≠îÊ≥ïÔºà‰∏ÄÔºâÔºötcolorbox ÂÆèÂåÖÁÆÄÊòéÊïôÁ®ã</a></li>
<li><a href="https://www.overleaf.com/learn/latex/Code_listing">Code listing - Overleaf</a></li>
<li><a href="https://www.overleaf.com/learn/latex/Code_Highlighting_with_minted">Code Highlighting with minted</a></li>
</ol>
</div>
</div>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-latex.html">latex</a> </div>
]]></description>
  <category><![CDATA[latex]]></category>
  <link>https://dou-meishi.github.io/org-blog/2024-01-11-CodeBlockinLaTeX/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2024-01-11-CodeBlockinLaTeX/notes.html</guid>
  <pubDate>Thu, 11 Jan 2024 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[An Advanced Guide to Unison]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org022e9e3">Preferences</a></li>
<li><a href="#orgd113678">Setting Preference</a></li>
<li><a href="#org7bba22d">Profiles</a></li>
<li><a href="#org4ccef14">Path Specification</a></li>
<li><a href="#org93e9ea3">Ignoring</a></li>
<li><a href="#org825797a">Backup</a></li>
<li><a href="#org29d0a02">Scenario: Synchronize with a USB Drive</a>
<ul>
<li><a href="#org79df062">Possible Errors</a></li>
<li><a href="#org23e7a8f">Syncrhonize with FAT/NTFS Filesystems</a></li>
</ul>
</li>
</ul>
</div>
</nav>
<p>
In the previous blog, we introduced basics of <a href="https://github.com/bcpierce00/unison">Unison</a>, the lightweight yet versatile file-synchronization tool for Unix and Windows.
In this blog, we will cover more advanced topics, including <i>profiles</i>, <i>ignoring</i>, <i>backup,</i> etc.
You can also find the complete documentation in <a href="https://raw.githubusercontent.com/bcpierce00/unison/documentation/unison-manual.pdf">the official manual</a>.
</p>
<div id="outline-container-org022e9e3" class="outline-2">
<h2 id="org022e9e3">Preferences</h2>
<div class="outline-text-2" id="text-org022e9e3">
<p>
Many details of Unison‚Äôs behavior are configurable by user-settable <i>preferences</i> (or, arguments,
options, in other words). If we type <code>unison -help</code>, then the outputs will look like
</p>

<pre class="example" id="orgd9e6bb0">
Usage: unison [options]
    or unison root1 root2 [options]
    or unison profilename [options]

Basic options:

  General:
   -doc xxx            show documentation ('-doc topics' lists topics)
   -version            print version and exit

  What to sync:
   ...

  How to sync:
   ...

  ...
</pre>

<p>
Here, we see that there are three ways to run Unison, and each way accepts <code>[options]</code>,
standing for various options grouped and detailed below the <i>Usage</i> section.
</p>

<p>
The most general way of running Unison is the first one <code>unison [options]</code>.
Indeed, you can set the two directories that you want to synchronize
inside the options. For example,
</p>

<div class="org-src-container">
<pre class="src src-bash">unison work mirror
</pre>
</div>

<p>
is equivalent to
</p>

<div class="org-src-container">
<pre class="src src-bash">unison -root work -root mirror
</pre>
</div>

<p>
Here, the preference <code>root</code> appears twice, standing for the root directories we want to synchronize.
</p>

<p>
The thrid way of running Unison, <code>unison profilename [options]</code>, is a convenient way
to apply a collection of preferences predefined in profile. Of course, the preferences given
after the <code>profilename</code> will override the values defined in the profile if necessary.
</p>

<p>
Unison provides a lot of preferences for customizing its behaviors and it is recommended to
go through these options once by typing <code>unison -help</code>.
</p>
</div>
</div>
<div id="outline-container-orgd113678" class="outline-2">
<h2 id="orgd113678">Setting Preference</h2>
<div class="outline-text-2" id="text-orgd113678">
<p>
There are two ways to set the values of preferences: temporarily, by providing command-line arguments
to a particular run of Unison, or permanently, by adding commands to a profile in the <code>.unison</code> directory on
the client host.
</p>

<p>
In the command line, if we want to set a value of a preference, say <code>p</code>, then we should be careful about its type.
</p>

<ul class="org-ul">
<li>If <code>p</code> is a boolean flag, then adding an argument <code>-p=false</code> will set <code>p</code> to false and adding <code>-p=true</code> (or simply
<code>-p</code>) will set <code>p</code> to true.</li>

<li>If <code>p</code> is a numeric or string preference, then adding an argument <code>-p value</code> is enough.</li>
</ul>

<p>
In the profile, a line in the form <code>p = value</code> works for both boolean flags and non-boolean falgs.
</p>
</div>
</div>
<div id="outline-container-org7bba22d" class="outline-2">
<h2 id="org7bba22d">Profiles</h2>
<div class="outline-text-2" id="text-org7bba22d">
<p>
A <i>profile</i> is a text file that specifies permanent settings for roots, paths, ignore patterns, and other preferences.
Profiles should reside in the <code>.unison</code> directory on the <i>client</i> machine.
The <code>.unison</code> directory is by default set to <code>$HOME/.unison</code> in Unix; see <a href="https://raw.githubusercontent.com/bcpierce00/unison/documentation/unison-manual.pdf">the official manual</a>
for more details on its location in other systems and how to change it.
</p>

<p>
If Unison is started  with just one argument <code>name</code> on the command line, it looks for a profile called
<code>name</code> (or <code>name.prf</code>, if not found) in the <code>.unison</code> directory.
 If Unison is started with no arguments, it will behave as <code>name</code> has been set to <code>default</code>,
i.e., looking for a profile called <code>default</code> or <code>default.prf</code>.
</p>

<p>
Inside a profile,
</p>

<ul class="org-ul">
<li>blank lines and lines beginning with <code>#</code> both are ignored;</li>
<li>a line of the form <code>p = value</code> sets the value of preference <code>p</code> to <code>value</code>;</li>
<li>Spaces and tabs before and after <code>p</code> and <code>value</code> are ignored;</li>
<li>Spaces, tabs, and non-printable characters within values are treated literally,
so that e.g. <code>root = /foo bar</code> refers to a directory containing a space;</li>
<li>a line of the form <code>include name</code> causes the file <code>name</code> (or <code>name .prf</code>,
if not found) to be read at the point, and included as if its contents;</li>
<li>a line of the form <code>source name</code> does the same as <code>include name</code> except that it does not
attempt to add a suffix to name;</li>
<li>Similar lines of the form <code>include? name</code> or <code>source? name</code> do the same
as their respective lines without the question mark except that it does not constitute an error to specify a
non-existing file name.</li>
</ul>

<p>
A profile may include a special preference <code>label</code> to provide a description of the options selected in this
profile. Its value is listed along with the profile name in the graphical user interface.
</p>
</div>
</div>
<div id="outline-container-org4ccef14" class="outline-2">
<h2 id="org4ccef14">Path Specification</h2>
<div class="outline-text-2" id="text-org4ccef14">
<p>
Several Unison preferences (e.g., <code>ignore</code>, <code>backup</code>, <code>merge</code>, etc.) specify individual paths or sets of paths.
These preferences can be set to any of the following patterns.
</p>

<ol class="org-ol">
<li><code>Name name</code> matches any path in which the last component matches <code>name</code>.
For example, <code>Name N</code> can match a pathlike <code>mirror/N</code>, even if it is a directory.</li>

<li><code>Path path</code> matches exactly the path <code>path</code>.</li>

<li><code>BelowPath path</code> matches the path <code>path</code> and any path below.</li>
</ol>

<p>
In those forms, the <code>name</code> or <code>path</code> argument can be a glob pattern, which means
<code>*</code>, <code>?</code>, <code>[]</code> and <code>{}</code> have their special meanings.
</p>
</div>
</div>
<div id="outline-container-org93e9ea3" class="outline-2">
<h2 id="org93e9ea3">Ignoring</h2>
<div class="outline-text-2" id="text-org93e9ea3">
<p>
We can instruct Unison to ignore paths by setting the preference <code>ignore</code>. For example, the below line
in a profile tells Unison to ignore the path <code>a/b</code>:
</p>

<pre class="example" id="orge65c55b">
ignore = Path a/b
</pre>

<p>
Of course, you can set <code>ignore</code> multiple times to ignore as many files as you want.
</p>

<p>
There is also an <code>ignorenot</code> preference, which specifies a set of patterns for paths that should not be
ignored, even if they match an ignore pattern.
</p>

<p>
Here are a few extra points regarding the ignore preference you probably want to know.
</p>

<ol class="org-ol">
<li>If a directory is ignored, then <i>all its descendants</i> will be too.</li>

<li>Be careful about renaming directories containing ignored files. Because Unison understands the rename
as a delete plus a create, any ignored files in the directory will be lost.</li>

<li>The interaction of these two sets of patterns can be a little tricky.
If a path matches an <i>ignore</i> pattern and does not match an
<i>ignorenot</i> pattern, then this whole path <i>including everything below it</i> will be ignored.
For example, if the ignore pattern contains <code>Name data</code> and the ignorenot pattern contains <code>Name *.py</code>,
then Unison still ignores a path like <code>data/a.py</code>.</li>
</ol>
</div>
</div>
<div id="outline-container-org825797a" class="outline-2">
<h2 id="org825797a">Backup</h2>
<div class="outline-text-2" id="text-org825797a">
<p>
When Unison overwrites (or deletes) a file or directory while propagating changes from the other replica,
it can keep the old version around as a backup. Similar to ignoring, you can set the preference <code>backup</code>
to require what kind of files should be backed up. For example,
</p>

<pre class="example" id="orga9a959c">
backup = Name *
</pre>

<p>
causes Unison to create backups for <i>all</i> files and directories. You can also set the preference
<code>backupnot</code> for exceptions, just like <code>ignorenot</code> for <code>ignore</code>.
</p>

<p>
The location of backup files are controlled by <code>backuploc</code>, whose value must be either
</p>

<ul class="org-ul">
<li><code>local</code>, meaning that backup files are stored in the same directory as the original;</li>
<li><code>central</code>, which is the default value, meaning that all backup files should be stored in
the directory specified by preference <code>backupdir</code>. The default value of <code>backupdir</code> is
<code>.unison/backup</code>.</li>
</ul>

<p>
We can have finer controls on backup files by setting preferences like
<code>maxbackups</code>, <code>backupprefix</code>, <code>backupsuffix</code> and etc.
</p>

<p>
It is important to note that Unison will backup will only be checked against updated paths,
not their descendants. For example, if you set <code>backup = Name *.txt</code> and then delete a whole directory named
<code>foo</code> containing some text files, these files will not be backed up because Unison will just check that <code>foo</code>
is updated and it does not match <code>*.txt</code>. Similarly, if the directory itself happened to be called <code>foo.txt</code>,
then the whole directory and all the files in it will be backed up, regardless of their names.
</p>
</div>
</div>
<div id="outline-container-org29d0a02" class="outline-2">
<h2 id="org29d0a02">Scenario: Synchronize with a USB Drive</h2>
<div class="outline-text-2" id="text-org29d0a02">
<p>
Assume We want to synchronize our home directory <code>/home/dou</code> with a USB drive,
mounted at <code>/media/dou/KINGSTON</code>.
It is very likely that Unison will raise errors time to time. But don't worry. Those errors are
intended and we explain them below.
</p>
</div>
<div id="outline-container-org79df062" class="outline-3">
<h3 id="org79df062">Possible Errors</h3>
<div class="outline-text-3" id="text-org79df062">
<p>
If our USB drive is empty, the first time we run Unison will simply copying files.
Suppose we have done and reject the USB drive. What will happen if we accidentally
run Unison again, e.g., triggered by a crontab task?
</p>

<p>
The answer is that Unison will try to <i>remove our whole home directory</i>!
As Unison keeps a records of the state of last synchronization,
it sees that <code>/home/dou</code> hasn't changed and <code>/media/dou/KINGSTON</code> is now empty.
So it will try the apply the changes made in the latter to the former,
which results the deletion of a whoe replica.
Fortunately, if <code>confirmbigdel</code> is set to true, which is the default case,
Unison will ask for the confirmation of such deletion.
If Unison is run in batch mode, it will simply abort and exit.
</p>

<p>
If what we want to do is synchronizing <code>/home/dou/Documents</code> with <code>/media/dou/KINGSTON/Documents</code>,
then Unison might throw the <i>No such file or directory</i> error.
This is the case when our USB drive is rejected, the default mount point <code>/media/dou/KINGSTON</code>
is also deleted. As the second root directory
 <code>/media/dou/KINGSTON/Documents</code>  does not exist, Unison will first try to
cd to the parent directory <code>/media/dou/KINGSTON</code>, which does not exist too.
So Unison exits with <i>Fatal Error: Cannot find canonical name of &#x2026;</i>
</p>
</div>
</div>
<div id="outline-container-org23e7a8f" class="outline-3">
<h3 id="org23e7a8f">Syncrhonize with FAT/NTFS Filesystems</h3>
<div class="outline-text-3" id="text-org23e7a8f">
<p>
If Unison is running on a Unix-like machine but told to synchronize
with FAT/NTFS filesystems, the <code>fat</code> preference should be set to true,
which is equivalent to
</p>

<ul class="org-ul">
<li>do not synchronize permissions, <code>perms = 0</code>;</li>
<li>never use chmod, <code>dontchmod = true</code>;</li>
<li>treat filenames as case insensitive, <code>ignorecase = true</code>;</li>
<li>do not attempt to synchronize symbolic links, <code>links = false</code>;</li>
<li>ignore inode number changes when detecting updates, <code>ignoreinodenumbers = true</code>.</li>
</ul>
</div>
</div>
</div>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-tool.html">tool</a> </div>
]]></description>
  <category><![CDATA[tool]]></category>
  <link>https://dou-meishi.github.io/org-blog/2024-01-07-ReviewUnison/advanced.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2024-01-07-ReviewUnison/advanced.html</guid>
  <pubDate>Sun, 07 Jan 2024 21:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Beginner's Guide to Unison]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org9d56d85">A Demo</a></li>
<li><a href="#orgde52666">Basic Concepts</a></li>
<li><a href="#org2820a37">Typical Usage</a></li>
<li><a href="#org849dc45">Caveats and Shortcomings</a></li>
<li><a href="#org6ac0c7d">Going Further</a></li>
</ul>
</div>
</nav>
<p>
<a href="https://github.com/bcpierce00/unison">Unison</a> is a file-synchronization tool for Unix and Windows. It allows two replicas of a collection of files and
directories to be stored on different hosts (or different disks on the same host), modified separately, and then
brought up to date by propagating the changes in each replica to the other.
</p>

<p>
<b>Note:</b> This review is a short summary of <a href="https://raw.githubusercontent.com/bcpierce00/unison/documentation/unison-manual.pdf">the official manual</a>.
Please use version 2.52 or newer to avoid version interoperability issues.
</p>
<div id="outline-container-org9d56d85" class="outline-2">
<h2 id="org9d56d85">A Demo</h2>
<div class="outline-text-2" id="text-org9d56d85">
<p>
Unison can be used with either of two user interfaces: a textual interface and a graphical interface.
</p>

<p>
Let's consider a simple scenario and see how to synchronize two directories on a single machine.
</p>

<ol class="org-ol">
<li value="0">install Unison. Basically, we need only two executable binary files, <code>unison</code> and <code>unison-gui</code>,
downloaded from the proper release tarball in <a href="https://github.com/bcpierce00/unison">its github repository</a>.</li>

<li><p>
Set up a <code>work</code> directory and a <code>mirror</code> direcotry for our illustration
</p>

<div class="org-src-container">
<pre class="src src-bash">    mkdir work
    touch work/a work/b
    mkdir work/d
    touch work/d/f
    cp -r work mirror
</pre>
</div></li>

<li>Try synchronizing <code>work</code> and <code>mirror</code>. Since they are identical, synchronizing them won‚Äôt propagate
any changes, but Unison will remember the current state of both directories so that it will be able to tell
next time what has changed by typing <code>unison work mirror</code>.

<ul class="org-ul">
<li><i>textual interface:</i> you should see a message notifying you that all the files are actually equal and then get returned to
the command line, and you may also get a warning message for creating <i>archives</i> (the private data structure used by Unison)
as this is the first run of Unison.</li>

<li><i>graphical interface:</i> You should get a big empty window with a message at the bottom notifying you <i>everything is up-to-date</i>.</li>
</ul></li>

<li><p>
Make some changes in <code>work</code> and <code>mirror</code>
</p>

<div class="org-src-container">
<pre class="src src-bash">    rm work/a
    <span style="color: #657b83; font-weight: bold;">echo</span> <span style="color: #2aa198;">"Hello"</span> &gt; work/b
    <span style="color: #657b83; font-weight: bold;">echo</span> <span style="color: #2aa198;">"Hello"</span> &gt; mirror/b
    date &gt; mirror/c
    <span style="color: #657b83; font-weight: bold;">echo</span> <span style="color: #2aa198;">"Hi there"</span> &gt; work/d/h
    <span style="color: #657b83; font-weight: bold;">echo</span> <span style="color: #2aa198;">"Hello there"</span> &gt; mirror/d/h
</pre>
</div></li>

<li><p>
Try synchronizing <code>work</code> and <code>mirror</code> again by typing <code>unison work mirror</code>.
</p>

<p>
Let us elaborate the behaviors of the textual interface in this case.
</p>

<ol class="org-ol">
<li value="0">Unison will display <i>only</i> the files that are different and ask for actions one by one.
If a file has been changed in the same way and remain identical in both directories,
Unison will simply note the file is up-to-date and nothing will be shown. So we expect three
changes to be decided: the absent file of <code>a</code> in <code>work</code>, the new file <code>c</code> in <code>mirror</code>
and the conflicting changes on <code>d/h</code>.</li>

<li><p>
Unison will notify the creation of <code>c</code> in <code>mirror</code> and prompt a line like
</p>

<pre class="example" id="org0529ae1">
         &lt;--- new file   c  [f]
</pre>

<p>
We can follow Unison‚Äôs recommendation, press <code>f</code> or <code>[ENTER]</code> at the prompt.
Or we can simply ignore this file and leave both replicas alone by pressing <code>/</code>.
Pressing <code>?</code> for a list of possible responses and their meanings.
See also <a href="https://stackoverflow.com/questions/64587602/how-does-one-use-the-matching-condition-commands-for-the-unison-cli">this question</a> for explanation on the key <code>L</code> and matching conditions.
</p></li>

<li><p>
Similarly, Unison will notify the delete of <code>a</code> in work and prompt a line like
</p>

<pre class="example" id="orgcf0810e">
deleted  ---&gt;            a  [f]
</pre></li>

<li><p>
For conflicting changes on <code>d/h</code>, Unison will prompt a line like
</p>

<pre class="example" id="orgb27bcb1">
new file &lt;-?-&gt; new file   d/h  []
</pre></li>

<li><p>
Suppose we skip the file <code>d/h</code> and accept changes on file <code>a</code> and <code>c</code>, Unison
will briefly summarize the actions it is supposed to do and asks for
confirmation
</p>

<pre class="example" id="org8870807">
2 items will be synced, 1 skipped
0 B to be synced from work to mirror
32 B to be synced from mirror to work

Proceed with propagating updates? []
</pre></li>

<li>Finally, if we confirm then Unison will apply changes and output logs of the process.</li>
</ol>

<p>
The usage of the graphical interface is similar. The main window shows all the files that have been modified.
To override a default action (or to select an action in the case when there is no default), first select the file
by clicking on its name, then press the desired action key metioned before.
When you are satisfied with the propagation of changes as shown in the main window,
click the <i>Go</i> button to set them in motion.
</p></li>
</ol>
</div>
</div>
<div id="outline-container-orgde52666" class="outline-2">
<h2 id="orgde52666">Basic Concepts</h2>
<div class="outline-text-2" id="text-orgde52666">
<p>
Below is a short summary of <a href="https://raw.githubusercontent.com/bcpierce00/unison/documentation/unison-manual.pdf">the official manual</a>.
</p>

<ol class="org-ol">
<li><i>Roots.</i> A replica‚Äôs root tells Unison where to find a set of files to be synchronized, either on the local machine or on
a remote host. The pattern of the root is <code>[protocol:]//[user@][host][:port][path]</code>.
When <code>path</code> is given without any protocol prefix, the protocol is assumed to be <code>file</code>.
Other possible protocol arguments include <code>ssh</code> and <code>socket</code>.
If <code>path</code> is a relative path, then it actually specifies a local root relative to the directory where Unison is started.</li>

<li><i>Paths.</i> A <i>path</i> refers to a point within a set of files being synchronized; it is specified relative to the root of the
replica. Formally, a path is just a sequence of names, separated by <code>/</code>.
The empty path (i.e., the empty sequence of names) denotes the whole replica.
Unison displays the empty path as <code>[root]</code>.</li>

<li><i>Descendants.</i>   If <code>p</code> is a path and <code>q</code> is a path beginning with <code>p</code>, then <code>q</code> is said to be a <i>descendant</i> of <code>p</code>.
Thus, each path is also a descendant of itself.</li>

<li><p>
<i>Contents.</i> The <i>contents</i> of a path <code>p</code> in a particular replica could be a file, a directory, a symbolic link, or absent (if p
does not refer to anything at all in that replica). More specifically:
</p>

<ol class="org-ol">
<li>If <code>p</code> refers to an ordinary file, then the contents of p are the actual contents of this file (a string of
bytes) plus the current permission bits of the file.</li>

<li>If <code>p</code> refers to a symbolic link, then the contents of <code>p</code> are just the string specifying where the link points.</li>

<li>If <code>p</code> refers to a directory, then the contents of p are just the token <i>DIRECTORY</i> plus the current
permission bits of the directory.</li>

<li>If <code>p</code> does not refer to anything in this replica, then the contents of <code>p</code> are the token <i>ABSENT</i>.</li>
</ol>

<p>
Unison keeps a record (named <i>archives</i>) of the contents of each path after each successful synchronization
of that path (i.e., it remembers the contents at the last moment when they were the same in the two replicas).
</p></li>

<li><i>Update.</i> A path is <i>updated</i> (in some replica) if its current contents are different from its contents the
last time it was successfully synchronized.</li>

<li><i>Conflicts.</i> A path is said to be <i>conflicting</i> if the following conditions <i>all</i> hold:

<ol class="org-ol">
<li>it has been updated in one replica,</li>

<li>any of its descendants has been updated in the other replica,</li>

<li>its contents in the two replicas are not identical.</li>
</ol></li>

<li><i>Reconciliation.</i> Unison operates in several distinct stages:

<ol class="org-ol">
<li>On each host, it compares its archive file (which records the state of each path in the replica when it
was last synchronized) with the current contents of the replica, to determine which paths have been updated.</li>

<li>It checks for <i>false conflicts</i>  ‚Äî paths that have been updated on both replicas, but whose current
values are identical. These paths are silently marked as synchronized in the archive files in both replicas.</li>

<li>It displays all the updated paths to the user. For updates that do not conflict, it suggests a default
action (propagating the new contents from the updated replica to the other). Conflicting updates are
just displayed. The user is given an opportunity to examine the current state of affairs, change the
default actions for nonconflicting updates, and choose actions for conflicting updates.</li>

<li>It performs the selected actions, one at a time. Each action is performed by first transferring the new
contents to a temporary file on the receiving host, then atomically moving them into place.</li>

<li>It updates its archive files to reflect the new state of the replicas.</li>
</ol></li>

<li><p>
<i>Invariants.</i> Unison is careful to protect both its internal state and the state of the replicas at every point in this
process. Specifically, the following guarantees are enforced:
</p>

<ol class="org-ol">
<li>At every moment, each path in each replica has either

<ol class="org-ol">
<li>its original contents (i.e., no change at all has been made to this path), or</li>

<li>its correct final contents (i.e., the value that the user expected to be propagated from the other replica).</li>
</ol></li>

<li>At every moment, the information stored on disk about Unison‚Äôs private state can be either

<ol class="org-ol">
<li>unchanged, or</li>
<li>updated to reflect those paths that have been successfully synchronized.</li>
</ol></li>
</ol>

<p>
If Unison gets interrupted during ensuring those guarantees, some manual cleanup may be required.
In this case, a file called DANGER.README will be left in the <code>.unison</code> directory,
containing information about the operation that was interrupted.
The next time you try to run Unison, it will notice this file and warn you about it.
</p>

<p>
If Unison is interrupted, it may sometimes leave temporary working files (with suffix <code>.tmp</code>) in the replicas.
It is safe to delete these files. Also, if the backups flag is set, Unison will leave around old versions of files
that it overwrites, with names like <code>file.0.unison.bak</code>. These can be deleted safely when they are no longer wanted.
</p>

<p>
If Unison finds that its archive files have been deleted (or that the archive format has changed and they
cannot be read, or that they don‚Äôt exist because this is the first run of Unison on these particular roots),
it takes a conservative approach: it behaves as though the replicas had both been completely empty at the
point of the last synchronization. Thus, It is also safe to delete those archive files on both replicas.
The next time Unison runs, it will assume that all the files it sees in the replicas are new.
</p></li>
</ol>
</div>
</div>
<div id="outline-container-org2820a37" class="outline-2">
<h2 id="org2820a37">Typical Usage</h2>
<div class="outline-text-2" id="text-org2820a37">
<p>
Once you are comfortable with the basic operation of Unison, you may find yourself wanting to use it
regularly to synchronize your commonly used files. There are several possible ways of going about this:
</p>

<ol class="org-ol">
<li>Synchronize your whole home directory, using the <i>Ignore facility</i> to
avoid synchronizing particular directories and files.</li>

<li>Synchronize your whole home directory, but tell Unison to synchronize only some of
the files and subdirectories within it. This can be accomplished by specifying the <code>-path</code>
arguments in your <i>profile</i>.</li>

<li>Create another directory called <code>shared</code> (or <code>current</code>, or whatever) on each host,
and put all the files you want to synchronize into this directory. Tell Unison to synchronize <code>shared</code>
among different hosts.</li>

<li>Create another directory called <code>shared</code> (or <code>current</code>, or whatever) on each host,
and put <i>links</i> to all the files you want to synchronize into this directory. Use the <code>follow</code> preference
to make Unison treat these links as transparent.</li>
</ol>

<p>
Unison is designed for synchronizing pairs of replicas. However, it is possible to use it to keep larger groups
of machines in sync by performing multiple pairwise synchronizations.
If you need to do this, the most reliable way to set things up is to organize the machines into a <i>star topology</i>
with one machine designated as the <i>hub</i> and the rest as <i>spokes</i> and with each spoke machine
synchronizing only with the hub.
</p>
</div>
</div>
<div id="outline-container-org849dc45" class="outline-2">
<h2 id="org849dc45">Caveats and Shortcomings</h2>
<div class="outline-text-2" id="text-org849dc45">
<p>
Here are some things to be careful of when using Unison.
</p>

<ol class="org-ol">
<li>Unison cannot understand rename, and sees it as a delete and a separate create.</li>

<li><p>
You need to be very <i>CAREFUL</i> when renaming directories containing <code>ignored</code> files.
</p>

<p>
For example, suppose Unison is synchronizing directory <code>A</code> between the two machines called the <i>local</i>
and the <i>remote</i> machine; suppose directory <code>A</code> contains a subdirectory <code>D</code>; and suppose <code>D</code> on the
local machine contains a file or subdirectory <code>P</code> that matches an ignore directive in the profile used to
synchronize. Thus path <code>A/D/P</code> exists on the local machine but not on the remote machine.
</p>

<p>
If <code>D</code> is renamed to <code>Dnew</code> on the remote machine, and this change is propagated to the local machine, all
such files or subdirectories <code>P</code> will be deleted. This is because Unison sees the rename as a delete and
a separate create: it deletes the old directory (including the ignored files) and creates a new one (not
including the ignored files, since they are completely invisible to it).
</p></li>

<li><p>
It could be very <i>DANGEROUS</i> to use Unison with removable media such as USB drives
unless you are careful.
</p>

<p>
If you synchronize a directory that is stored on removable media when the media is not present, it will look to Unison as though
the whole directory has been deleted, and it will proceed to delete the directory from the other replica!
</p></li>

<li>Archives are created based on names of roots (and other informations), meaning that renaming roots
results Unison think it never sync these before. For example, assume you have run Unison to sync
<code>work</code> and <code>mirror</code> before, and you rename <code>mirror</code> to <code>backup</code> then change some files in
<code>backup</code>. Now, running <code>unison</code> work backup will create new archives and ask you to resolve conflicts.
In this case, you may find the option <code>-prefer backup</code> be useful, which effectively choose files
in <code>backup</code> to resolve possible conflicts.</li>

<li>If you want to run Unison continuously as a crontab task, then you have to ensure the same
script will not be called unless its previous call has finished. Otherwise there will be two
running Unison instance caring about same targets and interfere each other. For example,
it could be that a sync of big files takes more than 10 minutes, which would create problems
if you have set every 10 minutes a new sync would be started.</li>

<li>The graphical user interface is single-threaded. This means that if Unison is performing some long-
running operation, the display will not be repainted until it finishes. We recommend not trying to do
anything with the user interface while Unison is in the middle of detecting changes or propagating files.</li>
</ol>
</div>
</div>
<div id="outline-container-org6ac0c7d" class="outline-2">
<h2 id="org6ac0c7d">Going Further</h2>
<div class="outline-text-2" id="text-org6ac0c7d">
<p>
The official manual is <a href="https://raw.githubusercontent.com/bcpierce00/unison/documentation/unison-manual.pdf">here</a> and the FAQ is <a href="https://alliance.seas.upenn.edu/~bcpierce/wiki/index.php">here</a>.
</p>

<p>
Besides the basic concepts mentioned in this blog, you may also want to look at the
following sections in the official manual:
</p>

<ul class="org-ul">
<li>Section 6.1 Running Unison</li>
<li>Section 6.2 The <code>.unison</code> Directory</li>
<li>Section 6.4 Preferences</li>
<li>Section 6.5 Profiles</li>
<li>Section 6.6 Sample Profiles</li>
<li>Section 6.7 Keeping Backups</li>
<li>Section 6.8 Merging Conflicting Versions</li>
<li>Section 6.12 Path Specification</li>
<li>Section 6.13 Ignoring Paths</li>
</ul>
</div>
</div>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-tool.html">tool</a> </div>
]]></description>
  <category><![CDATA[tool]]></category>
  <link>https://dou-meishi.github.io/org-blog/2024-01-07-ReviewUnison/basics.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2024-01-07-ReviewUnison/basics.html</guid>
  <pubDate>Sat, 06 Jan 2024 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Examples of Banach Spaces]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgbdb77ed">Prerequisite Concepts</a></li>
<li><a href="#orga80e53a">Definition</a></li>
<li><a href="#org48bb99d">Main Results</a></li>
<li><a href="#org1691a83">Examples</a></li>
</ul>
</div>
</nav>
<p>
This note gives some common examples of Banach spaces as well as some
counterexamples.
</p>

<p>
Readers may refer to <i>Chapter 7: Complete Metric Space and Functional
Space</i> in the book <i>Munkres, J. Topology Second Edition</i> for more
details.
</p>
<div id="outline-container-orgbdb77ed" class="outline-2">
<h2 id="orgbdb77ed">Prerequisite Concepts</h2>
<div class="outline-text-2" id="text-orgbdb77ed">
<ul class="org-ul">
<li>normed linear space</li>
<li>complete metric space</li>
<li>the metric induced by a norm</li>
<li>continuity</li>
</ul>
</div>
</div>
<div id="outline-container-orga80e53a" class="outline-2">
<h2 id="orga80e53a">Definition</h2>
<div class="outline-text-2" id="text-orga80e53a">
<p>
A complete normed linear space is called a <b>Banach space</b>.
</p>

<p>
Some common used functional spaces are listed below.
</p>

<ol class="org-ol">
<li>\(Y^X\): the set of all functions from \(X\) to \(Y\).</li>
<li>\(\mathcal{B}(X;Y)\): the set of all bounded functions from \(X\) to \(Y\).</li>
<li>\(\mathcal{C}(X;Y)\): the set of all continuous functions from \(X\) to
\(Y\).</li>
</ol>

<p>
Given a metric space \((Y,d)\), the <b>sup metric</b> on
\(\mathcal{B}(X;Y)\) corresponding to \(d\) is defined by
\[
\rho(f,g):=\sup_{x\in X}d(f(x),g(x)).
\]
</p>

<p>
The <b>standard bounded metric</b> \(\bar{d}\) derived from \(d\) is defined by
\[
\bar{d}(x,y):=\min(d(x,y),1).
\]
</p>

<p>
Given a metric space \((Y,d)\), the <b>uniform metric</b> on \(Y^X\)
corresponding to \(d\) is defined by
\[
\bar{\rho}(f,g):=\sup_{x\in X}\bar{d}(f(x),g(x)).
\]
Clearly, the relation between uniform metric and sup metric is
\[
\bar{\rho}(f,g) = \min(\rho(f,g),1).
\]
</p>

<p>
Given a normed linear space \((Y,\|\cdot\|)\), the <b>sup norm</b> on \(Y^X\)
corresponding to \(\|\cdot\|\) is defined by
\[
\|f\|_\infty:=\sup_{x\in X}\|f(x)\|.
\]
</p>
</div>
</div>
<div id="outline-container-org48bb99d" class="outline-2">
<h2 id="org48bb99d">Main Results</h2>
<div class="outline-text-2" id="text-org48bb99d">
<p>
<b>Lemma 1.</b> If \((Y,d)\) is complete, then \((Y^X,\bar{\rho})\) is also
complete.
</p>

<p>
<b>Lemma 2.</b> Let \(X\) be a topological space and \((Y,d)\) be a metric space
(not necessarily complete), then \(\mathcal{B}(X;Y)\) and
\(\mathcal{C}(X;Y)\) are both closed sets in \((Y^X,\bar{\rho})\).
</p>

<p>
<b>Theorem 3.</b> Let Let \(X\) be a topological space and \((Y,d)\) be a
<i>complete</i> metric space, then \(\mathcal{B}(X;Y)\) and \(\mathcal{C}(X;Y)\)
are both complete in the uniform metric.
</p>
</div>
</div>
<div id="outline-container-org1691a83" class="outline-2">
<h2 id="org1691a83">Examples</h2>
<div class="outline-text-2" id="text-org1691a83">
<p>
<b>Example.</b> \((\mathcal{C}[a,b],\|\cdot\|_\infty)\) is a Banach
space.
</p>

<p>
<b>Example.</b> \((\mathcal{B}[a,b],\|\cdot\|_\infty)\) is a Banach space.
</p>

<p>
<b>Example.</b> \((\ell_\infty,\|\cdot\|_\infty)\)
is a Banach space, where
\[
\ell_\infty := \{\mathbf{x}\in\mathbb{R}^\mathbb{N}\mid
\|\mathbf{x}\|_\infty < \infty\}.
\]
</p>

<p>
<b>Example.</b> \((\mathcal{C}^1[a,b],\|\cdot\|_\infty)\) is not complete,
because \(\mathcal{C}^1[a,b]\) is not closed in \((Y^X,\bar{\rho})\).
Consider \(f_n(x):=\sqrt{x^2+\frac{1}{n}}\). It is easy to check that
\(f_n\) converges to \(|x|\).
</p>
</div>
</div>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-math.html">math</a> </div>
]]></description>
  <category><![CDATA[math]]></category>
  <link>https://dou-meishi.github.io/org-blog/2023-10-23-BanachSpaceExample/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2023-10-23-BanachSpaceExample/notes.html</guid>
  <pubDate>Mon, 23 Oct 2023 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Compactness]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org18669c8">Prerequisite Concepts</a></li>
<li><a href="#orge0ae373">Definitions</a></li>
<li><a href="#org3a46d3c">Main Results</a></li>
<li><a href="#org7c2d47e">Detailed Proofs</a></li>
</ul>
</div>
</nav>
<p>
This note is a review of Chapter IV.8 of the book
<i>An introduction to set theory and topology</i>
by Freiwald.
</p>
<div id="outline-container-org18669c8" class="outline-2">
<h2 id="org18669c8">Prerequisite Concepts</h2>
<div class="outline-text-2" id="text-org18669c8">
<ul class="org-ul">
<li>topological space and its subspace</li>
<li>T1 topological space and T2 topological space</li>
<li>Lindel√∂f topological space</li>
<li>sequence convergence in a topological space</li>
<li>continuity of functions between topological spaces</li>
<li>first countability of topological spaces</li>
<li>pseudometric space and metric space</li>
<li>separable metric space</li>
</ul>
</div>
</div>
<div id="outline-container-orge0ae373" class="outline-2">
<h2 id="orge0ae373">Definitions</h2>
<div class="outline-text-2" id="text-orge0ae373">
<p>
Two equivalent definition of a <b>compact</b> topological space \((X,\mathcal{T})\):
</p>

<ol class="org-ol">
<li>Any open cover of \(X\) has a finite open subcover<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>.</li>
<li>Any family of closed sets \(\mathcal{F}\) with <i>Finite Intersection Property</i>
has \(\bigcap\mathcal{F}\neq\emptyset\)<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>.</li>
</ol>

<p>
A subset \(K\) in \((X,\mathcal{T})\) is called a <b>compact subset</b>
if the subspace \((K, \mathcal{T}_K)\) is compact<sup><a id="fnr.3" class="footref" href="#fn.3" role="doc-backlink">3</a></sup>.
</p>

<p>
Three more different notions of <i>compactness</i>.
</p>

<dl class="org-dl">
<dt>sequentially compact</dt><dd>every sequence has a convergent subsequence</dd>
<dt>countably compact</dt><dd>every countable open cover of \(X\) has a finite subcover</dd>
<dt>pseudocompact</dt><dd>every continuous function \(f:X\to\mathbb{R}\) is bounded.</dd>
</dl>

<p>
In addition, if \(X\) is a pseudometric space, there is another important property.
</p>

<dl class="org-dl">
<dt>totally bounded</dt><dd>for each \(\epsilon > 0\), \(X\) can be covered by
a finite number of \(\epsilon\)-balls.</dd>
</dl>
</div>
</div>
<div id="outline-container-org3a46d3c" class="outline-2">
<h2 id="org3a46d3c">Main Results</h2>
<div class="outline-text-2" id="text-org3a46d3c">
<p>
Let \((X, \mathcal{T})\) be a topological space.
</p>

<p>
<b>Theorem 8.5.</b> <a id="orgf2bc98f"></a> <i>Let \(K \subset X\).</i>
</p>

<p>
<i>1. \(K\) is closed \(\Longrightarrow\) \(K\) is compact, if \(X\) is compact.</i>
</p>

<p>
<i>2. \(K\) is compact \(\Longrightarrow\) \(K\) is closed, if \(X\) is Hausdorff.</i>
</p>

<p>
<b>Theorem 8.11.</b> <a id="org9891839"></a> <i>The following implications hold generally.</i>
\[ X\text{ is (sequentially) compact }\Rightarrow X\text{ is countably compact }
\Rightarrow X\text{ is pseudocompact}.\]
</p>

<p>
<b>Lemma 8.12.</b> <a id="org52049be"></a> \(X\) is countably compact \(\Longrightarrow\) \(X\) is sequentially compact,
if \(X\) is first countable.
</p>

<p>
<b>Lemma 8.16.</b> <a id="orgb017007"></a> A totally bounded pseudometric space is separable, and thus Lindel√∂f.
</p>

<p>
<b>Theorem 8.17.</b> <a id="orge3f6358"></a> In a pseudometric space \((X,d)\),
the property of compactness, sequentially compactness, countably compactness
and pseudocompactness are all equivalent.
</p>
</div>
</div>
<div id="outline-container-org7c2d47e" class="outline-2">
<h2 id="org7c2d47e">Detailed Proofs</h2>
<div class="outline-text-2" id="text-org7c2d47e">
<p>
<i>Proof to <a href="#orgf2bc98f">Theorem 8.5</a>.</i>
Assume \(K\) is closed and \(X\) is compact, then any open cover
of \(K\) would form an open cover of \(X\) if augmented by \(K^\mathsf{c}\).
This implies that a finite subcover of \(K\) exists.
</p>

<p>
Assume \(K\) is compact and \(X\) is Hausdorff. Pick any \(p\in
K^\mathsf{c}\).  For any \(q \in K\), there exists a neighborhood \(V_q\)
of \(q\) and a neighborhood \(W_q\) of \(p\) such that \(V_q \cap W_q =
\emptyset\). By the compactness of \(K\), there exists finite many \(q_i\)
such that \(\bigcup_{i=1}^n V_{q_i}\supset K\).  This implies that
\(\bigcap_{i=1}^n W_{q_i}\subset K^\mathsf{c}\) is a neighborhood of
\(p\). Hence, \(p\) is an interior point of \(K^\mathsf{c}\).
</p>

<p>
Q.E.D.
</p>

<p>
<i>Proof to <a href="#org9891839">Theorem 8.11</a>.</i>
</p>

<p>
Compact \(\Rightarrow\) countably compact. Obviously.
</p>

<p>
Sequentially compact \(\Rightarrow\) countably compact.
If not, there exists a countable open cover
\((V_i)_{i=0}^\infty\)
of \(X\) which has no finite subcover.
Let \(x_k\in\bigl(\bigcup_{i=0}^k V_i\bigr)^\mathsf{c}\).
Clearly, the sequence \((x_k)\) has no convergent subsequence.
This contradicts with the hypothesis.
</p>

<p>
Countably compact \(\Rightarrow\) pseudocompact.
For any real-valued continuous function \(f:X\to\mathbb{R}\),
\((f^{-1}(-n,n))_{n=1}^{\infty}\) is a countable open cover of \(X\),
and thus has a finite subcover. WLOG, assume the finite subcover
is \((f^{-1}(-n,n))_{n=1}^{M}\). Clearly, \(f\) is bounded by \(M\).
</p>

<p>
Q.E.D.
</p>

<p>
<i>Proof to <a href="#org52049be">Lemma 8.12</a>.</i>
First, we prove that \(X\) <span class="underline">is countably compact if and only if every sequence
has a cluster point</span>. The <i>if</i> part has been proved in Theorem 8.11. To prove
the <i>only if</i> part, assume \(X\) be countably compact and \((x_n)\) is a sequence
with no cluster point. Let \(T_n\) be the tail set of \((x_n)\):
\[T_n:=\{x_k\mid k\geq n\}.\]
Let \(\overline{T}_n\) be the closure of \(T_n\). Since \(X\) is countably compact,
the countable family of closed sets \(\{\overline{T}_n \mid n\in\mathbb{N}\}\)
must have
\[\bigcap_{n=0}^\infty\overline{T}_n\neq\emptyset.\]
Pick \(x\in \bigcap_{n=0}^\infty\overline{T}_n\).
For any \(n\), there is \(x\in \overline{T}_n\).
Hence, for any neighborhood \(N_x\) of \(x\), there is \(N_x\cap T_n\neq\emptyset\).
Recalling the definition of \(T_n\), we conclude that \(x\) is a cluster point
of \((x_n)\). However, this contradicts with the hypothesis that \((x_n)\) has
no cluster point. We finish the <i>only if</i> part.
</p>

<p>
Then we prove this lemma.
Let \(X\) be countably compact and first countable.
For any sequence \((x_n)\), it has a cluster point \(x\).
There must exist a subsequence of \((x_n)\) which converges to \(x\).
</p>

<ul class="org-ul">
<li>Let \((B_k)\) be a countable shrinking neighborhood base at \(x\).
Since \((x_n)\) is frequently in \(B_1\), we can pick \(n_1\) so that \(x_{n_1}\in B_1\).
Since \((x_n)\) is frequently in \(B_2\), we can pick \(n_2 > n_1\) so that \(x_{n_2}\in B_2\).
Continue inductively: having chosen \(n_1 < n_2 < \cdots < n_k\) so that
\(x_{n_k}\in U_k \subset U_{k-1} \subset \cdots U_1\), we can then choose \(n_{k+1} > n_k\)
so that \(x_{n_{k+1}}\in U_{k+1}\subset U_k\). Clearly, \((x_{n_k})_{k=1}^\infty\) converges
to \(x\).</li>
</ul>

<p>
In conclusion, if \(X\) is countably compact, then every sequence has a cluster point.
Since \(X\) is first countable, we conclude that every sequence has a convergent
subsequence.
</p>

<p>
Q.E.D.
</p>

<p>
<i>Proof to <a href="#orgb017007">Lemma 8.16</a>.</i>
Let \((X,d)\) be a totally bounded pseudometric space.
</p>

<p>
First, we prove that <span class="underline">a totally bounded pseudometric space is
separable</span>. For each \(n\in\mathbb{N}\), there exists finite many points
\(x^{(n)}_1,x^{(n)}_2,\ldots x^{(n)}_{k_n}\) such that \(X\) can be
covered by \(\frac{1}{n}\)-balls centered at these points.  We claim
that \[ E:=\bigcup_{n=1}^\infty \{x^{(n)}_i\mid 1\leq i\leq k_n\} \]
is a dense subset of \(X\).
</p>

<ul class="org-ul">
<li>For any \(x\in X\) and arbitrary small \(\epsilon > 0\),
we can find \(x^{(n)}_i\in E\) such that \(d( x^{(n)}_i, x ) < \epsilon\).
This is done by choosing \(n > \frac{1}{\epsilon}\) and \(i=1\).</li>
</ul>

<p>
Then, we prove that <span class="underline">a separable pseudometric space is second countable</span>.
Let \(D=\{x_k\mid x\in\mathbb{N}\}\) be a dense subset of \(X\).
We claim that
\[
\mathcal{O} := \bigcup_{k=1}^\infty \{B_{\frac{1}{n}}(x_k)\mid n\in\mathbb{N}\}
\]
is a countable topological base.
</p>

<ul class="org-ul">
<li>For any \(x\in V\in\mathcal{T}_d\), there exists some \(\epsilon > 0\)
such that \(B_\epsilon(x)\subset V\). As \(D\) is dense in \(X\), there is
\(x_k\in D\) such that \(d(x_k, x) < \frac{\epsilon}{2}\). Choose \(n\)
such that \(\frac{1}{n} < \frac{\epsilon}{2}\), we have
\(B_{\frac{1}{n}}(x_k)\subset B_\epsilon(x)\subset V\).</li>
</ul>

<p>
Finally, we prove that <span class="underline">a second countable pseudometric space is Lindel√∂f</span>.
Let \(\mathcal{B}\) be a countable base of \(\mathcal{T}\), and let \(\mathcal{U}\)
be an arbitrary open cover of \(X\). For any \(x\in X\),
there exists \(U_x\in\mathcal{U}\) such that \(x\in U_x\).
Since \(\mathcal{B}\) is a base, for each \(x\), there exists a \(B_x\in\mathcal{B}\)
such that \(x\in B_x\subset U_x\). Therefore,
\[
\mathcal{V}:=\bigcup_{x\in X}B_x
\]
forms an open cover of \(X\). However, \(\mathcal{V}\subset\mathcal{B}\) must be
countable. Hence, \(\mathcal{V}\) can be represented as
\[
\mathcal{V}:=\bigcup_{i=1}^\infty B_{x_i}.
\]
We conclude that \(\bigcup_{i=1}^\infty U_{x_i}\) is a countable subcover.
</p>

<p>
Q.E.D.
</p>

<p>
<i>Proof to <a href="#orge3f6358">Theorem 8.17</a>.</i>
Based on Theorem 8.11, we need only to prove the following implications.
</p>

<dl class="org-dl">
<dt>Countably compactness implies sequentially compactness</dt><dd>As any pseudometric space is first countable,
then countably compactness implies sequentially compactness by
Lemma 8.12.</dd>
<dt>Countably compactness implies compactness</dt><dd><p>
By Lemma 8.16, any totally bounded pseudometric space is Lindel√∂f.
Hence, it is sufficient to prove that a countably pseudometric space
is totally bounded.
</p>

<p>
If a countably compact pseudometric space is not totally bounded,
then there exists \(\epsilon > 0\) such that \(X\) cannot be covered
by finite many \(\epsilon\)-balls. Obviously, \(X\) is nonempty.
Pick \(x_1 \in X\). As \(\{B_\epsilon(x_1)\}\) cannot cover \(X\),
we can pick \(x_2\in X\) such that \(d(x_2,x_1) \geq \epsilon\).
Again, as \(\{B_\epsilon(x_1), B_\epsilon(x_2)\}\) cannot cover \(X\),
we can pick \(x_3\in X\) such that \(d(x_3,x_1) \geq \epsilon\)
and \(d(x_3,x_2)\geq \epsilon\). Continue inductively,
we may construct a sequence \((x_n)\) such that \(d(x_i,x_j) \geq \epsilon\)
for each \(i\neq j\). Clearly, this sequence has no convergent subsequence.
However, since we have proved that
<i>countably compactness implies sequentially compactness</i>, \((x_n)\) must have a
convergent subsequence, leading to a contradiction.
</p></dd>
<dt>Pseudocompactness implies countably compactness</dt><dd>Assume \(X\) is pseudocompact but is not countably compact.
As \(X\) is not countably compact, there exists a sequence \((x_n)\) with
no cluster point.

<ul class="org-ul">
<li>STEP I. Ensure \(d(x_n,x_m) > 0\) for all \(n\neq m\). If not, we may
pick a subsequence \((x_{a_k})\) such that \(d(x_{a_n},x_{a_m}) > 0\)
for all \(n\neq m\).

<ul class="org-ul">
<li>The subsequence is constructed by observing
the following fact: /for any \(n\), the set \[ E_n:= \{m > n \mid
      d(x_m,x_n) =0\} \] must be finite. Otherwise, there would be
convergent subsequence, and contradicts with the assumption that
\((x_n)\) has no cluster point.</li>
</ul></li>

<li>STEP II. Construct a sequence of open sets \((U_n)_{n=1}^\infty\)
such that 1) \(x_n\in U_n\); 2) \(U_i\cap U_j=\emptyset\) if \(i\neq
    j\); 3) \(\mathrm{diam}\,U_n\to 0\).

<ul class="org-ul">
<li>For any \(x_m\), there exists an open ball \(B_{\delta_m}(x_m)\)
containing only finte many \(x_n\)'s (since \(x_m\) is not a cluster
point of \((x_n)\)). Because \(d(x_n,x_m) > 0\) for all \(n\neq m\),
the \(\delta_m\) may be shrunk such that \(B_{\delta_m}(x_m)\)
contains no other \(x_n\)'s except \(x_m\). In other words,
\[
      d(x_m, x_n) \geq \delta_m,\qquad\forall n\neq m.
      \]
Let \(\epsilon_m=\min(\delta_m/3, \frac{1}{m})\). Then we claim
that \(U_n=B_{\epsilon_n}(x_n)\) forms the desired sequence of
open sets. Clearly, \(x_n\in U_n\) and
\(\operatorname{diam}U_n\to0\). To see that \(U_n\cap
      U_m=\emptyset\) if \(n\neq m\), we need only to note that
\(\epsilon_n + \epsilon_m \leq \delta_n/3 + \delta_m/3 <
      d(x_n,x_m)\).</li>
</ul></li>

<li>STEP III. Show \(f_n(x)=n\frac{d(x_n,U_n^\mathsf{c})}{d(x_n,U_n^\mathsf{c})}\)
and \(f=\sum_{n=1}^\infty f_n\) are well defined.

<ul class="org-ul">
<li>As \(x_n\not\in U_n^\mathsf{c}\) and \(U_n^\mathsf{c}\) is closed,
the distance \(d(x_n,U_n^\mathsf{c}\) cannot be 0. Moreover,
\(d(x,U_n^\mathsf{c}\neq 0\) if and only \(x\in U_n\). Hence,
for any \(x\), there exists at most one \(f_n\) such that
\(f_n(x)\neq 0\). Therefore, \(f=\sum_n f_n\) is finite at any
\(x\).</li>
</ul></li>

<li>STEP IV. Prove that \(f\) is an unbounded continuous function.

<ul class="org-ul">
<li><p>
Since \(f(x_n)=f_n(x_n)=n\), we have \(f\) is unbounded. To prove
\(f\) is continuous, we first note that \(f_n\) is continuous.
</p>

<ul class="org-ul">
<li>If \(d\) is a metric on \(X\) and \(E\) is a subset of \(X\), then
for any \(x,y\in X\), there is
\[
        d(x,E) \leq d(y,E) + d(x,y).
        \]
Therefore, \(d(\cdot,E)\) must be continuous.
(If \(d(x_n,x)\to0\), then \(|d(x_n,E)-d(x,E)|\leq d(x_n,x)\to0\).)</li>
</ul>

<p>
Then we prove that for any \(a\in X\), there exists a open set
\(V_a\) such that \(f|_{V_a}=\sum_{n=1}^Nf_n\) for some \(N\).
</p>

<ul class="org-ul">
<li>If \(d(a,x_n)=0\) for some \(n\), then \(V_a\) can be set to \(U_n\) and
\(N=n\).</li>

<li>Suppose \(d(a,x_n)>0\) for all \(n\). Since \(a\) is not a cluster
point of \(x_n\), there exists an open ball \(B_\delta(a)\)
containing no \(x_n\). In other words,
\[
        d(a,x_n) \geq \delta,\qquad\forall n.
        \]
Let \(V_a=B_{\delta/2}(a)\). Then for any \(x\in V_a\),
\[
        d(x,x_n) \geq d(a,x_n) - d(a,x) \geq \delta/2,\qquad\forall n.
        \]
Recalling that \(\operatorname{diam}U_n\to0\),
there must exist \(N\) such that for all \(n\geq N\),
\(\operatorname{diam}U_n < \delta/2\).
Therefore, for any \(x\in V_a\), we have \(x\not\in U_n\) for all
\(n\geq N\). In otherwords, \(f_n(x)=0\) for all \(n\geq N\) if
\(x\in V_a\).</li>
</ul>

<p>
Therefore, \(f\) is continous at any point \(a\).
</p></li>
</ul></li>
</ul></dd>
</dl>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
An open cover of \(X\) is a family of open sets \(\mathcal{O}\)
such that \(\bigcup\mathcal{O} \supset X\).
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
A family \(\mathcal{F}\) of sets with finite intersection property
if every finite subfamily of \(\mathcal{F}\) has nonempty intersection.
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3" role="doc-backlink">3</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Compactness is a property of topological spaces.
It is different from the definition of closed sets.
We can say a topological space is compact,
but it make no sense to say a topological space is closed.
</p></div></div>


</div>
</div><div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-math.html">math</a> </div>
]]></description>
  <category><![CDATA[math]]></category>
  <link>https://dou-meishi.github.io/org-blog/2023-09-19-Compactness/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2023-09-19-Compactness/notes.html</guid>
  <pubDate>Tue, 19 Sep 2023 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Optimality Conditions in Convex Optimization]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org0b251b8">Lagrangian and KKT Points</a></li>
<li><a href="#org7cdc0e7">Dual Problem</a></li>
</ul>
</div>
</nav>
<p>
Consider the following constrained optimization problem
\[\begin{aligned}
\min_{x\in\mathbb{R}^n}\quad & f(x) \\
\mathrm{s.t.}\quad & c_i(x) = 0,\qquad i\in\mathcal{E},\\
&c_i(x) \geq 0,\qquad i\in\mathcal{I}.
\end{aligned}\]
Here, \(f\) is a convex function, and
\(\{c_i \mid i\in\mathcal{E}\cup\mathcal{I}\}\) are linear functions. This
problem encompasses linear programming and quadratic programming, and
represents a special case of general convex optimization problems.
Therefore, it serves as an good starting point for learning about
optimization methods.
</p>

<p>
Let us start by some notations and terminologies.
</p>

<table>


<colgroup>
<col  class="org-left">

<col  class="org-left">

<col  class="org-left">
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Concept</th>
<th scope="col" class="org-left">Notation</th>
<th scope="col" class="org-left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Feasible set</td>
<td class="org-left">\(\Omega\)</td>
<td class="org-left">the set of \(x\) satisfying all constraints</td>
</tr>

<tr>
<td class="org-left">Minimizer</td>
<td class="org-left">\(x^\ast\)</td>
<td class="org-left">the smallest feasible point in its neighbors</td>
</tr>

<tr>
<td class="org-left">Mimimum</td>
<td class="org-left">\(f^\ast\)</td>
<td class="org-left">\(f(x^\ast)\)</td>
</tr>
</tbody>
</table>

<p>
Below is a very basic property of minimizers.
</p>

<p>
<i>Minimizer is stable.</i> If \(x^\ast\) is a minimizer of the considered
optimization problem, then
\[ \langle \nabla f(x^\ast), x-x^\ast \rangle \geq 0, \qquad \forall x \in\Omega.\]
</p>

<p>
In general, if \(f\) is convex, then for any two points \(x_1\) and \(x_2\),
there is
\[ f(x_2) \geq f(x_1) + \langle \nabla f(x_1), x_2 - x_1\rangle. \]
Therefore, for the considered problem any stable point is also a
minimizer.
</p>
<div id="outline-container-org0b251b8" class="outline-2">
<h2 id="org0b251b8">Lagrangian and KKT Points</h2>
<div class="outline-text-2" id="text-org0b251b8">
<p>
The <i>Lagrangian</i> of the considered problem is defined by
\[ L(x, u, v) := f(x) - u^\intercal \mathbf{g}(x) - v^\intercal \mathbf{h}(x),
\qquad x\in\mathbb{R}^n, u\in\mathbb{R}_+^{|\mathcal{I}|}, v\in\mathbb{R}^{|\mathcal{E}|},\]
where \(\mathbf{g}(x)\geq 0\) is the collection of inequality constraints
and \(\mathbf{h}(x)=0\) is the collection of equality constraints. It is
important to note that by writting \(L(x,u,v)\) the multiplier \(u\)
associated with inequality constraints is required to be nonnegative.
</p>

<p>
A KKT point \((x^\ast, u^\ast, v^\ast)\) is a point in the domain of
Lagrangian \(L\) which satsifies the following set of conditions (known as
KKT conditions) \[\begin{cases}
\nabla_{x}L(x^\ast, u^\ast, v^\ast) = 0, \\
\nabla_{u}L(x^\ast, u^\ast, v^\ast) \leq 0, \\
\nabla_{v}L(x^\ast, u^\ast, v^\ast) = 0, \\
\langle \nabla_{u}L(x^\ast, u^\ast, v^\ast), u^\ast\rangle = 0.
\end{cases}\] The last equality is known as the <i>Complementary
slackness</i> condition. In addition, \(u^\ast\geq 0\) is included implicitly
by writting \(L(x^\ast,u^\ast,v^\ast)\).
</p>

<p>
For the considered problem. KKT conditions are necessary.
</p>

<p>
<b>KKT conditions are necessary.</b> <i>If \(x^\ast\) is a minimizer of the
considered problem, then there exists
\(u^\ast\in\mathbb{R}_+^{|\mathcal{I}|}\) and
\(v^\ast\in\mathbb{R}^{|\mathcal{E}|}\) such that
\((x^\ast, u^\ast, v^\ast)\) is a KKT point.</i>
</p>

<blockquote>
<p>
The proof, which utilizes Farkas' lemma, is omitted.
</p>
</blockquote>

<p>
It turns out that KKT conditions are also sufficient to ensure
optimality. To see this, we need to introduce a useful concept <i>saddle
points of Lagrangian</i>.
</p>

<p>
<i>A saddle point of Lagrangian</i> is a point \((x^\ast, u^\ast, v^\ast)\)
which satisfying that
\[ L(x^\ast, u, v) \leq L(x^\ast, u^\ast, v^\ast) \leq L(x, u^\ast, v^\ast),\qquad\forall x, u, v.\]
It is implicit in this definition that \(u^\ast\geq0\) and \(u\geq0\), since
this requirement follows from writing out \(L(x^\ast, u^\ast, v^\ast)\)
and \(L(x^\ast, u, v)\). This convention will be assumed throughout,
unless otherwise specified.
</p>

<p>
The saddle point condition is the most restrictive condition for a
convex optimization problem.
</p>

<p>
<i>Any saddle point is a KKT point, and moreover, any saddle point is a
global minimizer.</i>
</p>

<blockquote>
<p>
<i>Proof.</i> Suppose \((x^\ast,u^\ast,v^\ast)\) is a saddle point. Fix
\(u^\ast,v^\ast\), \(L(\cdot,u^\ast,v^\ast)\) has a minimizer \(x^\ast\).
Therefore \[ \nabla_xL(x^\ast,u^\ast,v^\ast) = 0.\] Fix
\(x^\ast,u^\ast\), \(L(x^\ast,u^\ast,\cdot)\) has a maximizer \(v^\ast\).
Therefore \[ \nabla_vL(x^\ast,u^\ast,v^\ast) = 0.\] Fix
\(x^\ast,v^\ast\), \(L(x^\ast, u, v^\ast)\) has a maxmizer \(u^\ast\) on
\(\mathbb{R}_+^{|\mathcal{I}|}\). Therefore
\[ \langle \nabla_uL(x^\ast,u^\ast,v^\ast), u-u^\ast\rangle \leq 0,\qquad\forall u\geq 0.\]
By choosing \(u=u^\ast+\epsilon_i\), where \(\epsilon_i\) is a unit vector
with only one nonzero component,
\[ \nabla_uL(x^\ast,u^\ast,v^\ast) \leq 0.\] Moreover, by choosing
\(u=0\) and \(u=2u^\ast\), it is clear that
\[ \langle \nabla_uL(x^\ast,u^\ast,v^\ast), u^\ast\rangle = 0.\] This
concludes that \((x^\ast,u^\ast,v^\ast)\) is a KKT point.
</p>

<p>
As \((x^\ast,u^\ast,v^\ast)\) satisfies the KKT conditions, it holds
that \[\mathbf{g}(x^\ast)\geq0,\qquad\mathbf{h}(x^\ast)=0.\] This
implies that \(x^\ast\in\Omega\). In addition, KKT conditions imply that
\[(u^\ast)^\intercal \mathbf{g}(x^\ast) = 0,\qquad (v^\ast)^\intercal \mathbf{h}(x^\ast) = 0.\]
For any \(x\in\Omega\), there is
\[(u^\ast)^\intercal \mathbf{g}(x) \geq 0, \qquad (v^\ast)^\intercal \mathbf{h}(x) = 0.\]
Hence,
\[ f(x) \geq L(x, u^\ast, v^\ast) \geq L(x^\ast, u^\ast, v^\ast) = f(x^\ast),\]
where the second inequality uses the fact that
\((x^\ast,u^\ast,v^\ast)\) is a saddle point. This concludes that
\((x^\ast,u^\ast,v^\ast)\) is a global minimizer.
</p>

<p>
Q.E.D.
</p>
</blockquote>

<p>
Finally, we can prove the sufficiency of KKT conditions by showing that
for the considered problem any KKT point is a saddle point.
</p>

<p>
<b>KKT conditions are sufficient.</b> <i>For the considered problem, if
\((x^\ast, u^\ast, v^\ast)\) is a KKT point, then it is also a saddle
point of Lagrangian. Consequently, it is a global minimizer.</i>
</p>

<blockquote>
<p>
<i>Proof.</i> On one hand, \((u^\ast, v^\ast)\) is a maximizer of
\(L(x^\ast, \cdot, \cdot)\) on
\(\mathbb{R}^{|\mathcal{I}|}_+\times\mathbb{R}^{|\mathcal{E}|}\) because
\(\mathbf{g}(x^\ast)\geq 0\) and \(\mathbf{h}(x^\ast)=0\).
</p>

<p>
On the otherhand, \(x^\ast\) is a minimizer of
\(L(\cdot, u^\ast, v^\ast)\) on \(\mathbb{R}^n\) because it is convex and
there is \(\nabla_xL(x^\ast, u^\ast, v^\ast)=0\).
</p>

<p>
Q.E.D.
</p>
</blockquote>
</div>
</div>
<div id="outline-container-org7cdc0e7" class="outline-2">
<h2 id="org7cdc0e7">Dual Problem</h2>
<div class="outline-text-2" id="text-org7cdc0e7">
<p>
Recall the definition of Lagrangian
\[ L(x, u, v) := f(x) - u^\intercal \mathbf{g}(x) - v^\intercal \mathbf{h}(x),
\qquad x\in\mathbb{R}^n, u\in\mathbb{R}_+^{|\mathcal{I}|}, v\in\mathbb{R}^{|\mathcal{E}|}.\]
It is not hard to show that
\[ \max_{\substack{u\in\mathbb{R}_+^{|\mathcal{I}|}\\ v\in\mathbb{R}^{|\mathcal{E}|}}} L(x, u, v) =
\begin{cases}
\infty, \qquad \mathrm{if}\ x\not\in\Omega,\\
f(x),\qquad\mathrm{if}\ x\in\Omega.
\end{cases}\] Hence, the original optimization problem (referred to as
the <i>primal problem</i> below) can be rewritten as
\[ \min_{x\in\mathbb{R}^n}\max_{\substack{u\in\mathbb{R}_+^{|\mathcal{I}|}\\ v\in\mathbb{R}^{|\mathcal{E}|}}} L(x, u, v).\]
The <i>Dual problem</i> is then defined by
\[ \max_{\substack{u\in\mathbb{R}_+^{|\mathcal{I}|}\\ v\in\mathbb{R}^{|\mathcal{E}|}}} \min_{x\in\mathbb{R}^n} L(x, u, v).\]
</p>

<p>
Dual problem has the following properties:
</p>

<ul class="org-ul">
<li>the objective function \(\min_{x}L(x,u,v)\) is concave, regardless of
the convexity of \(f, \mathbf{g}\) and \(\mathbf{h}\).</li>
<li>the optimal objective value is not greater than the optimal value of
the primal problem.</li>
</ul>

<p>
The first property is expected because \(u\) and \(v\) appear linear in \(L\),
and the \(\min\) operator does not break the concavity. The second
property, also known as the <i>weak duality</i>, is a direct consequence of
the following general proposition.
</p>

<p>
<b>Min Max is greater or equal to Max Min</b>. <i>For any function
\(f: X\times Y \to \mathbb{R}\), the following inequality holds trivially</i>
\[ \min_{x\in X}\max_{y\in Y} f(x, y) \geq \max_{y\in Y}\min_{x\in X} f(x, y).\]
</p>

<p>
The <i>duality gap</i> is then defined as the difference between the optimal
values between the primal and dual problems. We have seen that this gap
is always nonnegative. If the duality gap is zero, then we say that
<i>strong duality</i> holds.
</p>

<p>
<b>Strong duality holds if constraints are linear.</b> <i>For the considered
problem, where \(f\) is convex and constraints are linear, the strong
duality holds if any minimizer of the primal problem exists.</i>
</p>

<blockquote>
<p>
<i>Proof.</i> Assume \(x^\ast\) is a minimizer of the primal problem. Due to
the necessity of KKT conditions, there exists \(u^\ast\) and \(v^\ast\)
such that \((x^\ast, u^\ast, v^\ast)\) forms a KKT point. In addition,
we have proved that, for the considered problem, any KKT point is also
a saddle point of Lagrangian. Hence, \[ \begin{aligned}
  f(x^\ast) &= \min_{x}\max_{u\geq0, v} L(x, u, v)& \qquad&  \\
  &\geq \max_{u\geq 0, v}\min_{x}L(x, u, v) & \qquad&\textsf{(weaker duality)} \\
  &\geq \min_{x}L(x, u^\ast, v^\ast)&\qquad& \\
  &= L(x^\ast, u^\ast, v^\ast) &\qquad&\textsf{(saddle point)} \\
  &= f(x^\ast)&\qquad &\textsf{(KKT point)}.
  \end{aligned} \]
</p>

<p>
Q.E.D.
</p>
</blockquote>
</div>
</div>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-math.html">math</a> </div>
]]></description>
  <category><![CDATA[math]]></category>
  <link>https://dou-meishi.github.io/org-blog/2023-04-30-OptimalityandKKTCondition/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2023-04-30-OptimalityandKKTCondition/notes.html</guid>
  <pubDate>Sun, 30 Apr 2023 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Detect Blackbox Running Environments in Algorithm Contests]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org2f5a6b2">Mathematical Analysis</a>
<ul>
<li><a href="#org32843b7">A simple case study</a></li>
<li><a href="#the-revised-simple-case">The revised simple case</a></li>
</ul>
</li>
<li><a href="#a-demo-determine-the-version-of-scipy">A Demo: determine the version of SciPy</a></li>
<li><a href="#discussion">Discussion</a></li>
</ul>
</div>
</nav>
<p>
<b>Authors: Dou Meishi, ChatGPT</b>
</p>

<p>
In some commercial algorithm contests, participants are required to
upload their code to a secure platform to check their results. To
protect the test case information, participants can only receive the
results of their code execution. However, certain information, such as
dependency details and package versions, is essential for developing
specific projects. This post provides a systematic way to collect
useful information about the blackbox running environment by analyzing
the execution results of carefully crafted scripts.
</p>
<div id="outline-container-org2f5a6b2" class="outline-2">
<h2 id="org2f5a6b2">Mathematical Analysis</h2>
<div class="outline-text-2" id="text-org2f5a6b2">
</div>
<div id="outline-container-org32843b7" class="outline-3">
<h3 id="org32843b7">A simple case study</h3>
<div class="outline-text-3" id="text-org32843b7">
<p>
Given a blackbox environment, we aim to detect the true value of a
single target variable \(X\), which can take values in a set \(U\). The
environment can answer queries about \(X\), but the feedback is given in
terms of another variable \(Y\), taking values in a set \(V\). The challenge
is to design a procedure that translates query outputs of \(X\) into
observations of \(Y\) to deduce the true value of \(X\).
</p>

<p>
To be specific, let us study the following simple case where \(X\) takes
discrete values in \(U=\{0, 1, 2, \ldots, 99\}\), and \(Y\) takes boolean
values in \(V=\{0, 1\}\).
</p>

<p>
We may apply the following procedure to determine the true value \(X^*\).
The basic idea is representing \(X^*\) via 7 bits of data and retrieve one
bit at a time through the value of \(Y\).
</p>

<ul class="org-ul">
<li>Repeat the following steps 7 times to generate 7 observations of \(Y\),
denoted by \(y_0, y_1, \ldots, y_6\):

<ul class="org-ul">
<li>For each iteration \(k = 0, 1, \ldots, 6\):

<ul class="org-ul">
<li>Determine the $k$-th bit of \(X^*\): set
\(Y = ( \lfloor \frac{X}{2^k} \rfloor ) \bmod 2\).</li>

<li>Emit \(Y\) as an observation and collect the $k$-th observation as
\(y_k\).</li>
</ul></li>
</ul></li>

<li>Analyze the observations and recover the true value of \(X^*\):

<ul class="org-ul">
<li>Set \(X = \sum_{k=0}^{6} y_k 2^k\).</li>

<li>Return \(X\) as the determined value of \(X^*\).</li>
</ul></li>
</ul>

<p>
In the provided example, we used the binary representation of \(X^*\) to
deduce its true value with a finite number of observations. Since any
integer can be uniquely represented using its binary representation, we
can generalize the procedure to any target variable \(X\) with \(n\)
possible values, which is nothing but a direct usage of the following
formula
</p>

<p>
\[ n = \sum_{k=0}^{N-1} y_k 2^k, \qquad\forall n = 0, 1, \ldots, 2^{N}-1.\]
</p>

<p>
<i>Fact I.</i> Given a target variable \(X\) that takes discrete values within
a set containing \(n\) elements, and a blackbox environment that can emit
at least two distinct states as observations, the true value \(X^*\) can
be determined with \(\lceil\log_2 n\rceil\) observations.
</p>

<p>
This fact could be easily extended in terms of the number of distinct
states.
</p>

<p>
<i>Fact II.</i> Given a target variable \(X\) that takes discrete values within
a set containing \(n\) elements, and a blackbox environment that can emit
\(m\) distinct states as observations, the true value \(X^*\) can be
determined with \(\lceil \frac{\log_2 n}{\log_2 m} \rceil\) observations.
</p>
</div>
</div>
<div id="outline-container-the-revised-simple-case" class="outline-3">
<h3 id="the-revised-simple-case">The revised simple case</h3>
<div class="outline-text-3" id="text-the-revised-simple-case">
<p>
Let's consider the previous example again. But this time assume the
target variable \(X\) is continous. Without loss of generality, we assume
\(X\) takes value in \(U=[0, 1]\). For continous variable, obtaining its
exact value is not reasonable. However, it is possible to narrow down
the range set \(U\) to a much smaller set \(U_0\subset U\) and ensure
\(X^*\in U_0\).
</p>

<ul class="org-ul">
<li>Repeat the following steps N times to generate N observations of \(Y\),
denoted by \(y_0, y_1, \ldots, y_{N-1}\):

<ul class="org-ul">
<li>For each iteration \(k = 0, 1, \ldots, N-1\):

<ul class="org-ul">
<li>Determine the $k$-th bit of \(X^*\): set
\(Y = ( \lfloor X\times 2^{k+1} \rfloor ) \bmod 2\).</li>

<li>Emit \(Y\) as an observation and collect the $k$-th observation as
\(y_k\).</li>
</ul></li>
</ul></li>

<li>Analyze the observations and recover the true value of \(X^*\):

<ul class="org-ul">
<li>Set \(X = 2^{-(N+1)} + \sum_{k=0}^{N-1} y_k 2^{-(k+1)}\).</li>

<li>Return \(X\) as the determined value of \(X^*\).</li>
</ul></li>
</ul>

<p>
In view of the following formula
</p>

<p>
\[ x = \sum_{k = 0}^{N-1} y_k 2^{-(k+1)} + \sum_{k=N}^{\infty} y_k 2^{-(k+1)},\qquad\forall x\in[0,1],\]
</p>

<p>
the following fact is clearly true.
</p>

<p>
<i>Fact III.</i> Given a target variable \(X\) that takes values within the
continuous set \([0, 1]\), and a blackbox environment that can emit at
least two distinct states as observations, an approximation \(X\) of the
true value \(X^*\) can be obtained with \(N\) observations to ensure the
approximation error \(|X-X^*| \leq 2^{-N-1}\).
</p>

<p>
This fact could be extended to \(m\) distinct states similarily.
</p>

<p>
<i>Fact IV.</i> Given a target variable \(X\) that takes values within the
continuous set \([0, 1]\), and a blackbox environment that can emit at
least \(m\) distinct states as observations, an approximation \(X\) of the
true value \(X^*\) can be obtained with \(N\) observations to ensure the
approximation error \(|X-X^*| \leq m^{-N-1}\).
</p>
</div>
</div>
</div>
<div id="outline-container-a-demo-determine-the-version-of-scipy" class="outline-2">
<h2 id="a-demo-determine-the-version-of-scipy">A Demo: determine the version of SciPy</h2>
<div class="outline-text-2" id="text-a-demo-determine-the-version-of-scipy">
<p>
Let us consider a scenario where we are participating in an algorithm
contest. The contest organizer provides a secure platform to execute our
code and return the result: (1) a score between 0 and 100, if our code
executes successfully, and (2) a warning indicating the failure to
execute our code. Our objective is to determine the version of SciPy in
the Python environment being used to run our code.
</p>

<p>
Currently, the version name <code>x.y.z</code> consists of
</p>

<ul class="org-ul">
<li>a major name <code>x</code>, which takes value in \(\{0, 1\}\);</li>
<li>a minor name <code>y</code>, which takes value in \(\{0, 1, \ldots, 19\}\);</li>
<li>a micro name <code>z</code>, which takes value in \(\{0, 1, \ldots, 9\}\).</li>
</ul>

<p>
See <a href="https://scipy.org/news/#releases">SciPy Release News</a> for a
complete release history.
</p>

<p>
A binary search requires at most 1 observation to determine <code>x</code>, at most
5 observations to determine <code>y</code> and at most 4 observations to determine
<code>z</code>
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">import</span> time

<span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">get_kbit</span>(n, k):
    <span style="color: #2aa198;">'''return the value of k-th bit of an integer n.</span>
<span style="color: #2aa198;">        n == sum(get_kbit(n, k) * 2**k for k in range(n))</span>
<span style="color: #2aa198;">    should hold trivially.'''</span>
    <span style="color: #859900; font-weight: bold;">return</span> (n // (2 ** k)) % 2

<span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">recover_from_bits</span>(bits):
    <span style="color: #2aa198;">'''restore n from outputs of get_kbit'''</span>
    <span style="color: #859900; font-weight: bold;">return</span> <span style="color: #657b83; font-weight: bold;">sum</span>(bk * 2**k <span style="color: #859900; font-weight: bold;">for</span> k, bk <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">enumerate</span>(bits))


<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">observation is simulated via exceptions</span>
<span style="color: #268bd2;">ObservationException</span> = <span style="color: #657b83; font-weight: bold;">type</span>(<span style="color: #2aa198;">'ObservationException'</span>, (<span style="color: #b58900;">BaseException</span>,), {})
<span style="color: #268bd2;">Observation0</span> = <span style="color: #657b83; font-weight: bold;">type</span>(<span style="color: #2aa198;">'Observation0'</span>, (ObservationException,), {})
<span style="color: #268bd2;">Observation1</span> = <span style="color: #657b83; font-weight: bold;">type</span>(<span style="color: #2aa198;">'Observation1'</span>, (ObservationException,), {})


<span style="color: #859900; font-weight: bold;">class</span> <span style="color: #b58900;">VersionQuerier</span>:

    <span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">__init__</span>(<span style="color: #859900; font-weight: bold;">self</span>, version: <span style="color: #657b83; font-weight: bold;">str</span>):
        <span style="color: #2aa198;">'''version should follow the pattern x.y.z'''</span>
        <span style="color: #859900; font-weight: bold;">self</span>.<span style="color: #268bd2;">version</span> = version

        <span style="color: #268bd2;">major</span>, <span style="color: #268bd2;">minor</span>, <span style="color: #268bd2;">micro</span> = version.split(<span style="color: #2aa198;">'.'</span>)

        <span style="color: #859900; font-weight: bold;">self</span>.<span style="color: #268bd2;">major</span> = <span style="color: #657b83; font-weight: bold;">int</span>(major)
        <span style="color: #859900; font-weight: bold;">self</span>.<span style="color: #268bd2;">minor</span> = <span style="color: #657b83; font-weight: bold;">int</span>(minor)
        <span style="color: #859900; font-weight: bold;">self</span>.<span style="color: #268bd2;">micro</span> = <span style="color: #657b83; font-weight: bold;">int</span>(micro)

    <span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">set_observation</span>(<span style="color: #859900; font-weight: bold;">self</span>, ob):
        <span style="color: #2aa198;">'''take an action to throw the corresponding observation.'''</span>
        <span style="color: #859900; font-weight: bold;">if</span> ob == 0:
            <span style="color: #859900; font-weight: bold;">raise</span> Observation0
        <span style="color: #859900; font-weight: bold;">elif</span> ob == 1:
            <span style="color: #859900; font-weight: bold;">raise</span> Observation1
        <span style="color: #859900; font-weight: bold;">else</span>:
            <span style="color: #859900; font-weight: bold;">raise</span> <span style="color: #b58900;">ValueError</span>

<span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">main</span>():
    <span style="color: #859900; font-weight: bold;">import</span> scipy

    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">inititliazation</span>
    <span style="color: #268bd2;">querier</span> = VersionQuerier(scipy.__version__)

    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">only the first constrol statement would be executed</span>
    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">comment those lines run before</span>

    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">check set_observation</span>
    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">querier.set_observation(0)</span>
    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">querier.set_observation(1)</span>
    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">querier.set_observation(2)</span>

    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">query major version name</span>
    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">querier.set_observation(get_kbit(querier.major, 0))     # output: 1</span>

    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">query minor version name</span>
    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">querier.set_observation(get_kbit(querier.minor, 0))     # output: 0</span>
    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">querier.set_observation(get_kbit(querier.minor, 1))     # output: 1</span>
    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">querier.set_observation(get_kbit(querier.minor, 2))     # output: 0</span>
    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">querier.set_observation(get_kbit(querier.minor, 3))     # output: 1</span>
    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">querier.set_observation(get_kbit(querier.minor, 4))     # output: 0</span>

    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">query micro version name</span>
    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">querier.set_observation(get_kbit(querier.micro, 0))     # output: 1</span>
    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">querier.set_observation(get_kbit(querier.micro, 1))     # output: 0</span>
    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">querier.set_observation(get_kbit(querier.micro, 2))     # output: 0</span>
    <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">querier.set_observation(get_kbit(querier.micro, 3))     # output: 0</span>

<span style="color: #859900; font-weight: bold;">if</span> <span style="color: #657b83; font-weight: bold;">__name__</span> == <span style="color: #2aa198;">'__main__'</span>:
    <span style="color: #859900; font-weight: bold;">try</span>:
        main()
    <span style="color: #859900; font-weight: bold;">except</span> Observation0 <span style="color: #859900; font-weight: bold;">as</span> e:
        <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">simulate a successful run with a particular score</span>
        <span style="color: #859900; font-weight: bold;">pass</span>
    <span style="color: #859900; font-weight: bold;">except</span> Observation1 <span style="color: #859900; font-weight: bold;">as</span> e:
        <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">simulate a failaure run due to some error of the code</span>
        <span style="color: #859900; font-weight: bold;">raise</span> e
    <span style="color: #859900; font-weight: bold;">except</span> <span style="color: #b58900;">BaseException</span> <span style="color: #859900; font-weight: bold;">as</span> e:
        <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">in case of any other errors</span>
        <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">simulate a failaure run due to time limit exceeded</span>
        time.sleep(5)
</pre>
</div>

<p>
The provided code defines a VersionQuerier class that simulates the
process of querying the version of SciPy installed in the environment.
It initializes the class with the actual version of SciPy and provides
methods to set and retrieve observations based on the k-th bit of each
part of the version number (major, minor, and micro).
</p>

<p>
The main function demonstrates how to use the VersionQuerier class by
querying the bits of the version number in sequence. This information
can be used to narrow down the range of possible version numbers.
</p>
</div>
</div>
<div id="outline-container-discussion" class="outline-2">
<h2 id="discussion">Discussion</h2>
<div class="outline-text-2" id="text-discussion">
<p>
If a contest organizer provides an upload limit of at least 20 times per
day and offers at least two distinct forms of feedback, a participant
can ascertain the true value of any integer variable once per day,
provided that it is not greater than \(10^6\). Furthermore, if the
participant can maintain stable occurrence of four different feedback
states (e.g., by observing their score instead of relying solely on
failed code submissions), the number of integer variables they can
determine will double. In general, this number grows linearly with
respect to the logarithm of the number of distinct feedbacks.
</p>

<p>
The procedure demonstrated in the previous section can be automated by
generating a script to be uploaded via another script, which can also
parse the result from the contest website in real time. Ultimately, this
leads to another standard problem: <i>the communication between two
systems</i>.
</p>
</div>
</div>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-think.html">think</a> </div>
]]></description>
  <category><![CDATA[think]]></category>
  <link>https://dou-meishi.github.io/org-blog/2023-04-28-DetectBlackBoxRunningEnvironment/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2023-04-28-DetectBlackBoxRunningEnvironment/notes.html</guid>
  <pubDate>Fri, 28 Apr 2023 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Simulating Swap Operations Without Modifying Data]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org69ecc87">Introduction</a></li>
<li><a href="#orgabd43f3">Background and Applications</a></li>
<li><a href="#org3735534">Problem Statement and Our Solution</a></li>
<li><a href="#explanation-of-the-implementation">Explanation of the Implementation</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#org1d506a5">Mathematical Justifications</a></li>
</ul>
</div>
</nav>
<p>
<b>Authors</b>: Dou Meishi, ChatGPT
</p>
<div id="outline-container-org69ecc87" class="outline-2">
<h2 id="org69ecc87">Introduction</h2>
<div class="outline-text-2" id="text-org69ecc87">
<p>
In many applications, it is often required to simulate swap operations
on a list of elements without actually modifying the underlying data.
This can be useful in various scenarios, such as when you need to
analyze the impact of different permutations on a given data structure
or when you want to maintain multiple views of the data with different
sorting orders.
</p>

<p>
In this blog post, we will discuss the problem of simulating swap
operations without modifying the data, explore the background and
applications, and provide a Python code example that demonstrates how to
implement this functionality using a simple class structure. Finally, we
will explain the reasons behind our implementation and conclude.
</p>
</div>
</div>
<div id="outline-container-orgabd43f3" class="outline-2">
<h2 id="orgabd43f3">Background and Applications</h2>
<div class="outline-text-2" id="text-orgabd43f3">
<p>
Swapping elements in a list is a fundamental operation in many
algorithms, such as sorting algorithms and combinatorial search
algorithms. However, there are situations where we want to simulate
these swaps without actually modifying the original data. Some possible
applications include:
</p>

<ol class="org-ol">
<li><b>Data visualization</b>: When working with interactive visualizations,
it is often necessary to display different views of the same data,
based on user interactions. By simulating swaps without modifying the
data, we can easily switch between different views without affecting
the underlying data.</li>

<li><b>Algorithm analysis</b>: Analyzing the performance of algorithms that
involve swapping elements can be done more efficiently by simulating
swaps without modifying the data. This allows us to observe the
impact of different permutations on the algorithm's performance
without the overhead of actually modifying the data structure.</li>

<li><b>Undo/redo functionality</b>: In some applications, like text editors or
image editing software, users may want to undo or redo certain
actions. By simulating swaps without modifying the data, we can
maintain a history of actions and easily revert to previous states
without affecting the original data.</li>
</ol>
</div>
</div>
<div id="outline-container-org3735534" class="outline-2">
<h2 id="org3735534">Problem Statement and Our Solution</h2>
<div class="outline-text-2" id="text-org3735534">
<p>
Suppose we have a list of elements and we want to simulate swap
operations on this list without modifying the actual data. We also want
to be able to retrieve the elements in their current order, reflecting
the simulated swap operations.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #268bd2;">data</span> = [<span style="color: #2aa198;">'alpha'</span>, <span style="color: #2aa198;">'beta'</span>, <span style="color: #2aa198;">'gamma'</span>, <span style="color: #2aa198;">'eta'</span>]
<span style="color: #268bd2;">vec</span> = Vector(data)

<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">swap 1-th and 3-th value</span>
vec.swap(1, 3)
<span style="color: #268bd2;">view</span> = vec.view
<span style="color: #268bd2;">expect_view</span> = [<span style="color: #2aa198;">'alpha'</span>, <span style="color: #2aa198;">'eta'</span>, <span style="color: #2aa198;">'gamma'</span>, <span style="color: #2aa198;">'beta'</span>]

<span style="color: #859900; font-weight: bold;">assert</span> <span style="color: #657b83; font-weight: bold;">all</span>(view[i]==expect_view[i] <span style="color: #859900; font-weight: bold;">for</span> i <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(<span style="color: #657b83; font-weight: bold;">len</span>(data)))

<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">swap 2-th and 3-th value</span>
vec.swap(2, 3)
<span style="color: #268bd2;">view</span> = vec.view
<span style="color: #268bd2;">expect_view</span> = [<span style="color: #2aa198;">'alpha'</span>, <span style="color: #2aa198;">'eta'</span>, <span style="color: #2aa198;">'beta'</span>, <span style="color: #2aa198;">'gamma'</span>]

<span style="color: #859900; font-weight: bold;">assert</span> <span style="color: #657b83; font-weight: bold;">all</span>(view[i]==expect_view[i] <span style="color: #859900; font-weight: bold;">for</span> i <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(<span style="color: #657b83; font-weight: bold;">len</span>(data)))

<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">restore data from view</span>
<span style="color: #268bd2;">restored_data</span> = [view[i] <span style="color: #859900; font-weight: bold;">for</span> i <span style="color: #859900; font-weight: bold;">in</span> vec.addr2name]

<span style="color: #859900; font-weight: bold;">assert</span> <span style="color: #657b83; font-weight: bold;">all</span>(data[i]==restored_data[i] <span style="color: #859900; font-weight: bold;">for</span> i <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(<span style="color: #657b83; font-weight: bold;">len</span>(data)))
</pre>
</div>

<p>
To address this problem, we propose the implementation of a class called
<code>Viewable_Mixin</code>. This class maintains two lists, <code>self.indices</code> and
<code>self.inverse_indices</code>, that store the forward and inverse mappings
between the view and the data, respectively. The <code>swap</code> method is used
to simulate swap operations on the view, while the <code>view</code> property
returns the current state of the view.
</p>

<p>
Below is the complete Python code that demonstrates how to simulate swap
operations without modifying the data using a simple class structure
called <code>Viewable_Mixin</code>:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">class</span> <span style="color: #b58900;">Viewable_Mixin</span>(<span style="color: #657b83; font-weight: bold;">object</span>):
    <span style="color: #2aa198;">'''Allow swap index without actually modifying the data.'''</span>

    <span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">__init__</span>(<span style="color: #859900; font-weight: bold;">self</span>, *args, **kws):
        <span style="color: #2aa198;">'''Assume data is a list'''</span>
        <span style="color: #657b83; font-weight: bold;">super</span>().__init__(*args, **kws)

        <span style="color: #859900; font-weight: bold;">self</span>.<span style="color: #268bd2;">indices</span> = <span style="color: #657b83; font-weight: bold;">list</span>(<span style="color: #657b83; font-weight: bold;">range</span>(<span style="color: #657b83; font-weight: bold;">len</span>(<span style="color: #859900; font-weight: bold;">self</span>)))
        <span style="color: #859900; font-weight: bold;">self</span>.<span style="color: #268bd2;">inverse_indices</span> = <span style="color: #657b83; font-weight: bold;">list</span>(<span style="color: #657b83; font-weight: bold;">range</span>(<span style="color: #657b83; font-weight: bold;">len</span>(<span style="color: #859900; font-weight: bold;">self</span>)))

    <span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">swap</span>(<span style="color: #859900; font-weight: bold;">self</span>, i, j):
        <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">swap i-th and j-th value without actually modifying data</span>
        <span style="color: #859900; font-weight: bold;">self</span>.<span style="color: #268bd2;">indices</span>[<span style="color: #268bd2;">i</span>], <span style="color: #859900; font-weight: bold;">self</span>.<span style="color: #268bd2;">indices</span>[j] = <span style="color: #859900; font-weight: bold;">self</span>.indices[j], <span style="color: #859900; font-weight: bold;">self</span>.indices[i]

        <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">update inverse_indices</span>
        <span style="color: #859900; font-weight: bold;">self</span>.inverse_indices[<span style="color: #859900; font-weight: bold;">self</span>.indices[i]] = i
        <span style="color: #859900; font-weight: bold;">self</span>.inverse_indices[<span style="color: #859900; font-weight: bold;">self</span>.indices[j]] = j

    <span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">__getitem__</span>(<span style="color: #859900; font-weight: bold;">self</span>, i):
        <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">return the view</span>
        <span style="color: #859900; font-weight: bold;">return</span> <span style="color: #657b83; font-weight: bold;">super</span>().__getitem__(<span style="color: #859900; font-weight: bold;">self</span>.indices[i])

    @<span style="color: #657b83; font-weight: bold;">property</span>
    <span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">view</span>(<span style="color: #859900; font-weight: bold;">self</span>):
        <span style="color: #859900; font-weight: bold;">return</span> [<span style="color: #859900; font-weight: bold;">self</span>[i] <span style="color: #859900; font-weight: bold;">for</span> i <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(<span style="color: #657b83; font-weight: bold;">len</span>(<span style="color: #859900; font-weight: bold;">self</span>))]

    @<span style="color: #657b83; font-weight: bold;">property</span>
    <span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">addr2name</span>(<span style="color: #859900; font-weight: bold;">self</span>):
        <span style="color: #859900; font-weight: bold;">return</span> <span style="color: #859900; font-weight: bold;">self</span>.inverse_indices


<span style="color: #859900; font-weight: bold;">class</span> <span style="color: #b58900;">Vector</span>(Viewable_Mixin, <span style="color: #657b83; font-weight: bold;">list</span>):

    <span style="color: #859900; font-weight: bold;">def</span> <span style="color: #268bd2;">__init__</span>(<span style="color: #859900; font-weight: bold;">self</span>, *args, **kws):
        <span style="color: #657b83; font-weight: bold;">super</span>().__init__(*args, **kws)


<span style="color: #268bd2;">data</span> = [<span style="color: #2aa198;">'alpha'</span>, <span style="color: #2aa198;">'beta'</span>, <span style="color: #2aa198;">'gamma'</span>, <span style="color: #2aa198;">'eta'</span>]
<span style="color: #268bd2;">vec</span> = Vector(data)

<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">swap 1-th and 3-th value</span>
vec.swap(1, 3)
<span style="color: #268bd2;">view</span> = vec.view
<span style="color: #268bd2;">expect_view</span> = [<span style="color: #2aa198;">'alpha'</span>, <span style="color: #2aa198;">'eta'</span>, <span style="color: #2aa198;">'gamma'</span>, <span style="color: #2aa198;">'beta'</span>]

<span style="color: #859900; font-weight: bold;">assert</span> <span style="color: #657b83; font-weight: bold;">all</span>(view[i] == expect_view[i] <span style="color: #859900; font-weight: bold;">for</span> i <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(<span style="color: #657b83; font-weight: bold;">len</span>(data)))

<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">swap 2-th and 3-th value</span>
vec.swap(2, 3)
<span style="color: #268bd2;">view</span> = vec.view
<span style="color: #268bd2;">expect_view</span> = [<span style="color: #2aa198;">'alpha'</span>, <span style="color: #2aa198;">'eta'</span>, <span style="color: #2aa198;">'beta'</span>, <span style="color: #2aa198;">'gamma'</span>]

<span style="color: #859900; font-weight: bold;">assert</span> <span style="color: #657b83; font-weight: bold;">all</span>(view[i] == expect_view[i] <span style="color: #859900; font-weight: bold;">for</span> i <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(<span style="color: #657b83; font-weight: bold;">len</span>(data)))

<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">restore data from view</span>
<span style="color: #268bd2;">restored_data</span> = [view[i] <span style="color: #859900; font-weight: bold;">for</span> i <span style="color: #859900; font-weight: bold;">in</span> vec.addr2name]

<span style="color: #859900; font-weight: bold;">assert</span> <span style="color: #657b83; font-weight: bold;">all</span>(data[i]==restored_data[i] <span style="color: #859900; font-weight: bold;">for</span> i <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #657b83; font-weight: bold;">range</span>(<span style="color: #657b83; font-weight: bold;">len</span>(data)))
</pre>
</div>
</div>
</div>
<div id="outline-container-explanation-of-the-implementation" class="outline-2">
<h2 id="explanation-of-the-implementation">Explanation of the Implementation</h2>
<div class="outline-text-2" id="text-explanation-of-the-implementation">
<p>
Our solution is based on creating a class called <code>Viewable_Mixin</code> that
maintains two lists: <code>self.indices</code> (name2addr) and
<code>self.inverse_indices</code> (addr2name). These lists represent the forward
and inverse mappings between the view and the data, respectively.
</p>

<p>
<code>self.indices</code> is initialized with a range of indices from 0 to the
length of the data minus 1. This list represents the mapping from the
view's indices to the data's indices. When we swap elements in the view,
we only swap their indices in this list, without actually modifying the
data.
</p>

<p>
<code>self.inverse_indices</code> is also initialized with a range of indices from
0 to the length of the data minus 1. This list represents the inverse
mapping from the data's indices to the view's indices. It is updated
whenever elements are swapped in the view, ensuring that the inverse
mapping remains consistent with the forward mapping.
</p>

<p>
The <code>swap</code> method takes two indices, i and j, and swaps the i-th and
j-th elements in the view without modifying the actual data. This is
achieved by swapping the corresponding indices in <code>self.indices</code> and
updating <code>self.inverse_indices</code>.
</p>

<p>
The <code>__getitem__</code> method is used to return the element in the view at a
given index. It does this by returning the data element at the index
specified by <code>self.indices[i]</code>.
</p>

<p>
Finally, the <code>view</code> and <code>addr2name</code> properties return the current state
of the view and the inverse mapping (<code>self.inverse_indices</code>),
respectively.
</p>
</div>
</div>
<div id="outline-container-conclusion" class="outline-2">
<h2 id="conclusion">Conclusion</h2>
<div class="outline-text-2" id="text-conclusion">
<p>
In this blog post, we have explored the problem of simulating swap
operations without modifying the underlying data. We provided a Python
code example that demonstrates how to achieve this using a simple class
structure called <code>Viewable_Mixin</code>. The solution maintains two lists,
<code>self.indices</code> and <code>self.inverse_indices</code>, to store the forward and
inverse mappings between the view and the data. By swapping elements in
the view and updating the mappings accordingly, we can efficiently
simulate swaps without modifying the actual data.
</p>

<p>
This approach can be useful in various applications, such as data
visualization, algorithm analysis, and undo/redo functionality, where it
is necessary to maintain multiple views of the same data or analyze the
impact of different permutations without affecting the underlying data.
</p>
</div>
</div>
<div id="outline-container-org1d506a5" class="outline-2">
<h2 id="org1d506a5">Mathematical Justifications</h2>
<div class="outline-text-2" id="text-org1d506a5">
<p>
It is possible to interprete <code>indices</code> and <code>inverse_indices</code> as two permutation matrix. To see this, one may write <code>data</code> and <code>view</code> as two column vectors and note the following equations.
</p>

$$ \begin{aligned}
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0
\end{bmatrix}
\begin{bmatrix}
\alpha \\ \beta \\ \gamma \\ \eta
\end{bmatrix}
&= \begin{bmatrix}
\alpha \\ \eta \\ \beta \\ \gamma
\end{bmatrix},
\\
\begin{bmatrix}
\alpha \\ \beta \\ \gamma \\ \eta
\end{bmatrix}
&= \begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
0 & 1 & 0 & 0
\end{bmatrix}
\begin{bmatrix}
\alpha \\ \eta \\ \beta \\ \gamma
\end{bmatrix}.
\end{aligned} $$

<p>
Hence, <code>indices</code> represents the permutation matrix in the first equation, denoted by \(A\) below, and <code>inversed_indices</code> represents the matrix in the second equation, denoted by \(B\).
</p>

<p>
Clearly, \(AB=I\). Moreover, because \(A\) and \(B\) are othrogonal matrix, \(A=B^\intercal\) holds too.
</p>

<p>
In this point of view, <code>indices[i]</code> stores the unique column index <code>j</code> such that \(A_{ij}=1\),
</p>

<p>
\[ \sum_{j}A_{ij} \mathtt{data[j]} = \mathtt{view[i]} = \mathtt{data[indices[i]]}.\]
</p>

<p>
Similarly, <code>inversed_indices[i]</code> stores the unique column index <code>j</code> such that \(B_{ij}=1\),
</p>

<p>
\[ \sum_{j}B_{ij} \mathtt{view[j]} = \mathtt{data[i]} = \mathtt{view[inversed\_indices[i]]}.\]
</p>

<p>
For arbitary vector \(v\), we have (introduce the notation that \(\mathbb{I}[\mathtt{cond}]=1\) if and only if \(\mathtt{cond}\) is true)
</p>

$$ \begin{aligned}
(BAv)_i &= \sum_{k} \sum_{j} A_{ik} B_{kj} v_j \\
&= \sum_{k} \mathbb{I}(k=\mathtt{indices}[i]) \sum_{j} \mathbb{I}(j=\mathtt{inversed\_indices}[k]) \cdot v_j \\
&= \sum_{k} \mathbb{I}(k=\mathtt{indices}[i]) \cdot v[\mathtt{inversed\_indices}[k]] \\
&= v[\mathtt{inversed\_indices}[\mathtt{indices}[i]]].
\end{aligned} $$

<p>
Thus, we have
\[i  = \mathtt{inversed\_indices}[\mathtt{indices}[i]].\]
This is the reflection of the matrix equation \(BA=I\).
</p>

<p>
Now look back to the equation transforming <code>data</code> to <code>view</code>
</p>

$$ \begin{bmatrix}
\mathbb{I}(j = \mathtt{indices}[0]) \\
\mathbb{I}(j = \mathtt{indices}[1]) \\
\mathbb{I}(j = \mathtt{indices}[2]) \\
\mathbb{I}(j = \mathtt{indices}[3])
\end{bmatrix}
\begin{bmatrix}
\mathtt{data}[0] \\
\mathtt{data}[1] \\
\mathtt{data}[2] \\
\mathtt{data}[3]
\end{bmatrix}
= \begin{bmatrix}
\mathtt{view}[0] \\
\mathtt{view}[1] \\
\mathtt{view}[2] \\
\mathtt{view}[3]
\end{bmatrix}, $$

<p>
where \(\mathbb{I}(j = \mathtt{indices}[0])\) denotes a row vector with
subscript \(j\). To swap the <code>view</code> vector without modifying the <code>data</code>
vector, we can swap rows of \(A\) to satisfying the transforming
equation. For example, if we want to swap the $i<sub>1</sub>$-th and $i<sub>2</sub>$-th
entry of <code>view</code>, we can create another indices to represent the new
permutation matrix:
</p>

$$ \begin{cases}
\mathtt{indices}'[i_1] &= \mathtt{indices}[i_2],\\
\mathtt{indices}'[i_2] &= \mathtt{indices}[i_1],\\
\mathtt{indices}'[i] &= \mathtt{indices}[i],\quad \forall i\not\in\{i_1,i_2\}.
\end{cases}$$
The inversed indices need to update accordingly:
$$ \begin{cases}
j_1 = \mathtt{indices}[i_1], \\
j_2 = \mathtt{indices}[i_2], \\
\mathtt{inversed\_indices}'[j_1] &= \mathtt{inversed\_indices}[j_2],\\
\mathtt{inversed\_indices}'[j_2] &= \mathtt{inversed\_indices}[j_1],\\
\mathtt{inversed\_indices}'[j] &= \mathtt{inversed\_indices}[j],\quad \forall j\not\in\{j_1,j_2\},
\end{cases}$$

<p>
or in a more intuitive expression
</p>

$$ \begin{cases}
\mathtt{inversed\_indices}'[\mathtt{indices}'[i_1]] &= \mathtt{inversed\_indices}[\mathtt{indices}'[i_2]] = i_1,\\
\mathtt{inversed\_indices}'[\mathtt{indices}'[i_2]] &= \mathtt{inversed\_indices}[\mathtt{indices}'[i_1]] = i_2,\\
\mathtt{inversed\_indices}'[\mathtt{indices}'[i]] &= \mathtt{inversed\_indices}[\mathtt{indices}'[i]] = i,\quad \forall i\not\in\{i_1, i_2\},
\end{cases} $$

<p>
It's easy to verify that \(\mathtt{inversed\_indices}'\) is indeed the inverse mapping of \(\mathtt{indices}'\).
</p>
</div>
</div>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-think.html">think</a> </div>
]]></description>
  <category><![CDATA[think]]></category>
  <link>https://dou-meishi.github.io/org-blog/2023-04-20-SwapBookkeeping/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2023-04-20-SwapBookkeeping/notes.html</guid>
  <pubDate>Thu, 20 Apr 2023 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Manage All Your Files]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgcd19614">Motivation</a></li>
<li><a href="#orge056e31">A combined solution</a></li>
<li><a href="#orge87b800">Further Discussion</a></li>
</ul>
</div>
</nav>
<p>
ÈöèÁùÄÂ∑•‰ΩúÂíåÂ≠¶‰π†ÁöÑÈúÄË¶ÅÔºå‰∏™‰∫∫ÁîµËÑëÈáåÁöÑÊñá‰ª∂Ë∂äÊù•Ë∂äÂ§öÔºåÂá†‰πéÊØèÂ§©ÈÉΩ‰ºöÂõ†‰∏∫ÂêÑÁßçÂêÑÊ†∑ÁöÑÂéüÂõ†ËÄå
Êàñ‰∏ãËΩΩÊàñÂàõÂª∫ÂêÑÁßçÊñá‰ª∂„ÄÇÂ¶Ç‰ΩïÊúâÊù°ÁêÜÁöÑÁÆ°ÁêÜËøô‰∫õÊñá‰ª∂ÊòØ‰∏Ä‰∏™ÈùûÂ∏∏ÂÄºÂæóËÆ®ËÆ∫ÂíåÊÄùËÄÉÁöÑÈóÆÈ¢ò„ÄÇ
</p>
<div id="outline-container-orgcd19614" class="outline-2">
<h2 id="orgcd19614">Motivation</h2>
<div class="outline-text-2" id="text-orgcd19614">
<p>
Âú®Ê≠£ÂºèËÆ®ËÆ∫‰πãÂâçÔºåÂÖàËÆ©Êàë‰ª¨ÁΩóÂàóÂá†ÁßçÊó•Â∏∏ÊÉÖÂΩ¢„ÄÇ
</p>

<ol class="org-ol">
<li>ÊØîÂ¶ÇÂΩìÂâçËøôÁØáÁ¨îËÆ∞ÔºåÂÆÉÊòØÊàëÂøÉ‰∏≠ÊÄùËÄÉÂæà‰πÖÁöÑËØùÈ¢òÔºå‰ΩÜ‰ªäÂ§©Êôö‰∏äÁªà‰∫éÊ≠£ÂºèÂÜôÊàêÊñáÂ≠óËÆ®ËÆ∫‰∏Ä‰∫å„ÄÇ
ÊòæÁÑ∂ÔºåÊàëÊúâÂøÖË¶ÅÂÆÉ‰øùÂ≠ò‰∏ãÊù•Ôºå‰ª•‰æõÂêéÁª≠ÁöÑÊÄùËÄÉÂíåÊîπËøõ„ÄÇ</li>

<li>ÊØîÂ¶Ç‰∏∫Â≠¶‰π†Êüê‰∏™ËØæÁ®ã/‰π¶Á±çÂàõÂª∫ÁöÑÁ¨îËÆ∞Ôºå‰ª•Âèä‰º¥ÈöèÂ≠¶‰π†ËÄå‰∫ßÁîüÁöÑÂÖ∂‰ªñÊñá‰ª∂Ôºå‰æãÂ¶Ç‰∏Ä‰∫õ‰ª£Á†ÅÔºåÂõæÁâáÁ≠âÁ≠â„ÄÇ</li>

<li>ÊØîÂ¶ÇÊØï‰∏öËÆ∫Êñá„ÄÇÂõ†‰∏∫Êàë‰ΩøÁî® latex
ÂÜôËÆ∫ÊñáÔºåÊâÄ‰ª•ËøôË¶ÅÊ±ÇÊàëÁÆ°ÁêÜÂíå‰øùÂ≠ò‰∏ÄÁ≥ªÂàóÊñá‰ª∂ÔºåÂåÖÊã¨Ê∫êÊñá‰ª∂ .tex,
ÂèÇËÄÉÊñáÁåÆÊï∞ÊçÆÂ∫ì .bib, ‰∏Ä‰∫õÁîüÊàêÁöÑÂõæÁâáÔºåÁîöËá≥‰∏Ä‰∫õ‰∏ãËΩΩ‰∏ãÊù•ÁöÑÂèÇËÄÉÊñáÁåÆ pdf.</li>

<li>ÊØîÂ¶ÇÊüêÊ¨°Ê±áÊä•„ÄÇÂêåÊ†∑ÔºåÂú®ÂáÜÂ§áËøô‰∏™Ê±áÊä•ÁöÑËøáÁ®ã‰∏≠ÔºåÊàëÂèØËÉΩ‰∏çÊ≠¢ÈúÄË¶Å‰øùÂ≠òÊ±áÊä•ÊâÄÁî®ÁöÑ
ppt, ËøòÊúâ‰∏éÊ≠§Áõ∏ÂÖ≥ÁöÑ ÊâÄÊúâÊñá‰ª∂Ôºå‰æãÂ¶ÇËßÜÈ¢ëÔºå‰ª£Á†ÅÔºåÂõæÁâáÁ≠âÁ≠â„ÄÇ</li>

<li>‰∏Ä‰∫õÂÖ∂‰ªñÁöÑ‰ªªÂä°„ÄÇÊØîÂ¶ÇÈíàÂØπÊüêÁØáÊñáÁ´†ÂÜôÁöÑÁ¨îËÆ∞ÔºåÈíàÂØπÊüê‰∏™Á†îÁ©∂ËØæÈ¢òÂÜôÁöÑÁªºËø∞Ë∞ÉÁ†îÔºåÈíàÂØπÂÆ°Á®øÊÑèËßÅÂÜôÁöÑÂèçÈ¶àÔºå
ÈíàÂØπÊüê‰∏™ÁâπÂà´ÈóÆÈ¢òÁöÑ‰∏ÄÁÇπÁÇπÂ∞èÁ†îÁ©∂ÔºåÈíàÂØπÊüê‰∏™ÁâπÂà´ÂñúÁà±Ê∏∏ÊàèÂÅöÁöÑÊîªÁï•Á≠âÁ≠â„ÄÇ</li>

<li>Êî∂ÈõÜÂà∞ÁöÑËµÑÊ∫ê„ÄÇ‰æãÂ¶ÇÂ≠ó‰ΩìÔºåËΩØ‰ª∂ÔºåÈü≥‰πêÁ≠âÁ≠â„ÄÇ</li>
</ol>

<p>
‰ªé‰∏äÈù¢ÁöÑËøô‰∫õÂÆûÈôÖÊÉÖÊôØÂèØ‰ª•ÁúãÂá∫ÔºåÊó•Â∏∏ÈáåÁ¢∞ËßÅÁöÑÂêÑÁßçÊñá‰ª∂Êù•Ê∫êÂçÅÂàÜÂπøÊ≥õ„ÄÇÈô§ÂéªÈü≥‰πêÔºåÁÖßÁâáÔºåËßÜÈ¢ëÂíåËΩØ‰ª∂Ëøô‰∫õ
Ê≤°Êúâ‰∏ä‰∏ãÊñáÁöÑËµÑÊ∫êÊñá‰ª∂Â§ñÔºåÂú®ÂêÑÁßçÂÖ∑‰ΩìÊÉÖÂΩ¢‰∏ãÈÅáËßÅÁöÑÊñáÊ°£ÊòæÁÑ∂‰∏çËÉΩÁÆÄÂçïÁ≤óÊö¥Âú∞ÈÉΩÊîæÂú®
<code>Documents</code> Êñá‰ª∂Â§π ‰∏ãËøõË°åÁÆ°ÁêÜ„ÄÇ
</p>

<p>
‰ª•ÂâçÊàëÁöÑÂÅöÊ≥ïÊòØÊ†πÊçÆÁ±ªÂà´ËøõË°åÂΩíÁ±ª„ÄÇÊØîÂ¶ÇÂ±û‰∫éÁ¨îËÆ∞ÁöÑÂΩí‰∫é <code>Notes</code>
Êñá‰ª∂Â§πÔºåÂ±û‰∫éÊ±áÊä•ÁöÑÂΩí‰∫é <code>Presentation</code> Êñá‰ª∂Â§πÔºå Â±û‰∫éÁßëÁ†îÁöÑÂΩí‰∫é <code>Research</code>
Êñá‰ª∂Â§πÔºåÂ±û‰∫éÂèÇËÄÉËµÑÊñôÁöÑÂΩí‰∫é <code>References</code> Êñá‰ª∂Â§π, Â±û‰∫é‰ª£Á†ÅÁöÑÂΩí‰∫é <code>Code</code>
Á≠âÁ≠â„ÄÇ ‰ΩÜÈöèÁùÄÊó∂Èó¥ÁöÑÊé®ÁßªÔºåÊàëÊ∏êÊ∏êÂèëÁé∞ËøôÁßçÊñπÊ≥ïÁöÑ‰∏Ä‰∫õÂºäÁ´Ø:
</p>

<ol class="org-ol">
<li><p>
Ë¶ÅÊ±ÇËÆæËÆ°‰∏ÄÁªÑÂÆåÁæéÁöÑ‰∫íÊñ•‰∏î‰∫íË°•ÁöÑÊ†áÁ≠æ‰Ωú‰∏∫ *Êñá‰ª∂Á±ªÂà´*„ÄÇÂØπÊàëËÄåË®Ä Musics,
Videos, Pictures ËøôÂá†‰∏™Ê†áÁ≠æÂ∑•‰ΩúÂæóÂæàÂ•ΩÔºå‰ΩÜÊòØ <code>Documents</code>
ÁöÑÊ¶ÇÂøµÂ∞±Â§™Â§ß‰∫ÜÁÇπ„ÄÇÈöèÁùÄÊó∂Èó¥Êé®ÁßªÔºå
Ëøô‰∏™Êñá‰ª∂Â§π‰∏ãÁöÑÊñáÊ°£Ë∂äÊù•Ë∂äÂ§öÔºå‰ª•Ëá≥‰∫éÊàëÂøÖÈ°ªÂàõÂª∫‰∫åÁ∫ßÊñá‰ª∂Á±ªÂà´Ôºå‰πüÂ∞±ÊòØÂ≠êÊñá‰ª∂Â§πËøõË°åÁÆ°ÁêÜÔºåÊØîÂ¶Ç
<code>Notes</code>, <code>Research</code>, <code>References</code>, <code>Presentation</code>
Á≠â„ÄÇÁÑ∂ËÄåÂèØ‰ª•È¢ÑËßÅÔºåÈöèÁùÄÊó∂Èó¥Êé®ÁßªÔºå
Ëøô‰∫õÂ≠êÊñá‰ª∂Â§π‰πü‰ºöË∂äÊù•Ë∂äÂ§ßÔºå‰ª•Ëá≥‰∫éÂ∞ÜÊù•ÈúÄË¶ÅÂàõÂª∫‰∏âÁ∫ßÊñá‰ª∂Á±ªÂà´„ÄÇ
</p>

<p>
‰ªéÂè¶‰∏Ä‰∏™ËßíÂ∫¶ËÆ≤ÔºåËøôÁßçÊñπÊ≥ïÈáå
*Êñá‰ª∂Á±ªÂà´ÁöÑËÆæËÆ°ÂÖà‰∫éÊâÄÈúÄË¶ÅÁÆ°ÁêÜÁöÑÊñá‰ª∂*„ÄÇËøôÂèØËÉΩÊòØÂØºËá¥ÂÆûÈôÖ‰ΩìÈ™åÂ∑ÆÁöÑÊ†πÊú¨ÂéüÂõ†„ÄÇ
Âú®Â∞ÜÊù•‰∏ÄÂπ¥ÁîöËá≥Âá†Âπ¥ÁöÑÊñá‰ª∂Âá∫Áé∞‰πãÂâçÔºåÊñá‰ª∂Á±ªÂà´Â∞±ÂøÖÈúÄÂÖàËÆæÂÆöÂ•Ω„ÄÇÂ¶ÇÊûúÂèçËøáÊù•ÔºåÂØπ‰∫éÂΩìÂâçÂ∑≤ÁªèÂ≠òÂú®ÁöÑÊñá‰ª∂ËøõË°åÂàÜÁ±ªÁÆ°ÁêÜÔºå
ÈÇ£‰πàÊñá‰ª∂Á±ªÂà´ÊòØÂÆπÊòìËÆæËÆ°ÁöÑ„ÄÇ‰ΩÜÂÆûÈôÖ‰ΩøÁî®‰∏≠Âπ∂‰∏çÊòØËøôÊ†∑ÔºåÊñ∞ÁöÑÊñá‰ª∂ÊØèÂ§©ÈÉΩÂú®Âá∫Áé∞„ÄÇËøôÂØºËá¥ÊûÅÊúâÂèØËÉΩÂá†‰∏™ÊúàÂâçËÆæËÆ°
ÁöÑÊñá‰ª∂Á±ªÂà´Âπ∂‰∏çËÉΩÂæàÂ•ΩÁöÑÂΩíÁ±ªËøôÂá†‰∏™ÊúàÂá∫Áé∞ÁöÑÊñ∞Êñá‰ª∂„ÄÇËøôÂ∞±ÊòØ
*Êñá‰ª∂Á±ªÂà´ËÆæËÆ°ÂÖà‰∫éË¢´ÁÆ°ÁêÜÊñá‰ª∂ÁöÑÂá∫Áé∞*„ÄÇ
</p></li>

<li>‰∏çÂêåÁ±ªÂà´ÁöÑÊñá‰ª∂Áõ∏‰∫í‰πãÈó¥ÊòØÂÜÖÂú®ËÅîÁ≥ªÁöÑÔºåÊòØÊûÑÊàê‰∏ä‰∏ãÊñáÂÖ≥Á≥ªÁöÑ„ÄÇÊØîÂ¶Ç‰∏∫‰∫ÜÊé¢Á©∂‰∏Ä‰∏™Â∞èÈóÆÈ¢òËÄåÂÜôÁöÑÊÄùËÄÉÂíå
‰∏∫‰∫ÜÈ™åËØÅÊàëÁöÑÊÉ≥Ê≥ïËÄåÂÜôÁöÑ‰ª£Á†Å„ÄÇÂ¶ÇÊûúÊåâÁÖßÁ±ªÂà´ÂΩíÁ±ªÔºåÈÇ£‰πàÊÄùËÄÉÁ¨îËÆ∞Âíå‰ª£Á†ÅÂ∫îËØ•ÂàÜÂ±û‰∏§‰∏™‰∏çÂêåÁöÑÊñá‰ª∂Â§π„ÄÇ
‰ΩÜËøôÂ∞±Á†¥Âùè‰∫ÜÊñá‰ª∂‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºå‰∏ç‰æø‰∫éÂêéÁª≠ÁöÑÂõûÈ°æÂíåÁªßÁª≠Êõ¥Êñ∞„ÄÇ</li>
</ol>

<p>
ÁÆÄËÄåË®Ä‰πãÔºåÂü∫‰∫éÊñá‰ª∂Á±ªÂà´ÁöÑÁÆ°ÁêÜÊñπÂºèÂçÅÂàÜ‰æùËµñ‰∫é <b>Êñá‰ª∂Á±ªÂà´</b> Ê†áÁ≠æÁöÑËÆæËÆ°,
ÂêåÊó∂ÔºåÂú®ÂàÜÁ±ªÁÆ°ÁêÜÁöÑÂêåÊó∂Ôºå‰πü‰ºöÂØºËá¥Êú¨Êù•Áõ∏‰∫í‰πãÈó¥ ÂÖ∑ÊúâËÅîÁ≥ªÁöÑÊñá‰ª∂Ë¢´ÂàÜÂºÄÊîæÁΩÆ„ÄÇ
</p>
</div>
</div>
<div id="outline-container-orge056e31" class="outline-2">
<h2 id="orge056e31">A combined solution</h2>
<div class="outline-text-2" id="text-orge056e31">
<p>
ÁõÆÂâçÁöÑËß£ÂÜ≥ÂäûÊ≥ïÊòØÔºöÂú®Âü∫‰∫éÁ±ªÂà´ÁÆ°ÁêÜÁöÑÂü∫Á°Ä‰∏ä,
ÂØπÊüê‰∫õÁ±ªÂà´ÁöÑÊñá‰ª∂Â§πËÆæËÆ°‰∏ÄÂ•ó‰∏ìÈó®ÁöÑÊñá‰ª∂ÁªÑÁªáÊñπÂºè„ÄÇ
ÂÖ∑‰ΩìËÄåË®ÄÔºå‰∏ªÁõÆÂΩïÊåâÁÖßÁ±ªÂà´ËøõË°åÁÆ°ÁêÜÔºåÊîæÊúâÊñá‰ª∂Â§π
</p>

<ul class="org-ul">
<li>Documents.
Êó•Â∏∏Â∑•‰ΩúÊâÄÁî®Êñá‰ª∂Â§πÔºåÊâÄÊúâÊñáÊ°£ÊàñËÄÖÁõ∏ÂÖ≥Êñá‰ª∂Â≠òÊîæ‰∫éÊ≠§ÔºåÊåâÁÖßÁâπÂà´ËÆæËÆ°ÁöÑÊñπÂºèËøõË°åÁªÑÁªá„ÄÇ</li>
<li>References. ÊîæÂêÑÁßç‰π¶Á±çÊàñËÄÖÂèÇËÄÉÊñáÁåÆ„ÄÇÊåâÁÖßÁâπÂà´ËÆæËÆ°ÁöÑÊñπÂºèËøõË°åÁªÑÁªá„ÄÇ</li>
<li>Plugins.
ÊîæÂêÑÁßçÂæàÊúâÁî®‰ΩÜÁõ∏ÂØπÂ∞è‰ºóÁöÑÂ∞èËΩØ‰ª∂„ÄÇÂêÑÁßçÂ§ßÂûãÁü•ÂêçËΩØ‰ª∂‰∏çÂøÖÊîæÂú®ËøôÈáå„ÄÇ</li>
<li>Pictures, Videos, Musics Á≠âÁ≠â„ÄÇ</li>
</ul>

<p>
ÁõÆÂâçÂÖ≥‰∫é <code>References</code>
Êñá‰ª∂Â§πÁöÑÁÆ°ÁêÜËøòÊ≤°ÊúâÂæàÊàêÁÜüÁöÑÊñπÊ°àÔºåÂè™ÊòØÁÆÄÂçïÂú∞Ê†πÊçÆÂêÑ‰∏™‰π¶Á±ç/ÊñáÁåÆÊâÄÂ±ûÁöÑÁ±ªÂà´ËøõË°å‰∫Ü‰∫åÁ∫ßÊñá‰ª∂Â§πÁöÑÂàÜÁ±ª„ÄÇ
ÂØπ‰∫éÂèØËÉΩÂ±û‰∫éÂ§ö‰∏™‰∏çÂêåÂ≠êÁ±ªÁöÑ‰π¶Á±ç/ÊñáÁåÆÔºåÂàôÂàõÂª∫Êã∑Ë¥ù (ÈìæÊé•)
ÂàÜÂà´ÊîæÂú®ÈúÄË¶ÅÊîæÁΩÆÁöÑÂ≠êÁ±ª‰∏ã„ÄÇ
</p>

<p>
<code>Documents</code> ÊòØÁõÆÂâçÁöÑ‰∏ªË¶Å‰ΩøÁî®ÁöÑÊñá‰ª∂Â§π,
ÈáåÈù¢ÊîæÁΩÆ‰∫ÜËøë‰∏§‰∏âÂπ¥Êù•ÊàëÊâÄÈúÄË¶ÅÁÆ°ÁêÜÁöÑÂá†‰πéÊâÄÊúâÊñá‰ª∂„ÄÇ
ÂÆÉÁöÑÁªÑÁªáÊñπÂºèÊòØ‰∏çÂü∫‰∫éÂ≠êÁ±ªÂà´ÁöÑ„ÄÇ‰∫ãÂÆû‰∏äÔºå‰∏ªË¶ÅÊòØÂá∫‰∫éËß£ÂÜ≥‰∏äÈù¢ÊèêÂà∞
"‰∏çÂêåÊñá‰ª∂Áõ∏‰∫í‰πãÈó¥Â≠òÂú®‰∏ä‰∏ãÊñáËÅîÁ≥ª" ÈóÆÈ¢òÁöÑËÄÉËôëÔºå=Documents=
Êñá‰ª∂Â§πË¢´ËÆæËÆ°Êàê‰∫ÜÊåâÁÖß <b>‰∫ã‰ª∂</b> ËøõË°åÁªÑÁªáÔºå‰ª•Êó∂Èó¥ +
‰∫ã‰ª∂ÂêçÁß∞ÁöÑÊñπÂºèÂëΩÂêçÂ≠êÊñá‰ª∂Â§π‰ª•‰øùÂ≠òÊñá‰ª∂ÁöÑÂΩ¢Âºè„ÄÇ‰æãÂ¶Ç‰∏ãÈù¢ÊòØ‰∏Ä‰∫õ <code>Documents</code>
‰∏ãÁöÑÂ≠êÊñá‰ª∂Â§πÁöÑÂÜÖÂÆπ„ÄÇ
</p>

<pre class="example" id="org359482d">
  ‚îú‚îÄ‚îÄ 2022-11-25-MidtermExam/
  ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ notes.md
  ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ report.pdf
  ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ report.tex
  ‚îú‚îÄ‚îÄ 2022-11-28-ReviewPIBSDE/
  ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ latex/
  ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ notes.md
  ‚îú‚îÄ‚îÄ 2022-12-04-PhDSubmission/
  ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ About-TOEFL-home-edition.pdf
  ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ CV.pdf
  ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ PS.pdf
  ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ RP.pdf
  ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ TOEFL-SCORE.pdf
  ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ academic-certificates/
  ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ academic-recommendation/
  ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ transcript/
  ‚îî‚îÄ‚îÄ 2022-12-09-NotesSystemPlan/
      ‚îî‚îÄ‚îÄ notes.md
</pre>

<p>
ÂèØ‰ª•ÁúãÂà∞ÔºåÂØπ‰∫éÁÆÄÂçïÁöÑÁ¨îËÆ∞Ôºå‰æãÂ¶Ç <code>2022-12-09-NotesSystemPlan/</code>,
ÈáåÈù¢ÂèØ‰ª•Âè™Êîæ‰∏Ä‰∏™ÂçïÁã¨ÁöÑÁ¨îËÆ∞Êñá‰ª∂ <code>notes.md</code>. Â¶ÇÊûúÈúÄË¶Å‰ΩøÁî® latex, ‰æãÂ¶Ç
<code>2022-11-25-MidtermExam</code> Âíå <code>2022-11-28-ReviewPIBSDE</code>,
ÈáåÈù¢‰πüÂèØ‰ª•ÊîæÂØπÂ∫îÁöÑÊñá‰ª∂„ÄÇÁâπÂà´ÁöÑÔºåÂØπ‰∫éÊüê‰∏™ÁâπÂÆöÁöÑ‰ªªÂä°ÔºåÊØîÂ¶Ç
<code>2022-12-04-PhDSubmission</code>, Êñá‰ª∂Â§πÈáåÂèØ‰ª•ÊîæÊ≠§‰ªªÂä°ÈúÄË¶ÅÁöÑÊâÄÊúâÁõ∏ÂÖ≥ËµÑÊñô„ÄÇ
</p>

<p>
ËøôÊ†∑‰∏ÄÊù•ÔºåÊâÄÊúâÁöÑÊñá‰ª∂ÈÉΩ‰ª•ÂÆÉ‰ª¨ÊâÄÈíàÂØπÁöÑ <b>‰∫ã‰ª∂</b> ËøõË°åÂ≠òÂÇ®„ÄÇ
ËôΩÁÑ∂ËøáÂéªÂçäÂπ¥Â§öÔºå‰ΩÜÊòØÂè™Ë¶ÅÊàëÁúãÂà∞ tree ÁöÑËæìÂá∫ÔºåÊàëÂ∞±ËÉΩÂæàËΩªÊùæÂú∞ËØ¥Âá∫‰∏äÈù¢Á§∫‰æãÈáå
ÈíàÂØπÁöÑÂà∞Â∫ïÊòØÈÇ£‰∫õ‰∫ã‰ª∂
</p>

<ul class="org-ul">
<li>2022-11-25-MidtermExam, Á†î‰∫å‰∏äÂ≠¶ÊúüÁöÑÊúü‰∏≠ËÄÉÊ†∏</li>
<li>2022-11-28-ReviewPIBSDE, ÂØπËØæÈ¢ò PIBSDE ÁöÑ‰∏Ä‰∫õÊÄùËÄÉ</li>
<li>2022-12-04-PhDSubmission, Áî≥ËØ∑ PhD ÈúÄË¶ÅÊèê‰∫§ÁöÑÁõ∏ÂÖ≥ÊùêÊñô</li>
<li>2022-12-09-NotesSystemPlan, ‰∏Ä‰∏™ÂÖ≥‰∫éÁ¨îËÆ∞Á≥ªÁªüÁöÑÂàùÊ≠•ÊÉ≥Ê≥ï</li>
</ul>

<p>
ÊàëËÆ§‰∏∫‰ª• <b>‰∫ã‰ª∂</b> ËøõË°åÁªÑÁªáÔºåÊåâÁÖßÊó∂Èó¥ +
‰∫ã‰ª∂ÂêçÁöÑÊñπÂºèÂàõÂª∫Â≠êÊñá‰ª∂Â§π‰øùÂ≠òÊñá‰ª∂ÁöÑÂ•ΩÂ§ÑÊúâ‰ª•‰∏ãÂá†ÁÇπÔºö
</p>

<ol class="org-ol">
<li>‰æø‰∫éÂõûÈ°æÂíåÊü•ÊâæÊñá‰ª∂„ÄÇÂ∞±ÂÆûË∑µ‰∏ãÊù•ÁöÑÁªèÈ™åËÄåË®ÄÔºå Êó∂Èó¥ +
‰∫ã‰ª∂Âêç‰ª•ÂèäË∂≥Â§üÊàëÂõûÂøÜËµ∑ÂíåÊ≠§Êñá‰ª∂Â§πÁöÑÂá†‰πéÊâÄÊúâÂÜÖÂÆπÔºå
ÂåÖÊã¨ÂàõÂª∫ÂÆÉÁöÑÁõÆÁöÑÔºåÈáåÈù¢‰∏ªË¶ÅÁî®‰∫éÊîæÈÇ£‰∫õÊñá‰ª∂Á≠âÁ≠â„ÄÇ</li>
<li>Âá†‰πé‰∏ç‰ºöÂõ†‰∏∫Êó∂Èó¥Â¢ûÂä†ËÄåÂ¢ûÂ§ßÁÆ°ÁêÜÈöæÂ∫¶„ÄÇÂÆûË∑µË°®ÊòéÔºåÂú®‰∏Ä‰∏™ÂÖ∑‰ΩìÁöÑÊó∂Èó¥ÊÆµÔºå
Êàë‰∏ÄËà¨ÊúÄÂ§öÂè™ÂÖ≥ÂøÉÂçÅÊù•‰∏™ <b>‰∫ã‰ª∂</b>
Êñá‰ª∂Â§πÔºåÂç≥Ëøô‰∏™Êó∂Èó¥ÊÆµÊàëÂè™ÈúÄË¶ÅÁÆ°ÁêÜËøô‰∫õÁâπÂÆöÁöÑÊñá‰ª∂Â§π„ÄÇ
ÂÖ∂‰ªñÊñá‰ª∂Â§πÂè™‰Ωú‰∏∫ËÉåÊôØÂ≠òÂú®„ÄÇÂæóÁõä‰∫éËøôÁßçÁªÑÁªáÂΩ¢ÂºèÔºåÊâÄÊúâ‰∏çÂ§™ÂÖ≥ÂøÉÁöÑÊñá‰ª∂ÈÉΩË¢´Êî∂ÂÆπÂú®Ëøô‰∫õËÉåÊôØ
Êñá‰ª∂Â§π‰∏≠„ÄÇÊç¢ËÄåË®Ä‰πãÔºå <code>Documents</code> Êñá‰ª∂Â§πÁ≠â‰ª∑‰∫éÊ∞∏ËøúÂè™ÊúâÂçÅÊù•‰∏™Â≠êÊñá‰ª∂Â§π„ÄÇ</li>
<li>Êñπ‰æøÂ§á‰ªΩ„ÄÇ‰∏ÄËà¨ËÄåË®ÄÊó∂Èó¥Ë∂ÖËøá‰∏ÄÂπ¥ÁöÑ‰∫ã‰ª∂Êñá‰ª∂Â§π‰∏çÂú®ÊúâÂæàÂ§ßÁöÑÂèòÂä®ÔºåÂèØ‰ª•Âú®ÂÅöÂ•ΩÂ§á‰ªΩÂêéÁõ¥Êé•Âà†Âéª„ÄÇ</li>
<li>ËÉΩ‰øùËØÅÊñá‰ª∂‰πãÈó¥ÁöÑËÅîÁ≥ª„ÄÇÂõ†‰∏∫Âêå‰∏Ä‰∏™‰∫ã‰ª∂ÁöÑÁõ∏ÂÖ≥Êñá‰ª∂ÈÉΩÊîæÂú®‰∏ÄËµ∑„ÄÇ</li>
<li>‰∏çÂøÖÊìçÂøÉÊØè‰∏™Êñá‰ª∂Âà∞Â∫ïËØ•Â¶Ç‰ΩïÂΩíÁ±ª„ÄÇÂΩìÊúâÈúÄË¶ÅÊó∂ÔºåÁõ¥Êé•ÂàõÂª∫‰∫ã‰ª∂Êñá‰ª∂Â§πÂç≥ÂèØ„ÄÇ</li>
<li>Êñπ‰æøÂêéÁª≠ÊâπÈáèÁÆ°ÁêÜ„ÄÇÊØîÂ¶ÇÂèØ‰ª•Âú®ÊØè‰∏™‰∫ã‰ª∂Êñá‰ª∂Â§π‰∏ãÊîæ‰∏Ä‰∏™Êï∞ÊçÆÊñá‰ª∂Ôºå
Áî®Êù•ÊåáÂÆöÊ≠§‰∫ã‰ª∂ÁöÑÂêÑÁßçÂÖÉÊï∞ÊçÆ„ÄÇ</li>
</ol>

<p>
ÂùèÂ§ÑÂàôÊúâ
</p>

<ol class="org-ol">
<li>ÂÜÖÂÆπÂ§™ÂàÜÊï£„ÄÇÂèØËÉΩËÆ∏Â§ö‰∏™ <b>‰∫ã‰ª∂</b> ÊòØÂõ¥Áªï‰∏Ä‰∏™ <b>‰∏ªÈ¢ò</b>
ÁöÑÔºåÊØîÂ¶ÇÈÉΩÊòØ‰∏™‰∫∫Á¨îËÆ∞„ÄÇ
‰ΩÜÂõ†‰∏∫ËøòÊúâÂÖ∂‰ªñÁöÑ‰∫ã‰ª∂Êñá‰ª∂Â§πÊ∑∑Âú®‰∏ÄËµ∑ÔºåÊâÄ‰ª•Â¶ÇÊûúÊàëÊÉ≥Ê£ÄÈòÖÊúÄËøë‰∏Ä‰∏™ÊúàÂÅöÁöÑÊâÄÊúâÁ¨îËÆ∞Ôºå
ÈÇ£‰πàÂè™ËÉΩÂàóÂá∫Ëøô‰∏Ä‰∏™ÊúàÂÜÖÂàõÂª∫ÁöÑÊñá‰ª∂Â§πÂêéÊâãÂä®Á≠õÈÄâ„ÄÇ</li>
</ol>
</div>
</div>
<div id="outline-container-orge87b800" class="outline-2">
<h2 id="orge87b800">Further Discussion</h2>
<div class="outline-text-2" id="text-orge87b800">
<p>
ÊàëËÆ§‰∏∫Êñá‰ª∂ÁöÑ <b>Â≠òÂÇ®</b> Âíå <b>ÊµèËßà/ÁÆ°ÁêÜ</b> ÊòØÂèØ‰ª•Âå∫ÂàÜÂºÄÁöÑ„ÄÇ
</p>

<p>
<b>Â≠òÂÇ®</b> Èù¢ÂêëÁöÑÊòØÂ≠òÂÇ®ËÆæÂ§á„ÄÇÂõ†‰∏∫Êñá‰ª∂Á≥ªÁªüÈÉΩÊòØ‰ª•Ê†ëÁöÑÂΩ¢ÂºèÁªÑÁªáÁöÑÔºå
ÊâÄ‰ª•ÊØè‰∏™Êñá‰ª∂Êúâ‰∏î‰ªÖÊúâ <i>ÂîØ‰∏Ä</i> ÁöÑÂ≠òÂÇ®‰ΩçÁΩÆ„ÄÇ
</p>

<p>
<b>ÊµèËßà/ÁÆ°ÁêÜ</b>
ÂàôÈù¢ÂêëÁöÑÊòØ‰ΩøÁî®ËÄÖ„ÄÇÊàë‰ª¨Âú®ËÄÉËôë‰∏Ä‰∏™Êñá‰ª∂Êó∂ÔºåÂÖ∂ÂÆûÂπ∂‰∏çÂÖ≥ÂøÉÂÆÉË¢´Â≠òÂÇ®Âú®Âì™Èáå„ÄÇ
Êàë‰ª¨ÁúüÊ≠£ÂØπ‰∏Ä‰∏™Êñá‰ª∂ÁöÑÂç∞Ë±°ÊòØÂÆÉÁöÑÁî®ÈÄîÔºåÂÆÉÁöÑÂÜÖÂÆπÔºåÂÆÉÁöÑÁ±ªÂà´„ÄÇ‰∏∫‰∫ÜÊñπ‰æøÊµèËßà/ÁÆ°ÁêÜÔºå
Êàë‰ª¨ÂèØ‰ª•Ê†πÊçÆÊñá‰ª∂ÁöÑÂêÑÁßçÂÖÉÊï∞ÊçÆÁªôÊâì‰∏äÂêàÈÄÇÁöÑÊ†áÁ≠æ„ÄÇËøô‰∫õÊ†áÁ≠æÂØπÊñá‰ª∂ÂΩí‰∫ÜÁ±ªÔºå
‰ΩÜÊ†áÁ≠æ‰∏éÊñá‰ª∂ÊâÄÂ≠òÊîæÁöÑ‰ΩçÁΩÆ‰∏çÂøÖÁõ∏ÂÖ≥„ÄÇ
</p>

<p>
‰∏Ä‰∏™ÂÖ∏ÂûãÁöÑ‰æãÂ≠êÂ∞±ÊòØ Git. Âú® Git ‰∏≠ÔºåÊñá‰ª∂‰ª•ÁâπÂÆöÁöÑÂΩ¢ÂºèÂ≠òÂÇ®Âú® <code>.git</code>
Êñá‰ª∂Â§π‰∏≠„ÄÇ ÂΩì‰ΩøÁî®ËÄÖÊåáÂÆöÈúÄË¶ÅÂì™‰∏™ÁâàÊú¨ÁöÑÂì™‰∏™Êñá‰ª∂Êó∂ÔºåGit
Êâç‰ºöÂ∞ÜÊåáÂÆöÁöÑÊñá‰ª∂ÂèñÂá∫ÊîæÂú®ÂΩìÂâçÊñá‰ª∂Â§π‰∏ã„ÄÇ
‰ΩøÁî®ËÄÖ‰∏çÂøÖÊ∏ÖÊ•öÊñá‰ª∂ÁöÑÂÖ∑‰ΩìÂ≠òÊîæÊñπÂºèÔºå‰ªñÊ∏ÖÊ•ö‰ªñÈúÄË¶ÅÂì™‰∫õÊñá‰ª∂Âç≥ÂèØ„ÄÇ
</p>

<p>
‰∏Ä‰∫õÊñáÁåÆÁÆ°ÁêÜËΩØ‰ª∂Ôºå‰æãÂ¶Ç Zotero
‰πüÊòØÈááÁî®ÁöÑËøôÁßçÈÄªËæë„ÄÇÂëàÁé∞Áªô‰ΩøÁî®ËÄÖÁöÑÊòØÊñáÁåÆÁöÑÂêÑÁßçÂÖÉÊï∞ÊçÆÔºå
‰æãÂ¶ÇÊñáÁ´†Ê†áÈ¢òÔºå‰ΩúËÄÖÔºåÂèëË°®Âπ¥‰ªΩÁ≠âÁ≠â„ÄÇËÄåËΩØ‰ª∂ÂÜÖÈÉ®ÂØπÊñáÁåÆÁöÑÁªÑÁªáÊñπÂºèÂíåËøô‰∫õÂÖÉÊï∞ÊçÆ
(Ê†áÁ≠æ) ÂÆåÂÖ®Êó†ÂÖ≥„ÄÇ
</p>
</div>
</div>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-think.html">think</a> </div>
]]></description>
  <category><![CDATA[think]]></category>
  <link>https://dou-meishi.github.io/org-blog/2023-04-09-ManageFiles/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2023-04-09-ManageFiles/notes.html</guid>
  <pubDate>Sun, 09 Apr 2023 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Visual Studion Code - A Morden Source Code Editor]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#interface">1. Interface</a></li>
<li><a href="#command-palette">2. Command Palette</a></li>
<li><a href="#customization">3. Customization</a></li>
<li><a href="#extensions">4. Extensions</a></li>
<li><a href="#snippets">5. Snippets</a></li>
</ul>
</div>
</nav>
<p>
Visual Studio Code is a lightweight but powerful source code editor
which runs on your desktop and is available for Windows, macOS and
Linux. It comes with built-in support for JavaScript, TypeScript and
Node.js and has a rich ecosystem of extensions for other languages and
runtimes (such as C++, C#, Java, Python, PHP, Go, .NET).
</p>

<p>
Here is the official documentation <a href="https://code.visualstudio.com/docs">VS Code - Get Started</a>
</p>

<p>
See also <a href="https://adamtheautomator.com/visual-studio-code-tutorial/">The Visual Studio Code Tutorial Worth Learning</a> for a brief
introduction to VS Code.
</p>

<p>
TODO
</p>

<ul class="org-ul">
<li class="off"><code>[&#xa0;]</code> Add a section to discuss math in Markdown. Consider Extension
<code>Markdown+Math</code>.</li>

<li class="off"><code>[&#xa0;]</code> Add a section to discuss output to html. Consider Extension
<code>Markdown All in One</code>.</li>
</ul>
<div id="outline-container-interface" class="outline-2">
<h2 id="interface">1. Interface</h2>
<div class="outline-text-2" id="text-interface">
<p>
The UI is divided into five areas:
</p>

<ul class="org-ul">
<li>Editor - The main area to edit your files. You can open as many
editors as you like side by side vertically and horizontally.</li>

<li>Side Bar - Contains different views like the Explorer to assist you
while working on your project.</li>

<li>Status Bar - Information about the opened project and the files you
edit.</li>

<li>Activity Bar - Located on the far left-hand side, this lets you switch
between views and gives you additional context-specific indicators,
like the number of outgoing changes when Git is enabled.</li>

<li>Panels - You can display different panels below the editor region for
output or debug information, errors and warnings, or an integrated
terminal. Panel can also be moved to the right for more vertical
space.</li>
</ul>


<figure id="orgc3a7d0e">
<img src="./vscode-UI.png" alt="vscode-UI.png">

<figcaption><span class="figure-number">Figure 1: </span>VS Code UI</figcaption>
</figure>
</div>
</div>
<div id="outline-container-command-palette" class="outline-2">
<h2 id="command-palette">2. Command Palette</h2>
<div class="outline-text-2" id="text-command-palette">
<p>
The Command Palette in VS Code is a menu that <b><i>provides access to all
functionality within VS Code and any installed extensions</i></b>. It can be
accessed via the View menu or by using the <code>Ctrl-Shift-P</code> shortcut. The
Command Palette is <i>a single location for managing tasks, settings,
snippets, and more</i>, making it a useful tool for easy and efficient
navigation within VS Code.
</p>
</div>
</div>
<div id="outline-container-customization" class="outline-2">
<h2 id="customization">3. Customization</h2>
<div class="outline-text-2" id="text-customization">
<p>
There are many things you can do to customize VS Code.
</p>

<ul class="org-ul">
<li>Change your theme <code>&gt;Preferences: Color Theme</code></li>

<li><p>
Change your keyboard shortcuts <code>&gt;Preferences: Open Keyboard Shortcuts</code>
</p>

<blockquote>
<p>
See also<a href="https://code.visualstudio.com/docs/getstarted/keybindings#_detecting-keybinding-conflicts">Detecting keybinding conflicts</a> if necessary.
</p>
</blockquote></li>

<li><p>
Tune your settings <code>&gt;Preferences: Open Settings (UI)</code>
</p>

<blockquote>
<p>
You can scope the settings that you only want for specific languages
by the language identifier, see <a href="https://code.visualstudio.com/docs/getstarted/tips-and-tricks#_language-specific-settings">Language specific settings</a>.
</p>
</blockquote></li>

<li>Create snippets</li>

<li>Install extensions</li>
</ul>

<p>
To manage settings, open the Command Palette with <code>Ctrl-Shift-P</code>. Type
"settings" and choose <code>Preferences: Open Settings (JSON)</code> or
<code>Preferences: Open Settings (UI)</code>
</p>

<p>
Depending on your platform, the user settings file is located here:
</p>

<ul class="org-ul">
<li>Windows <code>%APPDATA%\Code\User\settings.json</code></li>
<li>macOS <code>$HOME/Library/Application\ Support/Code/User/settings.json</code></li>
<li>Linux <code>$HOME/.config/Code/User/settings.json</code></li>
</ul>
</div>
</div>
<div id="outline-container-extensions" class="outline-2">
<h2 id="extensions">4. Extensions</h2>
<div class="outline-text-2" id="text-extensions">
<p>
One of the most important features of VS Code is its extensions. An
extension is a small package you can load into VS Code that makes
working with various resources easier.
</p>

<p>
Extensions are installed in a per user extensions folder. Depending on
your platform, the location is in the following folder:
</p>

<ul class="org-ul">
<li>Windows <code>%USERPROFILE%\.vscode\extensions</code></li>
<li>macOS <code>~/.vscode/extensions</code></li>
<li>Linux <code>~/.vscode/extensions</code></li>
</ul>
</div>
</div>
<div id="outline-container-snippets" class="outline-2">
<h2 id="snippets">5. Snippets</h2>
<div class="outline-text-2" id="text-snippets">
<p>
Snippets allow you to type a few characters in a code editor tab that
quickly expands to whatever you need. Each snippet is a single JSON
element that matches a particular syntax.
</p>

<p>
Snippets exist by language or extension and are stored in JSON files.
For example, you can have a set of PowerShell, ARM template, Python, C#,
and Javascript snippets installed simultaneously.
</p>

<p>
You can find snippets by opening up the command palette with
<code>Ctrl-Shift-P</code>, typing "snippets" and hitting Enter.
</p>

<p>
See <a href="https://code.visualstudio.com/docs/editor/userdefinedsnippets">Snippets in Visual Studio Code</a> for more details.
</p>
</div>
</div>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-tool.html">tool</a> </div>
]]></description>
  <category><![CDATA[tool]]></category>
  <link>https://dou-meishi.github.io/org-blog/2023-04-06-IntroVSCode/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2023-04-06-IntroVSCode/notes.html</guid>
  <pubDate>Thu, 06 Apr 2023 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Configure Git]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgc4f215c">1. Basic Configuration</a>
<ul>
<li><a href="#org089ff46">1.1. Transfer Existed Configuration</a></li>
<li><a href="#org5d574b0">1.2. First-Time Set up</a></li>
</ul>
</li>
<li><a href="#orgad78b2f">2. Add-ons</a>
<ul>
<li><a href="#orgde524a4">2.1. Prompt</a></li>
<li><a href="#org18381a8">2.2. Pager</a></li>
</ul>
</li>
<li><a href="#orgc415d4a">3. An Example of .gitconfig</a></li>
</ul>
</div>
</nav>
<p>
See <a href="https://git-scm.com/book/en/v2">Pro Git</a> if you are not familiar
with Git.
</p>
<div id="outline-container-orgc4f215c" class="outline-2">
<h2 id="orgc4f215c">1. Basic Configuration</h2>
<div class="outline-text-2" id="text-orgc4f215c">
</div>
<div id="outline-container-org089ff46" class="outline-3">
<h3 id="org089ff46">1.1. Transfer Existed Configuration</h3>
<div class="outline-text-3" id="text-org089ff46">
<ol class="org-ol">
<li>Copy the global git configuration file to <code>~/.gitconfig</code>.</li>

<li>Copy SSH keys to <code>~/.ssh/</code>.</li>

<li><p>
Add SSH key to <code>ssh-agent</code>.
</p>

<div class="org-src-container">
<pre class="src src-sh"><span style="color: #657b83; font-weight: bold;">eval</span> <span style="color: #2aa198;">"$(</span><span style="color: #6c71c4; font-weight: bold;">ssh-agent -s</span><span style="color: #2aa198;">)"</span>
ssh-add ~/.ssh/id_ed25519
</pre>
</div></li>
</ol>
</div>
</div>
<div id="outline-container-org5d574b0" class="outline-3">
<h3 id="org5d574b0">1.2. First-Time Set up</h3>
<div class="outline-text-3" id="text-org5d574b0">
<p>
See also <a href="https://git-scm.com/book/en/v2/Getting-Started-First-Time-Git-Setup">Pro Git - Section 1.6</a>
</p>

<ol class="org-ol">
<li><p>
Set up your identity
</p>

<div class="org-src-container">
<pre class="src src-sh">git config --global user.name <span style="color: #2aa198;">"John Doe"</span>
git config --global user.email johndoe@example.com
</pre>
</div></li>

<li><p>
Set up default editor
</p>

<div class="org-src-container">
<pre class="src src-sh">git config --global core.editor vim
</pre>
</div></li>

<li><p>
Set up default branch name
</p>

<div class="org-src-container">
<pre class="src src-sh">git config --global init.defaultBranch master
</pre>
</div></li>

<li><p>
Set up git aliases
</p>

<div class="org-src-container">
<pre class="src src-sh">git config --global alias.co checkout
git config --global alias.br branch
git config --global alias.ci commit
git config --global alias.st status
</pre>
</div></li>

<li><p>
(Optional) Review your settings
</p>

<div class="org-src-container">
<pre class="src src-sh">git config --list --show-origin
</pre>
</div></li>

<li>Set up SSH.

<ol class="org-ol">
<li>See  <a href="https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent">Generating  a new SSH key and adding it to the ssh-agent</a></li>
<li>See  <a href="https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account">Adding  a new SSH key to your GitHub account</a></li>
</ol></li>
</ol>
</div>
</div>
</div>
<div id="outline-container-orgad78b2f" class="outline-2">
<h2 id="orgad78b2f">2. Add-ons</h2>
<div class="outline-text-2" id="text-orgad78b2f">
</div>
<div id="outline-container-orgde524a4" class="outline-3">
<h3 id="orgde524a4">2.1. Prompt</h3>
<div class="outline-text-3" id="text-orgde524a4">
<p>
<a href="https://github.com/magicmonty/bash-git-prompt">bash-git-prompt</a>: A bash prompt that displays information about the
current git repository.  In particular the branch name, difference
with remote branch, number of files staged, changed, etc.
</p>


<figure id="org4cdd9fa">
<img src="./gitprompt.png" alt="gitprompt.png">

<figcaption><span class="figure-number">Figure 1: </span>Example of bash-git-prompt</figcaption>
</figure>
</div>
</div>
<div id="outline-container-org18381a8" class="outline-3">
<h3 id="org18381a8">2.2. Pager</h3>
<div class="outline-text-3" id="text-org18381a8">
<p>
<a href="https://github.com/dandavison/delta">delta</a>: A syntax-highlighting pager for git, diff, and grep output.
</p>


<figure id="org241678d">
<img src="./delta.png" alt="delta.png">

<figcaption><span class="figure-number">Figure 2: </span>Example of delta</figcaption>
</figure>
</div>
</div>
</div>
<div id="outline-container-orgc415d4a" class="outline-2">
<h2 id="orgc415d4a">3. An Example of .gitconfig</h2>
<div class="outline-text-2" id="text-orgc415d4a">
<pre class="example" id="org81bc8de">
[user]
    name = John Doe
    email = johndoe@example.com
[core]
    editor = code --wait
    pager = delta
[init]
    defaultBranch = master
[alias]
    a = add
    b = branch
    co = checkout
    ci = commit
    ca = commit -a
    cm = commit -m
    d = diff
    l = log --oneline -15
    la = log --oneline --all -15
    rb = rebase
    s = status

    last = log -1
    squash = rebase --autosquash -i
[interactive]
    diffFilter = delta --color-only
[delta]
    light = true      # set to true if you're in a terminal light background color
    side-by-side = true
[merge]
    conflictstyle = diff3
[diff]
    colorMoved = default
</pre>
</div>
</div>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-tool.html">tool</a> </div>
]]></description>
  <category><![CDATA[tool]]></category>
  <link>https://dou-meishi.github.io/org-blog/2023-04-05-ConfigGit/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2023-04-05-ConfigGit/notes.html</guid>
  <pubDate>Wed, 05 Apr 2023 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[ÊñóÂú∞‰∏ª‰∏≠ÁöÑÊ¶ÇÁéá]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgdd23619">Â∞èÁêÉÊ®°Âûã</a></li>
<li><a href="#org7384d1d">‰∫ã‰ª∂Ê®°Âûã</a></li>
<li><a href="#org1e7c44d">Ê®°ÂûãÂ∫îÁî®</a></li>
<li><a href="#orgf604436">ËøõÈò∂Â∫îÁî®</a></li>
</ul>
</div>
</nav>
<div id="outline-container-orgdd23619" class="outline-2">
<h2 id="orgdd23619">Â∞èÁêÉÊ®°Âûã</h2>
<div class="outline-text-2" id="text-orgdd23619">
<p>
<b>ÈóÆ.</b> Áé∞Êúâ \(N\) ‰∏™Â∞èÁêÉ, ÂÖ∂‰∏≠Êúâ \(M\) ‰∏™‰Ωú‰∫ÜÊ†áËÆ∞. ‰ªäÈöèÊú∫Âú∞ÊåëÂá∫ \(K\) ‰∏™, ÂÜôÂá∫ÂÖ∂‰∏≠Ê†áËÆ∞ÁêÉÁöÑ‰∏™Êï∞ÁöÑÂàÜÂ∏É.
</p>

<p>
<b>Á≠î</b>. ‰∏∫‰∫ÜÈóÆÈ¢òÊúâÊÑè‰πâÔºåÊàë‰ª¨‰∏çÂ¶®Ë¶ÅÊ±Ç \(N, M, K \geq 1\)Ôºå‰∏î \(N\geq M\) Âíå \(N\geq K\).
</p>

<p>
ËÆ∞ÊåëÂá∫Ê†áËÆ∞ÁêÉÁöÑ‰∏™Êï∞‰∏∫ \(X\), ÈÇ£‰πàÂøÖÁÑ∂Êúâ \(X \leq K\) Âíå \(X \leq M\)Ôºå‰ª•Âèä\(X
\geq 0\). Âè¶Â§ñÔºåÂ¶ÇÊûúÁªùÂ§ßÂ§öÊï∞ÁêÉÈÉΩË¢´Ê†áËÆ∞ÔºåÈÇ£‰πàÂè™Ë¶ÅÊåëÂá∫ÁöÑÁêÉ‰∏çËá≥‰∫éÂ§™Â∞ë \(K >
N-M\)ÔºåÈÇ£‰πà‰∏ÄÂÆöÊúâË¢´Ê†áËÆ∞ÁöÑÁêÉ. Êç¢ËÄåË®Ä‰πãÔºå\(X\) ÁöÑÊúÄÂ∞èÂèñÂÄºÊú™ÂøÖÊòØÈõ∂Ôºå‰πüÂèØËÉΩÊòØ
\(K - (N - M)\). Â¶ÇÊ≠§‰∏çÈöæÁúãÂá∫ \(X\) ÁöÑÂèØËÉΩÂèñÂÄº‰∏∫ \(\max(0, K-N+M)\) Âà∞
\(\min(K, M)\) ‰∏≠Èó¥ÁöÑ‰ªª‰ΩïÂÄº.
</p>

<p>
\[ \mathbb{P}(X=i) = \frac{
\binom{M}{i}\binom{N-M}{K-i}}{\binom{N}{K}},\qquad \max(0,K+M-N)\leq
i\leq \min(K,M). \]
</p>

<p>
<b>Ê≥®.</b> ËÆæ \(N, M, K\) ÊòØÂ§ß‰∫é 1 ÁöÑÊ≠£Êï¥Êï∞, ‰∏îÊúâ \(N\geq M\) Âíå \(N\geq K\), ÈÇ£‰πàÊúâÊÅíÁ≠âÂºè
\[ \sum_{i=\max(0,K+M-N)}^{\min(K,M)}\binom{M}{i}\binom{N-M}{K-i} = \binom{N}{K}.\]
</p>

<p>
<b>‰æã 1.</b> Âèñ \(N=54,M=6,K=20\), Âàô \(X\) ÂàÜÂ∏ÉÂ¶Ç‰∏ã
</p>

<table>


<colgroup>
<col  class="org-left">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">
</colgroup>
<tbody>
<tr>
<td class="org-left">\(X\)</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-right">3</td>
<td class="org-right">4</td>
<td class="org-right">5</td>
<td class="org-right">6</td>
</tr>

<tr>
<td class="org-left">\(p\)</td>
<td class="org-right">0.052</td>
<td class="org-right">0.22</td>
<td class="org-right">0.34</td>
<td class="org-right">0.26</td>
<td class="org-right">0.11</td>
<td class="org-right">0.02</td>
<td class="org-right">0.0015</td>
</tr>
</tbody>
</table>

<p>
<b>‰æã 2.</b> Âèñ \(N=54,M=6,K=17\), Âàô \(X\) ÂàÜÂ∏ÉÂ¶Ç‰∏ã
</p>

<table>


<colgroup>
<col  class="org-left">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">
</colgroup>
<tbody>
<tr>
<td class="org-left">\(X\)</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-right">3</td>
<td class="org-right">4</td>
<td class="org-right">5</td>
<td class="org-right">6</td>
</tr>

<tr>
<td class="org-left">\(p\)</td>
<td class="org-right">0.09</td>
<td class="org-right">0.29</td>
<td class="org-right">0.35</td>
<td class="org-right">0.2</td>
<td class="org-right">0.061</td>
<td class="org-right">0.0089</td>
<td class="org-right">0.00048</td>
</tr>
</tbody>
</table>

<p>
<b>‰æã 3.</b> Âèñ \(N=54,M=6,K=3\), Âàô \(X\) ÂàÜÂ∏ÉÂ¶Ç‰∏ã
</p>

<table>


<colgroup>
<col  class="org-left">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">
</colgroup>
<tbody>
<tr>
<td class="org-left">\(X\)</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-right">3</td>
</tr>

<tr>
<td class="org-left">\(p\)</td>
<td class="org-right">0.7</td>
<td class="org-right">0.27</td>
<td class="org-right">0.029</td>
<td class="org-right">0.00081</td>
</tr>
</tbody>
</table>

<p>
ËÆ°ÁÆó‰∏äËø∞Ê¶ÇÁéáÁöÑ‰ª£Á†Å <a href="./balls.py">Âú®Ëøô</a> „ÄÇ
</p>
</div>
</div>
<div id="outline-container-org7384d1d" class="outline-2">
<h2 id="org7384d1d">‰∫ã‰ª∂Ê®°Âûã</h2>
<div class="outline-text-2" id="text-org7384d1d">
<p>
<b>ÈóÆ.</b> Áé∞Êúâ‰∫ã‰ª∂Êóè \(\{A_i\}_{i=1}^n\), ÂÜôÂá∫Ëá≥Â∞ëÊúâ‰∏Ä‰∏™‰∫ã‰ª∂ÂèëÁîüÁöÑÊ¶ÇÁéá.
</p>

<p>
<b>Á≠î.</b> ËÆ∞ \(S_1=\sum_i\mathbb{P}(A_i)\) ‰∏∫‰∏Ä‰∏™‰∫ã‰ª∂ÂèëÁîüÁöÑÊ¶ÇÁéá‰πãÂíåÔºõËÆ∞
\(S_2=\sum_{i < j}\mathbb{P}(A_i\cap A_j)\) ‰∏∫‰∏§‰∏™‰∏çÂêå‰∫ã‰ª∂ÂèëÁîüÁöÑÊ¶ÇÁéá‰πãÂíåÔºõ
ËÆ∞ \[S_k=\sum_{i_1 < i_2 < \cdots < i_k}\mathbb{P}(A_{i_1} \cap
A_{i_2} \cap \cdots \cap A_{i_k})\] ‰∏∫ \(k\) ‰∏™‰∏çÂêå‰∫ã‰ª∂ÂèëÁîüÊ¶ÇÁéá‰πãÂíåÔºõÂàô
Êúâ \[\mathbb{P}\biggl(\bigcup_{i=1}^nA_i\biggr)=S_1 - S_2 + S_3 -
\cdots + (-1)^{n+1}S_n.\]
</p>

<p>
<b>ËØÅ.</b> ÂØπ \(n\) ‰ΩúÊï∞Â≠¶ÂΩíÁ∫≥Ê≥ï„ÄÇÊòæÁÑ∂ \(n=2\) Êó∂ÂëΩÈ¢òÊàêÁ´ã„ÄÇÂÅáËÆæÂëΩÈ¢òÂØπ \(n\) ÊàêÁ´ãÔºå
‰∏ãÈù¢ËØÅÂëΩÈ¢òÂØπ \(n+1\) ÊàêÁ´ã„ÄÇ
</p>

<p>
‰∏∫Ê∏ÖÊô∞Ëµ∑ËßÅÔºåÁî® \(S_k(\mathcal{E})\) ËÆ∞‰∫ã‰ª∂Êóè \(\mathcal{E}=\{E_i\}\) ‰∏≠
\(k\) ‰∏™‰∏çÂêå‰∫ã‰ª∂ÂèëÁîüÊ¶ÇÁéá‰πãÂíå„ÄÇ‰ª§ \(\mathcal{A}_n = \{A_i\}_{i=1}^n\) Âèä
\(\mathcal{A}_{n+1}=\{A_i\}_{i=1}^{n+1}\). ÈÇ£‰πàÂ∑≤Áü• \[
\mathbb{P}\biggl(\bigcup_{i=1}^nA_i\biggr)=S_1(\mathcal{A}_n) -
S_2(\mathcal{A}_n) + S_3(\mathcal{A}_n) - \cdots +
(-1)^{n+1}S_n(\mathcal{A}_n). \] Ê¨≤ËØÅ \[
\mathbb{P}\biggl(\bigcup_{i=1}^{n+1}
A_i\biggr)=S_1(\mathcal{A}_{n+1}) - S_2(\mathcal{A}_{n+1}) +
S_3(\mathcal{A}_{n+1}) - \cdots + (-1)^{n+1}S_n(\mathcal{A}_{n+1}). \]
</p>

<p>
‰∏çÈöæÁúãÂá∫ \[ \mathbb{P}\biggl(\bigcup_{i=1}^{n+1} A_i\biggr) =
\mathbb{P}(A_{n+1}) + \mathbb{P}\biggl(\bigcup_{i=1}^{n} A_i\biggr) -
\mathbb{P}\biggl(\bigcup_{i=1}^{n} (A_i\cap A_{n+1})\biggr). \]
</p>

<p>
‰ª§ \(\mathcal{B} = \{A_i\cap A_{n+1}\}_{i=1}^n\). ÈÇ£‰πà
</p>

$$
\begin{aligned}
\mathbb{P}\biggl(\bigcup_{i=1}^{n+1} A_i\biggr)
=& \mathbb{P}(A_{n+1}) + S_1(\mathcal{A}_n) \\
& - S_1(\mathcal{B}) - S_2(\mathcal{A}_n) \\
& + S_2(\mathcal{B}) + S_3(\mathcal{A}_n) \\
& - S_3(\mathcal{B}) - S_4(\mathcal{A}_n) \\
& \cdots \\
& +(-1)^{n+1}S_{n-1}(\mathcal{B}) + (-1)^{n+1}S_n(\mathcal{A}_n) \\
& +(-1)^{n+2}S_{n}(\mathcal{B}).
\end{aligned}
$$

<p>
Ê≥®ÊÑèÂà∞ \(S_k(\mathcal{B}) + S_{k+1}(\mathcal{A}_n) = S_{k+1}(\mathcal{A}_{n+1})\)
Âç≥ÂÆåÊàêËØÅÊòé„ÄÇ
</p>
</div>
</div>
<div id="outline-container-org1e7c44d" class="outline-2">
<h2 id="org1e7c44d">Ê®°ÂûãÂ∫îÁî®</h2>
<div class="outline-text-2" id="text-org1e7c44d">
<p>
‰ª•‰∏ãËÄÉËôëÁªèÂÖ∏ËßÑÂàô: 54 Âº†Áâå, Âú∞‰∏ªÂàÜÂà∞ 20 Âº†, ‰∏§‰∏™ÂÜúÊ∞ëÂêÑ 17 Âº†.
</p>

<ol class="org-ol">
<li><p>
Êë∏Âà∞ÁéãÁâåÁöÑÊ¶ÇÁéá
</p>

<table>


<colgroup>
<col  class="org-left">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">
</colgroup>
<tbody>
<tr>
<td class="org-left">‰∏™Êï∞</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">2</td>
</tr>

<tr>
<td class="org-left">Âú∞‰∏ª</td>
<td class="org-right">0.39</td>
<td class="org-right">0.48</td>
<td class="org-right">0.13</td>
</tr>

<tr>
<td class="org-left">ÂÜúÊ∞ë</td>
<td class="org-right">0.47</td>
<td class="org-right">0.44</td>
<td class="org-right">0.095</td>
</tr>
</tbody>
</table></li>

<li><p>
Êë∏Âà∞ 2 ÁöÑÊ¶ÇÁéá
</p>

<table>


<colgroup>
<col  class="org-left">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">
</colgroup>
<tbody>
<tr>
<td class="org-left">‰∏™Êï∞</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-right">3</td>
<td class="org-right">4</td>
</tr>

<tr>
<td class="org-left">Âú∞‰∏ª</td>
<td class="org-right">0.15</td>
<td class="org-right">0.38</td>
<td class="org-right">0.34</td>
<td class="org-right">0.12</td>
<td class="org-right">0.015</td>
</tr>

<tr>
<td class="org-left">ÂÜúÊ∞ë</td>
<td class="org-right">0.21</td>
<td class="org-right">0.42</td>
<td class="org-right">0.29</td>
<td class="org-right">0.08</td>
<td class="org-right">0.0075</td>
</tr>
</tbody>
</table></li>

<li><p>
Êë∏Âà∞ÁéãÁâåÊàñËÄÖ 2 ÁöÑÊ¶ÇÁéá
</p>

<table>


<colgroup>
<col  class="org-left">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">
</colgroup>
<tbody>
<tr>
<td class="org-left">‰∏™Êï∞</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-right">3</td>
<td class="org-right">4</td>
<td class="org-right">5</td>
<td class="org-right">6</td>
</tr>

<tr>
<td class="org-left">Âú∞‰∏ª</td>
<td class="org-right">0.052</td>
<td class="org-right">0.22</td>
<td class="org-right">0.34</td>
<td class="org-right">0.26</td>
<td class="org-right">0.11</td>
<td class="org-right">0.02</td>
<td class="org-right">0.0015</td>
</tr>

<tr>
<td class="org-left">ÂÜúÊ∞ë</td>
<td class="org-right">0.09</td>
<td class="org-right">0.29</td>
<td class="org-right">0.35</td>
<td class="org-right">0.2</td>
<td class="org-right">0.061</td>
<td class="org-right">0.0089</td>
<td class="org-right">0.00048</td>
</tr>
</tbody>
</table></li>
</ol>
</div>
</div>
<div id="outline-container-orgf604436" class="outline-2">
<h2 id="orgf604436">ËøõÈò∂Â∫îÁî®</h2>
<div class="outline-text-2" id="text-orgf604436">
<p>
<b>ÈóÆ.</b> ÂÜôÂá∫ÁªèÂÖ∏ËßÑÂàô‰∏ãÂú∞‰∏ªÊë∏Âà∞Ëá≥Â∞ë‰∏ÄÁÇ∏ (ÁéãÁÇ∏ÊàñËÄÖÂõõÁÇ∏) ÁöÑÊ¶ÇÁéá. ÂÜúÊ∞ëÂë¢?
</p>

<p>
<b>Á≠î</b>. ËÆ∞ \(A_0\) ‰∏∫‰∫ã‰ª∂ "Âú∞‰∏ªÊë∏Âà∞ÁéãÁÇ∏", \(A_i\) ‰∏∫‰∫ã‰ª∂ "Âú∞‰∏ªÊë∏Âà∞ÁÇ∏ \(i\)" (\(i=1,2,\ldots,13\)). ËÆ∞ \(a_m\) ‰∏∫‰ªé‰∏ÄÁªÑÊúâ \(m\) ‰∏™Ê†áËÆ∞ÁêÉÂÖ± 54 ‰∏™ÁêÉÁöÑÁêÉÂ†Ü‰∏≠ÂèñÂá∫ 20 ‰∏™ÁêÉÊó∂ÊÅ∞Â•ΩÊúâ \(m\) ‰∏™Ê†áËÆ∞ÁêÉÁöÑÊ¶ÇÁéá. ÈÇ£‰πà \[\mathbb{P}(A_0)=a_2,\qquad \mathbb{P}(A_i) = a_4,\quad i=1,2,\ldots,13.\] Ëøõ‰∏ÄÊ≠•Âú∞, Êúâ
</p>

$$
\begin{aligned}
S_1 &= \sum_{i=0}^{13}\mathbb{P}(A_i) = a_2 + 13 a_4,\\
S_2 &= \sum_{0\leq i < j \leq 13}\mathbb{P}(A_i\cap A_j) = \biggl( \sum_{\substack{1\leq j \leq 13\\i=0}} + \sum_{1\leq i <  j \leq 13} \biggr)\mathbb{P}(A_i\cap A_j)= 13 a_6 + \binom{13}{2}a_8,\\
S_3 &= \sum_{0\leq i < j < k \leq 13}\mathbb{P}(A_i\cap A_j\cap A_k) = \biggl( \sum_{\substack{1\leq j < k \leq 13\\i=0}} + \sum_{1\leq i <  j < k \leq 13} \biggr)\mathbb{P}(\cdots)= \binom{13}{2}a_{10} + \binom{13}{3}a_{12},\\
S_4 &= \sum_{0\leq i < j < k < \ell \leq 13}\mathbb{P}(\cdots) = \biggl( \sum_{\substack{1\leq j < k < \ell \leq 13\\i=0}} + \sum_{1\leq i <  j < k < \ell \leq 13} \biggr)\mathbb{P}(\cdots)= \binom{13}{3}a_{14} + \binom{13}{4}a_{16},\\
\cdots & \cdots,\\
S_k &= \binom{13}{k-1}a_{4k-2} + \binom{13}{k}a_{4k}.
\end{aligned}
$$

<p>
ÊòæÁÑ∂, ÂΩì \(k \geq 6\) Êó∂, \(S_k=0\). ÊïÖ \[\mathbb{P}\biggl(\bigcup_{i=0}^{13}A_i\biggr) = S_1 - S_2 + S_3 - S_4 + S_5 \approx 30.4\%.\]
ÂØπ‰∫éÂÜúÊ∞ë, Ëøô‰∏™Êï∞Â≠óÂèò‰∏∫
\[\mathbb{P}\biggl(\bigcup_{i=0}^{13}A'_i\biggr) = S'_1 - S'_2 + S'_3 - S'_4\approx 18.5\%.\]
</p>

<p>
Ê®°ÊãüÈ™åËØÅÁöÑ‰ª£Á†Å <a href="./bombs.py">Âú®Ëøô</a> „ÄÇ
</p>
</div>
</div>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-math.html">math</a> </div>
]]></description>
  <category><![CDATA[math]]></category>
  <link>https://dou-meishi.github.io/org-blog/2022-07-17-PokerProbability/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2022-07-17-PokerProbability/notes.html</guid>
  <pubDate>Sun, 17 Jul 2022 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Helly's Selection Theorem]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org1a30804">Statement</a></li>
<li><a href="#orgf387c0e">Problems</a></li>
<li><a href="#orgab585d5">Further discussion</a></li>
<li><a href="#org1f006fa">External Links&#xa0;&#xa0;&#xa0;<span class="tag"><span class="refs">refs</span></span></a></li>
</ul>
</div>
</nav>
<p>
By Bolzano-Weierstrass‚Äô theorem, we know that any bounded sequence of
real values has a convergent subsequence. This result can be extended
to finite dimensional space, i.e, any bounded sequence in
\(\mathbb{R}^n\) has a convergent subsequence. However, it is not true
in infinite dimensional space, say,
\(\mathbb{R}^{[0,1]}\). Nevertheless, there are two well-known theorem
to establish the convergence of a sequence of functions, the
<a href="https://en.wikipedia.org/wiki/Arzel%C3%A0%E2%80%93Ascoli_theorem">Arzel√†‚ÄìAscoli theorem</a> and <a href="https://en.wikipedia.org/wiki/Helly%27s_selection_theorem">Helly's selection theorem</a>. The main
difference between these two results is the notion of convergence of a
function sequence. Arzel√†‚ÄìAscoli theorem deals with the uniformly
convergence and Helly's selection theorem deals with the pointwise
convergence.
</p>
<div id="outline-container-org1a30804" class="outline-2">
<h2 id="org1a30804">Statement</h2>
<div class="outline-text-2" id="text-org1a30804">
<p>
<i>Lemma (Helly).</i> Suppose that \(\{f_n\}_{n\in\mathbb{N}}\) is a <i>uniformly
bounded</i> sequence of <i>increasing</i> functions on an interval. Then there is
a subsequence converging <i>pointwise</i> to an increasing function.
</p>

<p>
In probability theory, this theorem is often stated in the following manner.
</p>

<p>
<i>Theorem (Helly).</i> For every sequence of \(\{F_n\}\) of distribution
functions from \(\mathbb{R}\) to \([0,1]\), there exists a subsequence
\(\{F_{n_k}\}\) and a nondecreasing, right-continuous function \(F\) such
that \(\lim_k F_{n_k}(x) = F(x)\) at continuity points \(x\) of \(F\).
</p>

<p>
<i>Proof.</i> The proof is based on the <a href="./diagonal-argument.jpg">diagonal argument</a>. See <a href="./proof-Helly-by-Billingsley.png">here</a> for the
complete proof given in Billingsley's book.
</p>

<p>
<i>Remark.</i> The limiting funciton \(F\) may not be a distribution function.
For example, let \(F_n(x) = \mathbb{1}(x \geq n)\) be the distribution
function corresponding to a unit mass at \(n\). Then the limiting
function \(F(x)\equiv0\) is clearly not a distribution function.
</p>

<p>
<i>Remark.</i> The proof of this version can be adapted to prove the original
version. First, it is necessary that \(0 \leq F_n \leq 1\) but \(F_n\)
need not to be a distribution function.  Second, if we do not require
\(F\) to be right-continuous, then we can redefine the values of \(F\) at
discontinuity points, and select a finer subsequence such that \(\lim_k
F_{n_k}(x)=F(x)\) at every \(x\).  This is because the set of
discontinuity points of a monotone function is countable.
</p>
</div>
</div>
<div id="outline-container-orgf387c0e" class="outline-2">
<h2 id="orgf387c0e">Problems</h2>
<div class="outline-text-2" id="text-orgf387c0e">
<ol class="org-ol">
<li>Is there a subsequence of \((\sin (x/k))_{k=1}^\infty\) converges pointwise?</li>
<li>Is there a subsequence of \((\sin (x+k))_{k=1}^\infty\) converges pointwise?</li>
<li>Is there s subsequence of \((\sin (kx))_{k=1}^\infty\) converges pointwise?</li>
</ol>

<p>
Answers to the first two questions are positive, which can be shown
easily by Arzel√†‚ÄìAscoli theorem. The answer to the last question,
however, is negative; see <a href="https://math.stackexchange.com/questions/1380286/pointwise-almost-everywhere-convergent-subsequence-of-sin-nx">this discussion</a>. Hence, the last
one is a perfect example that shows the monotone condition is
essential in Helly's selection theorem.
</p>
</div>
</div>
<div id="outline-container-orgab585d5" class="outline-2">
<h2 id="orgab585d5">Further discussion</h2>
<div class="outline-text-2" id="text-orgab585d5">
<p>
<i>Pointwise convergence is weaker than uniformly convergence.</i> In
\(\mathbb{R}^{\mathbb{N}}\), let \(e^n=(e^n_k)_{k=1}^\infty\) be the
sequence with all zero entries except the \(n\)-th entry be 1. Then
the sequence \((e^n)_{n=1}^\infty\) is bounded under the sup
norm. Moreover, it converges pointwise to \(e^*\equiv0\). However, The
sequence \((e^n)\) is clearly divergent under the sup norm.
Indeed, by the diagonal argument, any uniformly bounded sequence in
\(\mathbb{R}^{\mathbb{N}}\) has a subsequence which converges pointwise.
</p>
</div>
</div>
<div id="outline-container-org1f006fa" class="outline-2">
<h2 id="org1f006fa">External Links&#xa0;&#xa0;&#xa0;<span class="tag"><span class="refs">refs</span></span></h2>
<div class="outline-text-2" id="text-org1f006fa">
<ol class="org-ol">
<li><a href="https://math.stackexchange.com/questions/397931/hellys-selection-theorem">Helly's selection theorem - Mathematics</a></li>
</ol>
</div>
</div>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-math.html">math</a> </div>
]]></description>
  <category><![CDATA[math]]></category>
  <link>https://dou-meishi.github.io/org-blog/2022-06-25-HellySelectionTheorem/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2022-06-25-HellySelectionTheorem/notes.html</guid>
  <pubDate>Sat, 25 Jun 2022 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Backpropagation Formula: An Optimal Control View]]></title>
  <description><![CDATA[
<p>
Consider the following optimal control problem (or equivalently, a constrained optimization problem),
</p>

$$
\begin{aligned}
\min_{u_0,u_1,\ldots,u_{N-1}}\ &\phi(x_N) + \sum_{k=0}^{N-1}g(k,x_k,u_k)\\
\operatorname{s.t.}\quad&x_{k+1} = f(k,x_k,u_k),\qquad k\in ‚ü¶0, N-1‚üß.
\end{aligned}
$$

<p>
With appropriate choices of \(f,g,\phi\) and initial state \(x_0\),
this optimal control problem can be seen as a training step a neural network for a singe sample point \((x^{(i)},y^{(i)})\).
</p>

<ul class="org-ul">
<li>Set \(x_0\) as the input of the neural network, i.e. \(x^{(i)}\);</li>
<li>Set \(u_k\) as the paramters of the \(k\)-th layer;</li>
<li>Set \(f(k,\cdot,u_k)\) as the operation of the \(k\)-th layer.</li>
</ul>

<p>
Thus, \(x_k\) becomes the output of \(k\)-th layer.
Then we need to specify the cost function.
</p>

<ul class="org-ul">
<li>Set \(\phi\) as the loss between \(x_N\) and the target \(y^{(i)}\).</li>
<li>Set \(g(k,x_k,\cdot)\) as the regularization loss of the \(k\)-th layer.</li>
</ul>

<p>
For example, for the widely used MSE loss with \(L_2\) regularization, the loss function is
\[ L(x^{(i)},y^{(i)}) = \|x_N-y^{(i)}\|^2 + \sum_{k=0}^{N-1}\|u_k\|^2, \]
where \(\phi(x_N)=\|x_N-y^{(i)}\|^2\) and \(g(k,x_k,u_k)=\|u_k\|^2\).
</p>

<p>
Back to the genral form. We need to calculate the derivatives of the cost functional (or objective function in optimization, loss function in machine learning) w.r.t \(u_0,u_1,\ldots,u_{N-1}\). Introduce the \(k\)-th tail cost as
\[ J_k := \phi(x_N) + \sum_{i=k}^{N-1}g(i,x_i,u_i),\qquad\forall k\in ‚ü¶0,N‚üß, \]
which can be seen as the function of the input \(x_k\) and hyperparameters \(u_k,u_{k+1},\ldots,u_{N-1}\) and \(J_0\) is the original cost. By induction, it is not hard to show that \(\partial J_k/\partial x_k\) satisfies the following <i>adjoint equation</i>
</p>

$$
\begin{aligned}
\frac{\partial}{\partial x_N}J_N &= \phi'(x_N)\\
\frac{\partial}{\partial x_k}J_k &= \frac{\partial g}{\partial x}(k,x_k,u_k) + \langle\frac{\partial f}{\partial x}(k,x_k,u_k), \frac{\partial}{\partial x_{k+1}}J_{k+1}\rangle,\qquad k\in‚ü¶0,N-1‚üß.
\end{aligned}
$$

<p>
This means the costate \(\partial J_k/\partial x_k\) can be calculated backwardly, i.e., form the last layer \(\phi'(x_N)\) to the very first layer \(\partial J_0/\partial x_0\). With the help of the costate, the rest part is straightforward
</p>

$$
\begin{aligned}
\frac{\partial}{\partial u_k}J_0 &= \frac{\partial}{\partial u_k}J_k\\
&= \frac{\partial}{\partial u_k}\bigl(g(k,x_k,u_k) + J_{k+1}\bigr)\\
&= \frac{\partial g}{\partial u}(k,x_k,u_k) + \frac{\partial x_{k+1}}{\partial u_k}\cdot \frac{\partial}{\partial x_{k+1}}J_{k+1}\\
&= \frac{\partial g}{\partial u}(k,x_k,u_k) + \frac{\partial f}{\partial u}(k,x_k,u_k)\cdot \frac{\partial}{\partial x_{k+1}}J_{k+1},\qquad k\in ‚ü¶0,N-1‚üß.
\end{aligned}
$$

<p>
To conclude, we introduce the Hamiltonian
\[ H(t,x,u,p) = g(t,x,u) + \langle p, f(t,x,u)\rangle. \]
In the calculation of the gradient of the loss function at point \((x^{(i)},y^{(i)})\), the <i>forward</i> phase is firstly executed to obtain the <i>state series</i>
</p>

$$
\begin{aligned}
x_0 &= x^{(i)}\\
x_{k+1} &= \nabla_pH(k,x_k,u_k,p_{k+1}),\qquad k\in ‚ü¶0,N-1‚üß.
\end{aligned}
$$

<p>
Then the <i>costate series</i> is obtained via the <i>backward phase</i>
</p>

$$
\begin{aligned}
p_{N} &= \phi'(x_N)\\
p_{k} &= \nabla_xH(k,x_k,u_k,p_{k+1}),\qquad k\in ‚ü¶0,N-1‚üß.
\end{aligned}
$$

<p>
At last, the gradient is
\[ \frac{\partial}{\partial u_k}J_0 = \nabla_uH(k,x_k,u_k,p_{k+1}),\qquad k\in ‚ü¶0,N-1‚üß.\]
</p>

<p>
<b>Further Readings</b>
</p>

<ul class="org-ul">
<li><a href="https://jmlr.org/papers/volume18/17-653/17-653.pdf">Li, Qianxiao, Long Chen, and Cheng Tai. "Maximum Principle Based Algorithms for Deep Learning." Journal of Machine Learning Research 18 (2018): 1-29.</a></li>
<li><a href="http://proceedings.mlr.press/v80/li18b.html">Li, Qianxiao, and Shuji Hao. "An optimal control approach to deep learning and applications to discrete-weight neural networks". International Conference on Machine Learning. PMLR, 2018.</a></li>
</ul>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-ai.html">ai</a> </div>
]]></description>
  <category><![CDATA[ai]]></category>
  <link>https://dou-meishi.github.io/org-blog/2021-11-07-BackpropagationFormula/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2021-11-07-BackpropagationFormula/notes.html</guid>
  <pubDate>Sun, 07 Nov 2021 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[ÈÖçÁΩÆ Ubuntu Á≥ªÁªü]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org769fedf">Get Ubuntu</a></li>
<li><a href="#orgca0ab58">Appearance</a>
<ul>
<li><a href="#org3f3e7bf">Customize Fonts</a></li>
<li><a href="#orgbc8f2c0">Customize Bash</a></li>
<li><a href="#org9d909e1">GNOME Tweaks</a></li>
<li><a href="#orgd7b95e6">Choose Cinnamon (Deprecated)</a></li>
</ul>
</li>
<li><a href="#orga80761b">Toolkits</a>
<ul>
<li><a href="#orgaef23cb">Install Edge</a></li>
<li><a href="#org89ca1fa">Install VS code</a></li>
<li><a href="#org7798fa9">Install Clash</a></li>
<li><a href="#orgb4ca26a">Install Git</a></li>
<li><a href="#orge414798">Install Emacs</a></li>
<li><a href="#orga0f7d7d">Install Python</a></li>
<li><a href="#org6c98b5d">Install TeXLive</a></li>
<li><a href="#org2c1f4c9">Customize Input Methods (Deprecated)</a></li>
</ul>
</li>
<li><a href="#orge8ca62e">Future Customization</a></li>
<li><a href="#org7f3af81">References&#xa0;&#xa0;&#xa0;<span class="tag"><span class="refs">refs</span></span></a></li>
</ul>
</div>
</nav>
<p>
Áî®‰∫Ü‰∫îÂπ¥ÁöÑÂ∞èÁ†¥Êú¨‰ªäÊó©ÂèàÁΩ¢Â∑•‰∫Ü.
Á≥ªÁªüÂêØÂä®Âç°Âú®Êüê‰∏™Á°¨ÁõòÊåÇËΩΩÂá∫Èîô.
Ë¥π‰∫ÜËÄÅÂä≤ÂÑø, ‰∏ÄÂ∫¶‰ª•‰∏∫Ëøô‰ΩçÊàòÂèãÁªà‰∫éËµ∞Âà∞ÂØøÂëΩÂ∞ΩÂ§¥,
ËøòÊÑüÊÖ®Ââç‰∏§Â§©ÊâçÂ•∂‰∫Ü‰∏ÄÂè£ËÉΩÂÜçÊàòÂá†Âπ¥Âë¢.
ÊúÄÂêéÂèëÁé∞‰ºº‰πéÊòØ UEFI ÂàÜÂå∫ÊåÇ‰∫Ü,
Âú® BIOS ÈáåË∞ÉÊàê Legacy Ê®°ÂºèÂ∞±ËÉΩÈ°∫Âà©ÂêØÂä®.
Â∞ΩÁÆ°ÁúãËµ∑Êù•ÊòØÂõûÂà∞‰∫Ü‰∏ÄÂ§©ÂâçÁöÑÊ†∑Â≠ê,
ËÉΩË∑ëËÉΩË∑≥ÁöÑ,
‰ΩÜÊàëÁü•ÈÅì,
ÁúãËµ∑Êù•ÊµÅÁïÖËøêË°åÁöÑË°®Èù¢‰∏ãÊòØÂ∑≤ÁªèÊúçÂΩπ‰∫îÂπ¥Â§öÁöÑÊÆãË∫Ø.
</p>

<p>
Áªà‰∫é, Êàë‰∏ãÂÆöÂÜ≥ÂøÉÂ∞ÜÂÆûÈ™åÂÆ§ÁîµËÑë‰πüÈÖçÁΩÆÊàêÂÆÉÁöÑÂΩ¢Áä∂.
‰ª•ÂêéÂ∞±ËÆ©Ëøô‰ΩçËÄÅÊàòÂèãÈÄÄÂ±Ö‰∫åÁ∫øÂêß.
</p>
<div id="outline-container-org769fedf" class="outline-2">
<h2 id="org769fedf">Get Ubuntu</h2>
<div class="outline-text-2" id="text-org769fedf">
<p>
Âú® Ubuntu ÂÆòÁΩë‰∏ãËΩΩ 20.04 LTS ÁâàÊú¨,
Áî® Universal USB Installer Â∞Ü <code>.iso</code> Êñá‰ª∂ÁÉßÂΩïÂà∞ U Áõò‰∏ä.
</p>

<blockquote>
<p>
22.04 LTS Áâà Ubuntu ÁöÑÂÆâË£ÖÊé®ËçêÂèÇËÄÉ <a href="https://ubuntu.com/tutorials/install-ubuntu-desktop#1-overview">ÂÆòÊñπÊåáÂçó</a>
</p>
</blockquote>

<p>
ÂêØÂä®ÁõòÂÅöÂ•ΩÂêéÁõ¥Êé•ÈÅµÁÖßÂºïÂØºÂÆâË£Ö,
Âõ†‰∏∫ÊáíÂæóËá™Â∑±ËßÑÂàíÂàÜÂå∫,
Â∞±Âπ≤ËÑÜÂ∞ÜÊâÄÊúâÊñáÊ°£Â§á‰ªΩÂêéÈÄâÊã©Ê†ºÂºèÂåñÊï¥‰∏™Á°¨Áõò,
‰∫§Áªô Ubuntu Ëá™Â∑±ÂàíÂàÜ.
(Ëøô‰∏ÄÊ≠•Â±ÖÁÑ∂Ê≤°Âá∫‰ªÄ‰πàÂ≤îÂ≠ê,
ÁùÄÂÆûËÆ©ÊàëÊúâÁÇπÂ∞èÊÉäËÆ∂.)
ÊúÄÂêéÊ≠£Â∏∏ÁôªÈôÜ,
</p>

<p>
Á≥ªÁªüÂÆâË£Ö, ÂÆåÊàê!
</p>
</div>
</div>
<div id="outline-container-orgca0ab58" class="outline-2">
<h2 id="orgca0ab58">Appearance</h2>
<div class="outline-text-2" id="text-orgca0ab58">
<p>
ÂÆâË£ÖÂÆåÊàêÂêéÈ¶ñÂÖàÊõ¥Êñ∞
</p>

<div class="org-src-container">
<pre class="src src-bash">sudo apt update
sudo apt upgrade
</pre>
</div>

<p>
ÁÑ∂ÂêéÊµèËßà `ËÆæÁΩÆ` Âπ∂Ê†πÊçÆÂñúÊ¨¢ËøõË°åË∞ÉÊï¥.
</p>

<p>
Âú® <code>~/.config/user-dirs.dirs</code> Èáå‰øÆÊîπÈªòËÆ§‰∏ãËΩΩË∑ØÂæÑ, ÈªòËÆ§Ê°åÈù¢Ë∑ØÂæÑ, ÈªòËÆ§ÂõæÁâáË∑ØÂæÑÁ≠âÁ≠â.
</p>
</div>
<div id="outline-container-org3f3e7bf" class="outline-3">
<h3 id="org3f3e7bf">Customize Fonts</h3>
<div class="outline-text-3" id="text-org3f3e7bf">
<p>
‰ªéÂ∞èÁ†¥Êú¨ÈáåÂ∞Ü <code>~/.local/share/fonts/</code> Â§çÂà∂Âà∞‰∏ªÁõÆÂΩï.
ÁÑ∂ÂêéÂà∑Êñ∞ÁºìÂ≠ò
</p>

<div class="org-src-container">
<pre class="src src-bash">sudo fc-cache -fsv
</pre>
</div>

<p>
Áî® <code>dconf-editor</code> Êõ¥ÊîπÁªàÁ´ØÂ≠ó‰Ωì‰∏∫ FiraCode Nerd Font Mono Ret 14
</p>
</div>
</div>
<div id="outline-container-orgbc8f2c0" class="outline-3">
<h3 id="orgbc8f2c0">Customize Bash</h3>
<div class="outline-text-3" id="text-orgbc8f2c0">
<p>
Â∞Ü <code>.bashrc</code>, <code>.bash_env</code>, <code>.bash_aliases</code>, <code>.inputrc</code>, <code>bin/</code> Â§çÂà∂Âà∞‰∏ªÁõÆÂΩï.
</p>

<p>
ÁÑ∂ÂêéÈÖçÁΩÆ GNOME ÁªàÁ´ØÈ¢úËâ≤.
</p>

<div class="org-src-container">
<pre class="src src-bash">sudo apt install dconf-cli git
git clone https://github.com/aruhier/gnome-terminal-colors-solarized.git
<span style="color: #657b83; font-weight: bold;">cd</span> gnome-terminal-colors-solarized
./install.sh
</pre>
</div>

<p>
Ë£ÖÂ•ΩÂêéËøêË°å <code>set_light.sh</code> (ÊàñËÄÖ <code>set_dark.sh</code>).
</p>

<p>
ÁÑ∂ÂêéÂÆâË£Ö <code>oh-my-posh</code>
</p>

<div class="org-src-container">
<pre class="src src-bash">curl -s https://ohmyposh.dev/install.sh | bash -s -- -d ~/bin
</pre>
</div>

<p>
Â§çÂà∂ <code>~/.config/oh-my-posh/</code> Âà∞ÂØπÂ∫î‰ΩçÁΩÆ.
</p>

<p>
ÈáçÂêØÁªàÁ´ØÂç≥ÈÖçÁΩÆÂÆåÊàê.
</p>
</div>
</div>
<div id="outline-container-org9d909e1" class="outline-3">
<h3 id="org9d909e1">GNOME Tweaks</h3>
<div class="outline-text-3" id="text-org9d909e1">
<p>
Ubuntu ÁöÑÂæàÂ§öÁ≥ªÁªüÂ§ñËßÇËÆæÁΩÆÈúÄË¶Å‰ΩøÁî® GNOME TweaksÔºå
ÂèÇËÄÉ <a href="https://linuxhint.com/gnome_tweak_installation_ubuntu/">How to Install the Gnome Tweak Tool on Ubuntu 22.04</a>.
</p>

<div class="org-src-container">
<pre class="src src-bash">sudo add-apt-repository universe
sudo apt install gnome-tweaks
</pre>
</div>

<p>
Êé•ÁùÄÂÆâË£Ö GNOME Shell Extensions, ÂèÇËÄÉ <a href="https://itsfoss.com/gnome-shell-extensions/">How to Use GNOME Shell Extensions [Complete Guide</a>].
</p>

<div class="org-src-container">
<pre class="src src-bash">sudo apt install gnome-shell-extensions
</pre>
</div>

<p>
‰∏Ä‰∫õ <del>ÊúâÁî®</del> ÊúâË∂£ÁöÑÊâ©Â±ïÂåÖÊã¨
</p>

<ul class="org-ul">
<li>Dash to Dock. ÂèØ‰ª•Ëá™ÂÆö‰πâ‰ªªÂä°Ê†èÂ§ñËßÇÂíåË°å‰∏∫.</li>
<li>User Themes. ÂèØ‰ª•ÈÄâÊã©Êõ¥Â§öÁöÑÁ≥ªÁªü‰∏ªÈ¢ò. ‰∏™‰∫∫ÂÅèÁà± <a href="https://www.gnome-look.org/p/1099856/">Ant alt style</a> (Application theme) + <a href="https://www.gnome-look.org/p/1360254">Oreo Pink</a> (Cursor theme) + <a href="https://www.pling.com/s/Gnome/p/1305251">Candy icons</a> (Icon theme).</li>
<li>Gesture Improvements. ÂèØ‰ª•Ëá™ÂÆö‰πâÊõ¥Â§öÁöÑËß¶Êë∏ÊùøÊâãÂäø.</li>
<li>Burn My Windows. ÂèØ‰ª•Ëá™ÂÆö‰πâÁ™óÂè£ÊâìÂºÄÂíåÂÖ≥Èó≠ÁöÑÂä®Áîª.</li>
<li>Desktop Cube. Workspace 3D ÂàáÊç¢Âä®Áîª.</li>
<li>Compiz alike magic lamp effect. MacOS-like Á™óÂè£ÊúÄÂ∞èÂåñÂä®Áîª.</li>
<li>Coverflow Alt-Tab. Alt-Tab 3D ÂàáÊç¢Âä®Áîª.</li>
</ul>
</div>
</div>
<div id="outline-container-orgd7b95e6" class="outline-3">
<h3 id="orgd7b95e6">Choose Cinnamon (Deprecated)</h3>
<div class="outline-text-3" id="text-orgd7b95e6">
<p>
<del>‰∏™‰∫∫Êõ¥‰π†ÊÉØ Win È£éÊ†ºÁöÑ UI
‰∫éÊòØÂÅèÁà± Cinnamon Ê°åÈù¢ÁéØÂ¢É</del>
<b>Áé∞Âú®Êõ¥ÂñúÊ¨¢ Ubuntu 22.04 LTS Jellyfish Ëá™Â∏¶ UI.</b>
</p>

<p>
Âú®ÂëΩ‰ª§Ë°åÂÆâË£ÖÂêé, ÁôªÈôÜÁïåÈù¢Âè≥‰∏ãËßíÈÄâÊã© Cinnamon ÂêØÂä®Âç≥ÂèØ.
</p>

<div class="org-src-container">
<pre class="src src-bash">sudo apt install cinnamon-desktop-environment
</pre>
</div>

<p>
ËøõÂÖ• Cinnamon ÂêéÊâæÂà∞ Á≥ªÁªüËÆæÁΩÆ -&gt; ‰∏ªÈ¢ò,
Èöè‰æøË∞É‰∏ÄË∞É.
</p>
</div>
</div>
</div>
<div id="outline-container-orga80761b" class="outline-2">
<h2 id="orga80761b">Toolkits</h2>
<div class="outline-text-2" id="text-orga80761b">
</div>
<div id="outline-container-orgaef23cb" class="outline-3">
<h3 id="orgaef23cb">Install Edge</h3>
<div class="outline-text-3" id="text-orgaef23cb">
<p>
‰∏ãËΩΩ Edge, ËÆæÁΩÆÂ≠ó‰Ωì Fira Sans + Lora, ÁôªÈôÜÂ∏êÂè∑.
</p>
</div>
</div>
<div id="outline-container-org89ca1fa" class="outline-3">
<h3 id="org89ca1fa">Install VS code</h3>
<div class="outline-text-3" id="text-org89ca1fa">
<p>
‰∏ãËΩΩ VS Code, ÁôªÂΩïË¥¶Âè∑. See also <a href="../2023-04-06-IntroVSCode/notes.html">this note</a> for a brief intro to VS code.
</p>
</div>
</div>
<div id="outline-container-org7798fa9" class="outline-3">
<h3 id="org7798fa9">Install Clash</h3>
<div class="outline-text-3" id="text-org7798fa9">
<p>
ÂáÜÂ§áÁßëÂ≠¶‰∏äÁΩë.
‰ªéÂ∞èÁ†¥Êú¨‰∏äÂ§çÂà∂ <del>ÊàñËÄÖ‰ªé github ‰∏ä‰∏ãËΩΩ</del> Âà∞ <del>ÊúÄÊñ∞ÁâàÊú¨ÁöÑ</del> clash,
ÁÑ∂ÂêéËøõË°åÂ¶Ç‰∏ãÈÖçÁΩÆ
</p>

<div class="org-src-container">
<pre class="src src-bash">mkdir ~/clash
mv ~/Download/clash-linux-amd64-v1.7.1 ~/clash/clash
<span style="color: #657b83; font-weight: bold;">cd</span> ~/clash/
sudo chmod a+x ./clash
</pre>
</div>

<p>
ÁÑ∂Âêé‰ªé‰ª£ÁêÜÂïÜÈÇ£ÈáåÊãøÂà∞ÈÖçÁΩÆÊñá‰ª∂Âπ∂Ë¶ÜÁõñ <code>~/clash/config.yaml</code>,
Âú®Êú¨Âú∞ÁΩëÁªúËøûÊé•ÈáåËÆæÁΩÆ‰ª£ÁêÜÁ´ØÂè£.
ÊúÄÂêéÂêØÂä® clash Âπ∂ÁôªÈôÜÁΩëÂùÄ <a href="http://clash.razord.top/#/settings">http://clash.razord.top/#/settings</a> or <a href="https://yacd.haishan.me/">https://yacd.haishan.me/</a> ËøõË°åËäÇÁÇπÈÄâÊã©.
</p>
</div>
</div>
<div id="outline-container-orgb4ca26a" class="outline-3">
<h3 id="orgb4ca26a">Install Git</h3>
<div class="outline-text-3" id="text-orgb4ca26a">
<p>
ÂÆâË£Ö <code>git</code> ÂêéÁôªÈôÜ GitHub, ÊåâÁÖßÊèêÁ§∫ÁîüÊàê SSH key Âπ∂Ê∑ªÂä†.
See also <a href="../2023-04-05-ConfigGit/notes.html">this note</a> for details.
</p>

<p>
ÂÆâË£Ö diff Á®ãÂ∫èÁöÑËØ≠Ê≥ïÈ´ò‰∫ÆÁâà <a href="https://github.com/dandavison/delta">delta</a>.
</p>

<p>
Â∞Ü <code>~/.gitconfig</code> Â§çÂà∂Âà∞ÂØπÂ∫î‰ΩçÁΩÆ.
</p>

<p>
<del>ÈÖçÁΩÆ <a href="https://github.com/magicmonty/bash-git-prompt">bash git prompt</a></del> (<code>oh-my-posh</code> Â∑≤ÈíàÂØπ <code>git</code> ‰ªìÂ∫ìÂÅö‰∫Ü prompt segment, ËøôÈáåÊó†ÈúÄÂÜçÊ¨°ÈÖçÁΩÆ)
</p>
</div>
</div>
<div id="outline-container-orge414798" class="outline-3">
<h3 id="orge414798">Install Emacs</h3>
<div class="outline-text-3" id="text-orge414798">
<p>
Ê∑ªÂä†Ê∫êÂπ∂‰∏ãËΩΩÊúÄÊñ∞ÁöÑ Emacs
</p>

<div class="org-src-container">
<pre class="src src-bash">sudo add-apt-repository ppa:ubuntu-elisp/ppa
sudo apt update
sudo apt install emacs-snapshot
</pre>
</div>

<p>
‰∏ãËΩΩÂêéÂ§çÂà∂Â∞èÁ†¥Êú¨ <code>~/.emacs.d</code> Âà∞ÂØπÂ∫î‰ΩçÁΩÆ.
</p>
</div>
</div>
<div id="outline-container-orga0f7d7d" class="outline-3">
<h3 id="orga0f7d7d">Install Python</h3>
<div class="outline-text-3" id="text-orga0f7d7d">
<p>
Ubuntu 20.04 LTS ÊòØËá™Â∏¶ <code>Python3.8</code> ÁöÑ,
‰ΩÜ‰∏Ä‰∫õÁ¨¨‰∏âÊñπÂåÖËøòÊòØÈúÄË¶ÅËá™Â∑±Ë£Ö.
</p>

<div class="org-src-container">
<pre class="src src-bash">sudo apt install python3-pip
pip3 install --user numpy matplotlib pandas jupytext
</pre>
</div>

<p>
ÁÑ∂ÂêéÂÆâË£Ö <a href="https://github.com/dunovank/jupyter-themes">jupyter-themes</a> Âπ∂ËøõË°å‰∏ÄÂÆöÁöÑÈÖçÁΩÆ.
</p>

<div class="org-src-container">
<pre class="src src-bash">pip3 install --user jupyterthemes
jt -t solarizedd -T -N -kl -f dejavu -fs 140
pip3 install --user jupyter_contrib_nbextensions
jupyter contrib nbextension install --user
</pre>
</div>

<p>
ÂêØÂä® jupyter ÂêéÂç≥ÂèØÊâãÂä®ÈÄâÊã©ÂºÄÂêØÂêÑÁ±ªÊâ©Â±ï.
</p>
</div>
</div>
<div id="outline-container-org6c98b5d" class="outline-3">
<h3 id="org6c98b5d">Install TeXLive</h3>
<div class="outline-text-3" id="text-org6c98b5d">
<p>
See official doc
</p>
</div>
</div>
<div id="outline-container-org2c1f4c9" class="outline-3">
<h3 id="org2c1f4c9">Customize Input Methods (Deprecated)</h3>
<div class="outline-text-3" id="text-org2c1f4c9">
<p>
<b>Ubuntu 22.04 LTS Ëá™Â∏¶ÁöÑËæìÂÖ•Ê≥ïÂ∑≤ÁªèË∂≥Â§ü‰ΩøÁî®Ôºå‰∏çÂøÖÂÜçËøõË°åÂêéÁª≠ÈÖçÁΩÆ</b>
</p>

<p>
Âç∏ËΩΩ <code>ibus</code> ÂêéÂÆâË£Ö <code>fcitx</code>, ÈáçÂêØÂêéÂè≥‰∏ãËßíÁÇπÂáªÂ∞è‰ºÅÈπÖÂõæÊ†áËøõ‰∏ÄÊ≠•ÈÖçÁΩÆ.
</p>

<div class="org-src-container">
<pre class="src src-bash">sudo apt purge ibus
sudo apt install fcitx
sudo apt install fcitx-googlepinyin
sudo apt install fcitx-config-gtk
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-orge8ca62e" class="outline-2">
<h2 id="orge8ca62e">Future Customization</h2>
<div class="outline-text-2" id="text-orge8ca62e">
<ul class="org-ul">
<li>Customize titlebar</li>
<li>More funny animations</li>
</ul>
</div>
</div>
<div id="outline-container-org7f3af81" class="outline-2">
<h2 id="org7f3af81">References&#xa0;&#xa0;&#xa0;<span class="tag"><span class="refs">refs</span></span></h2>
<div class="outline-text-2" id="text-org7f3af81">
<ul class="org-ul">
<li><a href="https://einverne.github.io/post/2021/03/linux-use-clash.html">Âú® Linux ‰∏ä‰ΩøÁî® Clash ‰Ωú‰ª£ÁêÜ</a></li>
<li><a href="http://www.webupd8.org/2011/04/solarized-must-have-color-paletter-for.html">SOLARIZED: A MUST HAVE COLOR SCHEME FOR GNOME TERMINAL, VIM, GEDIT AND LOTS MORE</a></li>
<li><a href="https://stackoverflow.com/questions/36419342/how-to-wrap-code-text-in-jupyter-notebooks">How to wrap code/text in Jupyter notebooks</a></li>
</ul>
</div>
</div>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-tool.html">tool</a> </div>
]]></description>
  <category><![CDATA[tool]]></category>
  <link>https://dou-meishi.github.io/org-blog/2021-10-25-CustomizeUbuntu/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2021-10-25-CustomizeUbuntu/notes.html</guid>
  <pubDate>Mon, 25 Oct 2021 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Convolution in CNN]]></title>
  <description><![CDATA[
<nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org86ca42d">Input/Output</a></li>
<li><a href="#orgd8401fb">Parameters</a>
<ul>
<li><a href="#org18a189c">stride</a></li>
<li><a href="#orgcce0330">padding</a></li>
<li><a href="#org2f7da97">dilation</a></li>
</ul>
</li>
<li><a href="#org4b5b062">Correlation and Convolution</a></li>
<li><a href="#orgda9a597">References</a></li>
</ul>
</div>
</nav>
<p>
ËøôÁØáÁ¨îËÆ∞ÊòØÂØπ <code>torch.nn.Conv2d</code> [ <a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d">ÂÆòÊñπÊñáÊ°£</a> ] ÁöÑËß£ÈáäÔºå
‰∏ªË¶ÅÁõÆÊ†áÊòØÂºÑÊ∏ÖÊ•öÂú®ËøôÂ±Ç layer ‰∏≠ÂèëÁîü‰∫Ü‰ªÄ‰πàÔºö
ËæìÂÖ•ËæìÂá∫ÊòØ‰ªÄ‰πàÔºüÊòØÊÄé‰πà‰ªéËæìÂÖ•Âà∞ËæìÂá∫ÁöÑÔºü
ÊúÄÂêéË°•ÂÖÖ‰∫Ü‰∏Ä‰∏™Âõ∞Êâ∞ÊàëÂæà‰πÖÁöÑÈóÆÈ¢òÔºö
‰∏∫‰ªÄ‰πàË¶ÅÂè´ convolution?
</p>
<div id="outline-container-org86ca42d" class="outline-2">
<h2 id="org86ca42d">Input/Output</h2>
<div class="outline-text-2" id="text-org86ca42d">
<p>
ÊñáÊ°£‰∏≠ÊòéÁ°ÆÁªôÂá∫‰∫ÜËæìÂÖ•ËæìÂá∫ÁöÑÁ±ªÂûãÔºöÊúâÂõõ‰∏™Áª¥Â∫¶ (N, C, H, W) ÁöÑ tensor,
Áî® NumPy ÁöÑÊúØËØ≠Êù•ËØ¥Â∞±ÊòØ 4d array.
ÂÖ∂‰∏≠
</p>

<ul class="org-ul">
<li>N: Batch size, Âç≥Ê†∑Êú¨ÁöÑ‰∏™Êï∞ÔºåÊàñËÄÖËØ¥‰∏ÄÊ¨°ËÆ°ÁÆóÊìç‰ΩúÁöÑÂõæÁâáÂº†Êï∞„ÄÇ
Ê≥®ÊÑèÔºåN ÁöÑÂ§ßÂ∞èÂÆåÂÖ®‰∏çÂΩ±ÂìçÁΩëÁªúÁªìÊûÑÔºå
ÊâÄ‰ª•ÂÆÉ‰πüÊ≤°ÊúâÂá∫Áé∞Âú®ÊûÑÈÄ†ÁΩëÁªúÊâÄÈúÄÁöÑÂèÇÊï∞Ë°®Èáå„ÄÇ
‰∏™‰∫∫ËÆ§‰∏∫Ëøô‰∏™Áª¥Â∫¶ÁöÑÂ≠òÂú®ÂÆåÂÖ®ÊòØ‰∏∫‰∫ÜËÆ°ÁÆó‰∏äÁöÑÊñπ‰æø„ÄÇ
Âõ†‰∏∫ÂØπÊï∞ÊçÆÂ∫ì‰∏≠‰∏çÂêåÁöÑÂõæÁâáËøõË°åÁöÑÊìç‰ΩúÊòØÂÆåÂÖ®‰∏ÄÊ†∑ÁöÑÔºå
ÊâÄ‰ª•Âú®ËÆ≠ÁªÉÊàñËÄÖÈ¢ÑÊµãÊó∂‰∏çÂ¶®Áõ¥Êé•Â§öÂº†ÂêåÊó∂ËÆ°ÁÆó„ÄÇ
ÊòæÁÑ∂ÔºåËøô‰∏™Áª¥Â∫¶ÁöÑÂ§ßÂ∞èÂßãÁªà‰∏çÂèò„ÄÇ
ËæìÂÖ•ÊúâÂ§öÂ∞ëÂº†ÂõæÁâáÔºå
ËæìÂá∫Â∞±ÊòØÂ§öÂ∞ëÂº†ÂõæÁâáÂØπÂ∫îÁöÑËÆ°ÁÆóÁªìÊûú„ÄÇ
ÂÆûÈôÖ‰ΩøÁî®Êó∂ËÆ©Ëøô‰∏™Áª¥Â∫¶ÂßãÁªà‰∏∫ 1 ‰πüÂèØ„ÄÇ
ÂΩìÁÑ∂ÔºåÂ¶ÇÊûúË¶ÅËøõË°å BatchNorm Êìç‰ΩúÔºå
ÈÇ£ËæìÂÖ•Ê†∑Êú¨Êï∞ÂøÖÈ°ªÂ§ß‰∫é 1.</li>

<li>C: Channel size, Âç≥ÂõæÁâáÁöÑÈÄöÈÅìÊï∞„ÄÇ
Ëã•ËæìÂÖ•ÊòØ RGB ÂõæÂÉèÔºåC<sub>in</sub> Â∞±ÊòØ 3ÔºåËã•ÊòØÁÅ∞Â∫¶ÂõæÂÉèÂàô‰∏∫ 1„ÄÇ
Ê≥®ÊÑèÔºåËæìÂá∫ÁöÑ Channel size ÊòØ‰∏ç‰∏ÄÊ†∑ÁöÑÔºåC<sub>out</sub> Â∫îÂΩìÁêÜËß£‰∏∫ÁâπÂæÅÁöÑÈÄöÈÅì„ÄÇ
ÂêéÈù¢‰ºöÊèêÂà∞ÔºåÂú®ËÆ°ÁÆóÊØè‰∏Ä‰∏™ËæìÂá∫ÈÄöÈÅìÁöÑÁªìÊûúÊó∂ÔºåÊâÄÊúâËæìÂÖ•ÈÄöÈÅìÁöÑÂõæÂÉèÊï∞ÊçÆÈÉΩË¢´Áî®Âà∞‰∫Ü„ÄÇ
‰πüÂ∞±ÊòØËØ¥Ôºå <b>Âπ∂‰∏çÊòØ</b> ÂØπ R G B ‰∏â‰∏™ÈÄöÈÅìÁöÑÂõæÂÉèÂàÜÂà´ËøõË°åÂêåÊ†∑ÁöÑÂ§ÑÁêÜ„ÄÇ
ËøôÊ†∑Âç≥‰ΩøËæìÂá∫Âè™Êúâ 1 ‰∏™ÈÄöÈÅìÔºåËÆ°ÁÆóÁªìÊûú‰πü‰ºöËÄÉËôëÈ¢úËâ≤Â∏¶Êù•ÁöÑÂΩ±Âìç„ÄÇ
Âú®ÊûÑÈÄ† Conv2d layer Êó∂ÁöÑ‰∏§‰∏™ÂøÖË¶ÅÂèÇÊï∞Â∞±ÊòØ C<sub>in</sub> Âíå C<sub>out</sub>„ÄÇ
ÂÆûÈôÖ‰ΩøÁî®Êó∂‰∏ÄËà¨‰πü‰ªÖÈúÄÊåáÂÆöËøô‰∏§‰∏™ÂèÇÊï∞ (Â§ñÂä†‰∏Ä‰∏™ kernel size).</li>

<li>H &amp; W: ÂõæÁâáÈ´òÂ∫¶ÂíåÂÆΩÂ∫¶„ÄÇÊ≥®ÊÑèÔºå height ‰ª£Ë°®ÁöÑÁª¥Â∫¶ÊòØÂú® width ‰πãÂâçÁöÑ„ÄÇ
ÊâÄ‰ª•ÂÉèÁ¥†ÂùêÊ†á (h, w) ‰ª£Ë°®ÁöÑÊòØÂõæÂÉè‰ªéÂ∑¶‰∏äËßíÂºÄÂßãÊï∞ÔºåÂêëÂè≥Êï∞Á¨¨ w Ê†ºÔºå
ÂæÄ‰∏ãÊï∞ h Ê†ºÁöÑÂÉèÁ¥†ÁÇπ„ÄÇÂíåÊï∞Â≠¶‰∏≠Â∏∏ËßÅÁöÑÁ¨¨‰∏Ä‰∏™Áª¥Â∫¶‰ª£Ë°®Ê®™ËΩ¥ÔºåÁ¨¨‰∫å‰∏™Áª¥Â∫¶‰ª£Ë°®Á∫µËΩ¥
ÈùûÂ∏∏‰∏çÂêå„ÄÇËøô‰∏§‰∏™Áª¥Â∫¶ÁöÑÂ§ßÂ∞èÂú®ËæìÂÖ•ËæìÂá∫‰∏≠‰∏çÂøÖÁõ∏Âêå„ÄÇ
H<sub>out</sub>, W<sub>out</sub> ÊòØ‰æùÊçÆ H<sub>in</sub>, W<sub>in</sub> Âíå‰∏Ä‰∫õÂÖ∂ÂÆÉÂèÇÊï∞Ëá™Âä®Á°ÆÂÆöÁöÑ„ÄÇ
ËøôÁßç‰æùËµñÂÖ≥Á≥ªÊòØ convolution Êìç‰ΩúÊú¨Ë∫´ÂÜ≥ÂÆöÁöÑÔºåÊâÄ‰ª•‰∏çËÉΩÁõ¥Êé•ÊåáÂÆöËæìÂá∫ÁöÑÂ§ßÂ∞èÔºå
ÈúÄË¶ÅÈÄöËøáË∞ÉÊï¥ÂÖ∂‰ªñÂèÇÊï∞Êù•Èó¥Êé•ÂÆûÁé∞„ÄÇ</li>
</ul>

<p>
ÊÄªÁöÑÊù•ËØ¥ÔºåÈô§ÂéªÁ¨¨‰∏Ä‰∏™Áª¥Â∫¶ N, ËæìÂÖ•ÂíåËæìÂá∫ÁöÑ shape ÊòØÈùûÂ∏∏‰∏çÂêåÁöÑ.
C<sub>in</sub>, C<sub>out</sub> ÈúÄË¶ÅÂú®ËÆæËÆ°ÁΩëÁªúÊó∂Â∞±Á°ÆÂÆöÂ•ΩÔºå
H<sub>out</sub>, W<sub>out</sub> Âàô‰æùËµñ‰∫é H<sub>in</sub> Âíå W<sub>out</sub>.
</p>
</div>
</div>
<div id="outline-container-orgd8401fb" class="outline-2">
<h2 id="orgd8401fb">Parameters</h2>
<div class="outline-text-2" id="text-orgd8401fb">
<p>
Âú®‰ªãÁªç Conv2d ÁöÑÂêÑÁßçÂèÇÊï∞‰πãÂâçÔºåÂÖàÁÆÄÂçïÂú∞‰ªãÁªçËæìÂá∫ÊòØÊÄéÊ†∑‰ªéËæìÂÖ•ÁÆóÂæóÁöÑ„ÄÇÂÆòÊñπ
ÊñáÊ°£‰∏≠ÁªôÂá∫ÁöÑÂÖ¨ÂºèÊòØ \[ output(N_i, C_{outj}) = bias(C_{outj}) +
\sum_{k}^{C_{in}-1} weight(C_{outj},k) \star input(N_i, k) \] ÂÖ∂‰∏≠
\(\star\) ÊòØ <b>correlation</b> ÁÆóÁ¨¶„ÄÇ
</p>

<p>
Âõ†‰∏∫ input, output ÈÉΩÊòØ 4d array, ÊâÄ‰ª•ÊåáÂÆö N, C ‰∏§‰∏™Áª¥Â∫¶ÂêéÂæóÂà∞ÁöÑÂÖ∂ÂÆûÊòØÁü©Èòµ„ÄÇ
‰∏äÂºèÂÖ∂ÂÆûÊòØÁü©ÈòµÁ≠âÂºè„ÄÇ
ÁÑ∂ÂêéÊàë‰ª¨ËßÇÂØüÂà∞ weight Âíå N Êó†ÂÖ≥Ôºå
ËøôÂú®ÂâçÈù¢Â∑≤ÁªèËß£Èáä‰∫ÜÔºåÂõ†‰∏∫ N Ëøô‰∏™Áª¥Â∫¶Áõ∏ÂΩì‰∫éÊï∞ÊçÆÂ∫ì‰∏≠Ê†∑Êú¨ÁöÑÁºñÂè∑Ôºå
ÂΩìÁÑ∂‰∏çÂèØËÉΩÊØè‰∏™Ê†∑Êú¨ÈÉΩÂØπÂ∫î‰∏ÄÂ•óÁΩëÁªúÂèÇÊï∞„ÄÇ
‰ΩÜËøô‰∏ç‰ª£Ë°® weight Â∞±Â∞ë‰∫Ü‰∏Ä‰∏™Áª¥Â∫¶ÔºåÂÆÉ‰ªçÁÑ∂ÊòØ 4d array,
Âè™‰∏çËøá shape ÂíåËæìÂÖ•ËæìÂÖ•Á®çÊúâ‰∏çÂêåÔºåÊòØ C<sub>out</sub>, C<sub>in</sub>, K, K.
ËÄå K ÊòØ kernel size.
Â¶ÇÊûúÊàë‰ª¨ËÆ© N Âíå C<sub>out</sub> Âùá‰∏∫ 1,
ÈÇ£‰πà output.shape Â∞±ÊòØ 1, 1, H<sub>out</sub>, W<sub>out</sub>,
Á≠â‰ª∑‰∫é‰∫åÁª¥Áü©Èòµ„ÄÇ
Ê≠§Êó∂ÁöÑËÆ°ÁÆóËøáÁ®ãÁ≠â‰ª∑‰∫éÁî® C<sub>in</sub> ‰∏™ K x K Â§ßÂ∞èÁöÑ kernel ÂàÜÂà´ÂØπ
ÊØè‰∏™ËæìÂÖ• channel ÁöÑÂõæÂÉèÁü©Èòµ‰ΩúÊª§Ê≥¢ÁÑ∂Âêé <b>Âä†ÊùÉÊ±ÇÂíå</b> „ÄÇ
</p>

<p>
Âú®ÊúÄÁÆÄÂçïÁöÑÊÉÖÂΩ¢Ôºå
N, C<sub>out</sub>, C<sub>in</sub> ÈÉΩÊòØ 1 Êó∂Ôºå
Âè™Êúâ‰∏Ä‰∏™ kernelÔºåÂπ∂‰∏îËæìÂÖ•ËæìÂá∫ÈÉΩÂèØËßÜ‰∏∫ 2d array,
Áî® k ‰ª£Ë°® kernel size.
ÈÇ£‰πàËæìÂá∫ÂèØ‰ª•ÂÜôÊàê
</p>

<pre class="example" id="orgfde8628">
output[i,j] = dot(kernel, input[i:i+k, j:j+k])
</pre>

<p>
ËøôÈáå dot Ë°®Á§∫Â∞Ü‰∏§‰∏™Áü©ÈòµÂ±ïÂºÄ‰∏∫‰∏ÄÁª¥ÂêëÈáèÂêé‰ΩúÂÜÖÁßØ„ÄÇ
Ëøô‰∏™ÂÖ¨ÂºèÂèØ‰ª•Áî®‰∏ãÂõæÂΩ¢Ë±°Âú∞Ë°®Ëø∞„ÄÇ
</p>


<figure id="org80d1527">
<img src="./convolve-demo.gif" alt="convolve-demo.gif">

</figure>

<p>
‰∏äÂõæ‰∏≠ÁªøËâ≤ÁöÑ image Ë°®Á§∫ËæìÂÖ•Áü©ÈòµÔºåÈªÑËâ≤ÁöÑ 3 x 3 Áü©Èòµ‰ª£Ë°® kernel,Âè≥ËæπÁöÑÁ∫¢
Ëâ≤Áü©ÈòµÊòØËæìÂá∫ÁªìÊûú„ÄÇÁî®Ëøô‰∏™ÂÖ¨ÂºèÂèØ‰ª•ËÆ°ÁÆóÂá∫ËæìÂá∫Áü©ÈòµÁöÑÂΩ¢Áä∂„ÄÇ‰∏∫‰∫Ü‰øùËØÅ‰∏ãÊ†á‰∏çË∂ä
ÁïåÔºåÈúÄË¶Å i + k ‚â§ H<sub>in</sub>, ÂêåÊó∂ j + k ‚â§ W<sub>out</sub>, ÊâÄ‰ª• \[ H_{out}
= 1 + \max i = H_{in} - k + 1,\qquad W_{out} = 1 + \max j = W_{in} -
k + 1. \]
</p>
</div>
<div id="outline-container-org18a189c" class="outline-3">
<h3 id="org18a189c">stride</h3>
<div class="outline-text-3" id="text-org18a189c">
<p>
ÂèÇÊï∞ stride ÊéßÂà∂ÊØèÊ¨° kernel ÊªëÂä®ÁöÑÊ≠•ÈïøÔºåÁî®ÂÖ¨ÂºèË°®Ëø∞Â∞±ÊòØ
</p>

<pre class="example" id="org0c647d8">
output[i,j] = dot(kernel, input[stride*i:stride*i + k,
                                stride*j:stride*j + k])
</pre>


<figure id="org5b5d99b">
<img src="./convolve-stride.png" alt="convolve-stride.png">

</figure>
</div>
</div>
<div id="outline-container-orgcce0330" class="outline-3">
<h3 id="orgcce0330">padding</h3>
<div class="outline-text-3" id="text-orgcce0330">
<p>
‰ªé‰πãÂâçÁöÑÂÖ¨ÂºèÁúãÂá∫Ôºå‰∏ÄËà¨ËæìÂá∫Áü©ÈòµÁöÑÂ§ßÂ∞èÊòØÂ∞è‰∫éËæìÂÖ•Áü©ÈòµÁöÑ„ÄÇ
Ë¶ÅÊâ©Â±ïËæìÂá∫Áü©ÈòµÂ§ßÂ∞è‰πüÂæàÁÆÄÂçïÔºåÂè™Ë¶ÅÂ§ÑÁêÜ input array ‰∏ãÊ†áË∂äÁïåÁöÑÈóÆÈ¢òÂ∞±Â•Ω‰∫Ü„ÄÇ
‰πãÂâçÁöÑÂ§ÑÁêÜÊñπÂºèÊàê‰∏∫ valid padding, Âç≥‰∏çÂÖÅËÆ∏‰∏ãÊ†áË∂äÁïå„ÄÇ
Âè¶‰∏ÄÁßçÂ§ÑÁêÜÊñπÂºèÊòØ zero padding, Âç≥Ë∂äÁïåÂÖÉÁ¥†ÂùáËßÜ‰∏∫ 0.
zero padding ‰πüÊúâÂ•ΩÂá†ÁßçÊñπÂºèÔºåÊ†πÊçÆÂØπ‰∏ãÊ†áË∂äÁïåÁöÑÂÆπË∫´Á®ãÂ∫¶ËÄåÂÆö„ÄÇ
Âú® Conv2d ‰∏≠Áî® padding Êù•ÊéßÂà∂ padding ÊñπÂºè„ÄÇ
</p>

<ul class="org-ul">
<li>padding=0 Ë°®Á§∫‰ΩøÁî® valid padding, ‰∏çÂÖÅËÆ∏‰∏ãÊ†áË∂äÁïå„ÄÇ</li>
<li>padding=1 Ë°®Á§∫‰ΩøÁî® zero padding, ÂÖÅËÆ∏Á¨¨‰∏Ä‰∏™ÊåáÊ†á‰∏∫ -1 Êàñ H<sub>in</sub>Ôºå  ÂêåÊó∂ÂÖÅËÆ∏Á¨¨‰∫å‰∏™ÊåáÊ†á‰∏∫ -1 Âíå W<sub>in</sub> Âπ∂‰ª§Ê≠§Á±ªËÆøÈóÆÁöÑÁªìÊûú‰∏∫ 0.    ÂÖ∂‰ªñÁ±ªÂûãÁöÑË∂äÁïå‰∏çË¢´ÂÖÅËÆ∏„ÄÇ</li>
<li>padding=2 ÂêåÁêÜÔºåÂÖÅËÆ∏Âú®Ê®™ÂêëÂíåÁ∫µÂêëË∂äÁïå‰∏§‰∏™ÂÖÉÁ¥†„ÄÇ</li>
</ul>

<p>
ÂØπ‰∫é 3 x 3 Â§ßÂ∞èÁöÑ kernel, Â¶ÇÊûú stride=1,
ÈÇ£‰πàÂè™Ë¶ÅËÆæÁΩÆ padding=1 Â∞±ËÉΩËÆ©ËæìÂá∫ËæìÂá∫ÊúâÁõ∏ÂêåÁöÑÂ§ßÂ∞è„ÄÇ
</p>


<figure id="org4b36932">
<img src="./convolve-padding.gif" alt="convolve-padding.gif">

</figure>
</div>
</div>
<div id="outline-container-org2f7da97" class="outline-3">
<h3 id="org2f7da97">dilation</h3>
<div class="outline-text-3" id="text-org2f7da97">
<p>
ÊéßÂà∂ kernel Â¶Ç‰ΩïË¶ÜÁõñÂú®ËæìÂÖ•Áü©Èòµ‰∏äÔºåÁî®ÂÖ¨ÂºèË°®Ëø∞Â∞±ÊòØ
</p>

<pre class="example" id="orgebd6a85">
output[i,j] = dot(kernel, input[i:i+k*dilation:dilation,
                                j:j+k*dilation:dilation])
</pre>


<figure id="orge285662">
<img src="./convolve-dilation.gif" alt="convolve-dilation.gif">

</figure>
</div>
</div>
</div>
<div id="outline-container-org4b5b062" class="outline-2">
<h2 id="org4b5b062">Correlation and Convolution</h2>
<div class="outline-text-2" id="text-org4b5b062">
<p>
ÊàëÊúÄÊó©Êé•Ëß¶ÁöÑ correlation, convolution ÊòØÂú®ÂÖâÂ≠¶ËØæ‰∏äÔºå
ÈÇ£ÈáåÊòØÂØπ‰∏§‰∏™ËøûÁª≠‰ø°Âè∑ f, g ÂÆö‰πâÁöÑ
</p>

$$
\begin{aligned}
f \ast g (t) &:= \int_{-\infty}^{\infty} f(\tau) g(t-\tau) d\tau,
\qquad ({\rm convolution})\\
f \star g(t) &:= \int_{-\infty}^{\infty} f(\tau) g(t+\tau) d\tau,
\qquad ({\rm correlation}).
\end{aligned}
$$

<p>
ÊâÄ‰ª•Êï∞Â≠¶‰∏äËØ¥ÔºåÊàë‰ª¨‰πãÂâçËÆ°ÁÆóÁöÑÂπ∂‰∏çÊòØ convolution, ËÄåÊòØ correlation.
Êàë‰∏™‰∫∫ËÆ§‰∏∫ convolution ÊòØÊ≤øÁî®‰∫Ü‰πãÂâç filter ÁöÑÁß∞ÂëºÔºå
Âú®ÈÇ£Èáå kernel ‰∏ÄËà¨ÊòØ‰∏≠ÂøÉÂØπÁß∞ÁöÑÔºå
correlation, convolution ËÆ°ÁÆóÁªìÊûúÁõ∏Âêå„ÄÇ
‰ΩÜÊúÄÊó©Ê≤°ÊúâÊé•Ëß¶ÂõæÂÉèÂ§ÑÁêÜ‰∏≠ filter Ê¶ÇÂøµÂâçËøô‰∏™Áß∞ÂëºËÆ©ÊàëËø∑ÊÉë‰∫ÜÂæà‰πÖÔºå
‰∏ÄÁõ¥Ê≤°ÊÉ≥ÊòéÁôΩ‰∏∫Âï•ÊòéÊòéÂÅöÁöÑÊòØ
correlation Âç¥Âè´ convolution.
</p>
</div>
</div>
<div id="outline-container-orgda9a597" class="outline-2">
<h2 id="orgda9a597">References</h2>
<div class="outline-text-2" id="text-orgda9a597">
<ul class="org-ul">
<li><a href="https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148">Understanding of Convolutional Neural Network (CNN) ‚Äî Deep Learning</a></li>
<li><a href="https://cs231n.github.io/convolutional-networks/#norm">Convolutional Neural Networks (CNNs / ConvNets)</a></li>
</ul>
</div>
</div>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-ai.html">ai</a> </div>
]]></description>
  <category><![CDATA[ai]]></category>
  <link>https://dou-meishi.github.io/org-blog/2020-12-04-Conv2dNote/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2020-12-04-Conv2dNote/notes.html</guid>
  <pubDate>Fri, 04 Dec 2020 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[Gaussian Quadrature and Borwein Integrals]]></title>
  <description><![CDATA[
<p>
We discuss how to apply the Gaussian quadrature rule to evaluate the
Borwein integral \[ \textrm{pr}_h(\Delta|\bar{c}) = \frac{1}{2\pi}
\int_{-\infty}^{+\infty}\cos\Delta t\prod_{m=k+1}^{k+h} \frac{\sin
\bar{c}Q^mt}{\bar{c}Q^mt}\,dt, \qquad k\in\mathbb{N}, \quad
h\in\mathbb{N}_+,\quad \bar{c}, \Delta\in(0,\infty). \]
</p>

<p>
See full text <a href="./fourier-transform-of-sinc.pdf">here</a>.
</p>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-math.html">math</a> </div>
]]></description>
  <category><![CDATA[math]]></category>
  <link>https://dou-meishi.github.io/org-blog/2020-01-14-GaussianQuadrature/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2020-01-14-GaussianQuadrature/notes.html</guid>
  <pubDate>Tue, 14 Jan 2020 00:00:00 +0800</pubDate>
</item>
<item>
  <title><![CDATA[The Jordan Normal Form]]></title>
  <description><![CDATA[
<p>
Êàë‰ª¨ËÆ®ËÆ∫Â§çÊï∞Âüü \(\mathbb{C}\) (ÊàñËÄÖ‰ªªÊÑè‰ª£Êï∞Èó≠Âüü) ‰∏äÁöÑ \(n\) Èò∂ÊñπÈòµ \(A\) ÁöÑ
ÂØπËßíÂåñÈóÆÈ¢òÔºåËøôÁ≠â‰ª∑‰∫éÁ†îÁ©∂Á∫øÊÄßÂèòÊç¢ \(\mathscr{A}\colon x\mapsto Ax\) Âú®‰Ωï
ÁµÑÂü∫‰∏ãÊúâÊúÄÁÆÄÂçïÁöÑÁü©ÈòµË°®Á§∫„ÄÇ‰ª£Êï∞Èó≠ÁöÑÊù°‰ª∂ÊòØ‰∏∫‰∫Ü‰øùËØÅÊàë‰ª¨ÊâÄËÄÉËôëÁöÑÂ§öÈ°πÂºèÊÄªÂèØ
‰ª•ÂàÜËß£‰∏∫‰∏ÄÊ¨°Âõ†Âºè‰πòÁßØ„ÄÇÊØîÂ¶ÇÂèØ‰ª•Â∞ÜÁü©Èòµ \(A\) ÁöÑÁâπÂæÅÂ§öÈ°πÂºè
\(p_\textrm{char}(x)= \operatorname{det} \,(xI-A)\) Âõ†ÂºèÂàÜËß£‰∏∫
\(p_\textrm{char}(x) = \prod_{j=1}^s (x-\lambda_j)^{n_j}\)ÔºåËøôÈáå
\(\{\lambda_j,\; j=1, 2, \ldots, s\}\) ÊòØ \(A\) ÁöÑÁâπÂæÅÂ§öÈ°πÂºèÁªÑÊàêÁöÑÈõÜÂêàÔºåÁß∞
‰∏∫Ë∞±Ôºå\(n_j\) ÊòØÁâπÂæÅÂÄº \(\lambda_j\) ÁöÑ‰ª£Êï∞ÈáçÊï∞ÔºåÊª°Ë∂≥ \(\sum_{j=1}^s n_j =
n\)„ÄÇÈô§‰∫ÜÁâπÂæÅÂ§öÈ°πÂºèÔºåÈõ∂ÂåñÂ§öÈ°πÂºè‰πüÊòØÂàªÁîªÁü©ÈòµÊÄßË¥®ÁöÑÊúâÂà©Â∑•ÂÖ∑„ÄÇ
</p>

<p>
See full text <a href="./The-Jordan-Normal-Form.pdf">here</a>.
</p>
<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-math.html">math</a> </div>
]]></description>
  <category><![CDATA[math]]></category>
  <link>https://dou-meishi.github.io/org-blog/2019-10-29-JordanNormalForm/notes.html</link>
  <guid>https://dou-meishi.github.io/org-blog/2019-10-29-JordanNormalForm/notes.html</guid>
  <pubDate>Tue, 29 Oct 2019 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
