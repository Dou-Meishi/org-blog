<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<link rel="alternate"
      type="application/rss+xml"
      href="https://dou-meishi.github.io/org-blog/rss.xml"
      title="RSS feed for https://dou-meishi.github.io/org-blog/">
<title>Dou Meishi's Blog</title>
<meta name="author" content="Dou Meishi">
<meta name="referrer" content="no-referrer">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href= "https://gongzhitaao.org/orgcss/org.css" rel="stylesheet" type="text/css" />
<link href= "https://dou-meishi.github.io/org-blog/static/dou-org-blog.css" rel="stylesheet" type="text/css" />
<!-- Math Support by KaTeX -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
</head>
<body>
<div id="preamble" class="status"><div class="header">
  <div class="sitelinks">
    <a href="https://dou-meishi.github.io/org-blog/index.html">Home</a>
    |
    <a href="https://dou-meishi.github.io/org-blog/archive.html">All Posts</a>
  </div>
</div>
</div>
<div id="content">
<h1 class="title">Posts tagged "ai":</h1><h2 class="post-title"><a href="https://dou-meishi.github.io/org-blog/2024-10-30-LearnTransformer/notes.html">What Happens in A Transformer Layer</a></h2><div class="post-date">30 Oct 2024</div><p>
Transformers serve as the backbone of large language models. Just as
 convolutional networks revolutionized image processing, transformers
 have significantly advanced natural language processing since their
 introduction. The efficient parallel computation and transfer
 learning capabilities of transformers have led to the rise of
 <i>pre-trained paradigm</i>. In this approach, a large-scale
 transformer-based model, referred to as the <i>foundation model</i>, is
 trained on a significant volume of data and subsequently utilized for
 downstream tasks through some form of fine-tuning. Our familiar
 friend ChatGPT is such an example, where GPT stands for generative
 pre-trained transformers. Meanwhile, transformer-base models achieve
 state-of-the-art performace for many different modalities, including
 text, image, video, point cloud, and audio data, and have been used
 for both discriminative and generative applications.
</p>...<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-ai.html">ai</a> </div><h2 class="post-title"><a href="https://dou-meishi.github.io/org-blog/2024-09-24-ImplementBP/notes.html">An In-Depth Introduction to Backpropagation and Automatic Differentiation</a></h2><div class="post-date">24 Sep 2024</div><p>
Backpropagation and automatic differentiation (AD) are fundamental
components of modern deep learning frameworks. However, many
practitioners pay little attention to their implementations and may
regard them as some sort of "black magic". It indeed looks like magic
that PyTorch can virtually calculate derivatives of an arbitrary
function defined by the user, and even accommodate flow control
elements like conditional execution, which is mathematically not
differentiable. Although we understand that mathematically they
primarily employ the chain rule, it remains unclear how they
efficiently apply it to a function whose form is entirely unknown and
will be determined by the user.
</p>...<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-ai.html">ai</a> </div><h2 class="post-title"><a href="https://dou-meishi.github.io/org-blog/2024-02-19-MNISTwithPytorch/notes.html">MNIST: the Hello World Example in Image Recognition</a></h2><div class="post-date">19 Feb 2024</div><p>
In this post we will train a simple CNN (<i>Convolutional Neural
Network</i>) classifier in PyTorch to recognize handwritten digits in
MNIST dataset.
</p>...<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-ai.html">ai</a> </div><h2 class="post-title"><a href="https://dou-meishi.github.io/org-blog/2021-11-07-BackpropagationFormula/notes.html">Backpropagation Formula: An Optimal Control View</a></h2><div class="post-date">07 Nov 2021</div><p>
Consider the following optimal control problem (or equivalently, a constrained optimization problem),
</p>...<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-ai.html">ai</a> </div><h2 class="post-title"><a href="https://dou-meishi.github.io/org-blog/2020-12-04-Conv2dNote/notes.html">Convolution in CNN</a></h2><div class="post-date">04 Dec 2020</div><p>
这篇笔记是对 <code>torch.nn.Conv2d</code> [ <a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d">官方文档</a> ] 的解释，
主要目标是弄清楚在这层 layer 中发生了什么：
输入输出是什么？是怎么从输入到输出的？
最后补充了一个困扰我很久的问题：
为什么要叫 convolution?
</p>...<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-ai.html">ai</a> </div><div id="archive">
<a href="https://dou-meishi.github.io/org-blog/archive.html">Other posts</a>
</div>
</div>
<div id="postamble" class="status">Created by <a href="https://github.com/bastibe/org-static-blog/">Org Static Blog</a>
</div>
</body>
</html>
