<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<link rel="alternate"
      type="application/rss+xml"
      href="https://dou-meishi.github.io/org-blog/rss.xml"
      title="RSS feed for https://dou-meishi.github.io/org-blog/">
<title>Dou Meishi's Blog</title>
<meta name="author" content="Dou Meishi">
<meta name="referrer" content="no-referrer">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="google-site-verification" content="_Ly4i8BW_CWeGaFdsQgJ2xN-yOkGpSDnLw8LitvkEsw" />
<link href= "https://gongzhitaao.org/orgcss/org.css" rel="stylesheet" type="text/css" />
<link href= "https://dou-meishi.github.io/org-blog/static/dou-org-blog.css" rel="stylesheet" type="text/css" />
<!-- Math Support by KaTeX -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
</head>
<body>
<div id="preamble" class="status"><div class="header">
  <div class="sitelinks">
    <a href="https://dou-meishi.github.io/org-blog/index.html">Home</a>
    |
    <a href="https://dou-meishi.github.io/org-blog/archive.html">All Posts</a>
  </div>
</div>
</div>
<div id="content">
<h1 class="title">Posts tagged "ai":</h1><h2 class="post-title"><a href="https://dou-meishi.github.io/org-blog/2025-04-24-PracticeOptuna/notes.html">A Beginner's Guide to Optuna</a></h2><div class="post-date">24 Apr 2025</div><p>
Many sophisticated algorithms, particularly deep learning-based ones,
have various hyperparameters that control the running process.  Those
hyperparameters often have a significant impact on
performance. Manually searching for a good combination of those
hyperparameters is tedious, and even keeping track of all results
becomes unwieldy as the number of hyperparameters grows
beyond 10. Today's topic, <i>Optuna</i>, is such a tool that helps automate
the search process and stores all results in a structured
database. Even better, its web-based dashboard lets us explore these
results intuitively with just a few clicks.
</p>...<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-ai.html">ai</a> </div><h2 class="post-title"><a href="https://dou-meishi.github.io/org-blog/2025-02-20-ConvMathVisualCode/notes.html">Gradients of Convolution: Direct Computation and Linear Algebra Perspective</a></h2><div class="post-date">20 Feb 2025</div><p>
Convolution operations are foundational in deep learning for
extracting features in image tasks. Calculating their gradients is
critical for training convolutional neural networks, enabling
backpropagation to update parameters and minimize loss. This post
derives these gradients through two complementary approaches: direct
differentiation of the convolution definition, and a linear algebra
perspective which naturally introduces the <i>transposed convolution</i>
(also known as <i>deconvolution</i>).
</p>...<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-ai.html">ai</a> </div><h2 class="post-title"><a href="https://dou-meishi.github.io/org-blog/2024-11-28-UseEinops/notes.html">Practical Einops: Tensor Operations Based on Indices</a></h2><div class="post-date">28 Nov 2024</div><p>
People are familiar with vectors and matrices operations but are less
familiar with tensor operations. In machine learning, <i>tensors</i> often
refer to batched vectors or batched matrices and are represented by an
array-like object with multiple indices. Due to this reason, tensors
operations in most Python packages, including NumPy, PyTorch and
TensorFlow, are typically named after vectors and matrices operations.
However, tensors themselves have a particular useful operation, called
<i>contraction</i>, which uses index-based notations and can cover most
vectors and matrices operations. This index-based notations
intuitively and verbosely describe the relationship between the
components of input and output tensors. Today's topic, the Python's
<a href="https://github.com/arogozhnikov/einops">einops</a> package, extends these notations and provides an elegant API
for flexible and powerful tensor operations.
</p>...<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-ai.html">ai</a> </div><h2 class="post-title"><a href="https://dou-meishi.github.io/org-blog/2024-10-30-LearnTransformer/notes.html">What Happens in A Transformer Layer</a></h2><div class="post-date">30 Oct 2024</div><p>
Transformers serve as the backbone of large language models. Just as
 convolutional networks revolutionized image processing, transformers
 have significantly advanced natural language processing since their
 introduction. The efficient parallel computation and transfer
 learning capabilities of transformers have led to the rise of
 <i>pre-trained paradigm</i>. In this approach, a large-scale
 transformer-based model, referred to as the <i>foundation model</i>, is
 trained on a significant volume of data and subsequently utilized for
 downstream tasks through some form of fine-tuning. Our familiar
 friend ChatGPT is such an example, where GPT stands for generative
 pre-trained transformers. Meanwhile, transformer-base models achieve
 state-of-the-art performace for many different modalities, including
 text, image, video, point cloud, and audio data, and have been used
 for both discriminative and generative applications.
</p>...<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-ai.html">ai</a> </div><h2 class="post-title"><a href="https://dou-meishi.github.io/org-blog/2024-09-24-ImplementBP/notes.html">An In-Depth Introduction to Backpropagation and Automatic Differentiation</a></h2><div class="post-date">24 Sep 2024</div><p>
Backpropagation and automatic differentiation (AD) are fundamental
components of modern deep learning frameworks. However, many
practitioners pay little attention to their implementations and may
regard them as some sort of "black magic". It indeed looks like magic
that PyTorch can virtually calculate derivatives of an arbitrary
function defined by the user, and even accommodate flow control
elements like conditional execution, which is mathematically not
differentiable. Although we understand that mathematically they
primarily employ the chain rule, it remains unclear how they
efficiently apply it to a function whose form is entirely unknown and
will be determined by the user.
</p>...<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-ai.html">ai</a> </div><h2 class="post-title"><a href="https://dou-meishi.github.io/org-blog/2024-02-19-MNISTwithPytorch/notes.html">MNIST: the Hello World Example in Image Recognition</a></h2><div class="post-date">19 Feb 2024</div><p>
In this post we will train a simple CNN (<i>Convolutional Neural
Network</i>) classifier in PyTorch to recognize handwritten digits in
MNIST dataset.
</p>...<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-ai.html">ai</a> </div><h2 class="post-title"><a href="https://dou-meishi.github.io/org-blog/2021-11-07-BackpropagationFormula/notes.html">Backpropagation Formula: An Optimal Control View</a></h2><div class="post-date">07 Nov 2021</div><p>
Consider the following optimal control problem (or equivalently, a constrained optimization problem),
</p>...<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-ai.html">ai</a> </div><h2 class="post-title"><a href="https://dou-meishi.github.io/org-blog/2020-12-04-Conv2dNote/notes.html">Convolution in CNN</a></h2><div class="post-date">04 Dec 2020</div><p>
这篇笔记是对 <code>torch.nn.Conv2d</code> [ <a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d">官方文档</a> ] 的解释，
主要目标是弄清楚在这层 layer 中发生了什么：
输入输出是什么？是怎么从输入到输出的？
最后补充了一个困扰我很久的问题：
为什么要叫 convolution?
</p>...<div class="taglist"><a href="https://dou-meishi.github.io/org-blog/tags.html">Tags</a>: <a href="https://dou-meishi.github.io/org-blog/tag-ai.html">ai</a> </div><div id="archive">
<a href="https://dou-meishi.github.io/org-blog/archive.html">Other posts</a>
</div>
</div>
<div id="postamble" class="status">Created by <a href="https://github.com/bastibe/org-static-blog/">Org Static Blog</a>
</div>
</body>
</html>
